{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/RAG/blob/main/LAB03_FineTuning_vs_Adapters_vs_RAG_Exemple_Open_Source.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWuAGMaEnlxO"
      },
      "source": [
        "# LAB03 : Fine-Tuning vs Adapters vs RAG â€“ Comparaison des 3 approches\n",
        "\n",
        "\n",
        "## Objectif\n",
        "- Comparer comment le fine-tuning, les adapters et RAG rÃ©solvent le mÃªme problÃ¨me : faire rÃ©pondre un LLM Ã  des questions spÃ©cifiques Ã  un domaine.\n",
        "\n",
        "## DurÃ©e estimÃ©e\n",
        "- ~30 minutes\n",
        "\n",
        "## Livrables\n",
        "- Notebook montrant les rÃ©sultats Q&A des trois stratÃ©gies\n",
        "\n",
        "## Exemple Open Source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzknAvmgnlxP"
      },
      "source": [
        "## Ã‰tape 1 : Setup (5 min)\n",
        "\n",
        "Installation des bibliothÃ¨ques nÃ©cessaires :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZSnV-GlnlxQ"
      },
      "outputs": [],
      "source": [
        "# !pip install -q --upgrade pip\n",
        "\n",
        "# Installer les packages principaux\n",
        "!pip install -q datasets\n",
        "!pip install -q transformers\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain-community\n",
        "!pip install -q langchain-huggingface\n",
        "\n",
        "# Ignorer les erreurs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q chromadb\n",
        "!pip install -q -U langchain-chroma\n",
        "# Ignorer les erreurs"
      ],
      "metadata": {
        "id": "4Lg5zJQgfyBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2-eLsHanlxQ"
      },
      "source": [
        "## Ã‰tape 2 : CrÃ©ation d'une base de connaissances simple (5 min)\n",
        "\n",
        "Nous crÃ©ons une base de connaissances fictive sur l'IA Agentique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llazsGWFnlxQ",
        "outputId": "876faecc-428f-4757-ca0f-cf638bd642aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Base (KB):\n",
            "  1. Agentic AI agents use memory, tools, and goals to act.\n",
            "  2. LangChain and CrewAI are popular frameworks for building AI agents.\n",
            "  3. Retrieval-Augmented Generation (RAG) improves accuracy by fetching external knowledge.\n",
            "  4. Agents can iterate and refine their actions based on feedback.\n",
            "  5. Tool calling allows agents to interact with APIs and external systems.\n",
            "  6. Les agents d'IA agentiques utilisent la mÃ©moire, des outils et des objectifs pour agir.\n",
            "  7. LangChain et CrewAI sont des frameworks populaires pour la crÃ©ation d'agents d'IA.\n",
            "  8. La gÃ©nÃ©ration augmentÃ©e par rÃ©cupÃ©ration (RAG) amÃ©liore la prÃ©cision en rÃ©cupÃ©rant des connaissances externes.\n",
            "  9. Les agents peuvent itÃ©rer et affiner leurs actions en fonction des retours d'information.\n",
            "  10. L'appel d'outils permet aux agents d'interagir avec des API et des systÃ¨mes externes.\n",
            "\n",
            "Test Questions:\n",
            "  1. What are the key components of Agentic AI?\n",
            "  2. Name one framework for AI agents.\n",
            "  3. How does RAG improve answers?\n",
            "  4. Quels sont les composants clÃ©s de l'IA agentique ?\n",
            "  5. Citez un framework pour agents d'IA\n",
            "  6. Comment la RAG amÃ©liore-t-elle les rÃ©ponses ?\n"
          ]
        }
      ],
      "source": [
        "# Base de connaissances sur l'IA Agentique\n",
        "kb = [\n",
        "    \"Agentic AI agents use memory, tools, and goals to act.\",\n",
        "    \"LangChain and CrewAI are popular frameworks for building AI agents.\",\n",
        "    \"Retrieval-Augmented Generation (RAG) improves accuracy by fetching external knowledge.\",\n",
        "    \"Agents can iterate and refine their actions based on feedback.\",\n",
        "    \"Tool calling allows agents to interact with APIs and external systems.\",\n",
        "    \"Les agents d'IA agentiques utilisent la mÃ©moire, des outils et des objectifs pour agir.\",\n",
        "    \"LangChain et CrewAI sont des frameworks populaires pour la crÃ©ation d'agents d'IA.\",\n",
        "    \"La gÃ©nÃ©ration augmentÃ©e par rÃ©cupÃ©ration (RAG) amÃ©liore la prÃ©cision en rÃ©cupÃ©rant des connaissances externes.\",\n",
        "    \"Les agents peuvent itÃ©rer et affiner leurs actions en fonction des retours d'information.\",\n",
        "    \"L'appel d'outils permet aux agents d'interagir avec des API et des systÃ¨mes externes.\"\n",
        "]\n",
        "\n",
        "# Questions de test\n",
        "questions = [\n",
        "    \"What are the key components of Agentic AI?\",\n",
        "    \"Name one framework for AI agents.\",\n",
        "    \"How does RAG improve answers?\",\n",
        "    \"Quels sont les composants clÃ©s de l'IA agentique ?\",\n",
        "    \"Citez un framework pour agents d'IA\",\n",
        "    \"Comment la RAG amÃ©liore-t-elle les rÃ©ponses ?\",\n",
        "]\n",
        "\n",
        "print(\"Knowledge Base (KB):\")\n",
        "for i, doc in enumerate(kb, 1):\n",
        "    print(f\"  {i}. {doc}\")\n",
        "\n",
        "print(\"\\nTest Questions:\")\n",
        "for i, q in enumerate(questions, 1):\n",
        "    print(f\"  {i}. {q}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNR7cYlknlxQ"
      },
      "source": [
        "## Ã‰tape 3 : Fine-Tuning (dÃ©mo conceptuelle : 5 min)\n",
        "\n",
        "**Fine-tuning** = consiste Ã  mettre Ã  jour les poids du modÃ¨le avec de nouveaux exemples Ã©tiquetÃ©s.\n",
        "\n",
        "Voici Ã  quoi ressemblerait un dataset de fine-tuning :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH49Anb1nlxQ"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# DonnÃ©es d'entraÃ®nement pour fine-tuning\n",
        "train_data = Dataset.from_dict({\n",
        "    \"prompt\": [\n",
        "        \"Q: What are the key components of Agentic AI?\\nA:\",\n",
        "        \"Q: Name one framework for AI agents.\\nA:\",\n",
        "        \"Q: How does RAG improve answers?\\nA:\"\n",
        "    ],\n",
        "    \"completion\": [\n",
        "        \" Agentic AI agents use memory, tools, and goals to act.\",\n",
        "        \" LangChain is a framework for building AI agents.\",\n",
        "        \" RAG improves accuracy by fetching external knowledge before answering.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Fine-Tuning Training Dataset:\")\n",
        "print(train_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINE-TUNING OBSERVATIONS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "âœ“ Avantages:\n",
        "  - ModÃ¨le hautement adaptÃ© au domaine\n",
        "  - Connaissances \"baked-in\" et rapides\n",
        "\n",
        "âœ— InconvÃ©nients:\n",
        "  - CoÃ»teux (calcul + infrastructure)\n",
        "  - Rigide (difficile de mettre Ã  jour les connaissances)\n",
        "  - NÃ©cessite un rÃ©entraÃ®nement pour chaque nouveau domaine\n",
        "  - DÃ©pend de la qualitÃ© des donnÃ©es d'entraÃ®nement\n",
        "\n",
        "ğŸ“Œ Avec OpenAI ou Hugging Face, vous uploaderiez d'abord ce dataset pour faire le fine-tuning ensuite.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeIXdYg2nlxR"
      },
      "source": [
        "## Ã‰tape 4 : Adapters / LoRA (dÃ©mo conceptuelle : 5 min)\n",
        "\n",
        "**Adapters** = petites couches parameter-efficient que l'on entraÃ®ne au lieu de rÃ©-entraÃ®nez le modÃ¨le entier.\n",
        "\n",
        "C'est une alternative au fine-tuning complet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdqvT1v6nlxR"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Charger un petit modÃ¨le pour la dÃ©mo\n",
        "model_name = \"distilgpt2\"\n",
        "try:\n",
        "    tok = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"âœ“ ModÃ¨le chargÃ©: {model_name}\")\n",
        "    print(f\"  Nombre total de paramÃ¨tres: {total_params:,}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Note: TÃ©lÃ©chargement du modÃ¨le Ã©chouÃ© (attendu en mode hors-ligne)\")\n",
        "    print(f\"Erreur: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ADAPTERS / LoRA OBSERVATIONS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "âœ“ Avantages:\n",
        "  - TRÃˆS peu de paramÃ¨tres Ã  entraÃ®ner (~0.1-1% du modÃ¨le original)\n",
        "  - Ã‰conomique (moins de calcul, moins de mÃ©moire)\n",
        "  - Modulaire (plusieurs adapters pour diffÃ©rents domaines)\n",
        "  - Rapide Ã  mettre Ã  jour\n",
        "\n",
        "âœ— InconvÃ©nients:\n",
        "  - NÃ©cessite toujours une infrastructure d'entraÃ®nement\n",
        "  - Performance lÃ©gÃ¨rement infÃ©rieure au fine-tuning complet\n",
        "  - NÃ©cessite un framework compatible (PEFT, AdapterHub)\n",
        "\n",
        "ğŸ“Œ Avec LoRA, vous ne mettriez Ã  jour que quelques millions de paramÃ¨tres\n",
        "    au lieu de milliards, rendant l'entraÃ®nement beaucoup plus efficace.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ByhjDujnlxR"
      },
      "source": [
        "## Ã‰tape 5 : RAG â€“ dÃ©mo intÃ©ractive (5 min)\n",
        "\n",
        "Contrairement Ã  l'entraÃ®nement, **RAG** rÃ©cupÃ¨re les connaissances externes au moment de l'exÃ©cution.\n",
        "\n",
        "Construisons un systÃ¨me RAG complet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV1rjy-UnlxR"
      },
      "source": [
        "### Configuration de RAG avec LangChain et ChromaDB\n",
        "\n",
        "Nous utilisons ChromaDB (une alternative lÃ©gÃ¨re Ã  FAISS) pour cette dÃ©mo :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "381WLHkTnlxR"
      },
      "outputs": [],
      "source": [
        "# from langchain_community.vectorstores import Chroma\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os\n",
        "\n",
        "print(\"LangChain et ChromaDB importÃ©s avec succÃ¨s.\")\n",
        "print(\"\\nğŸ“Œ Configuration du systÃ¨me RAG...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG3JMYuInlxR"
      },
      "source": [
        "__________________\n",
        "## Option gratuite : RAG avec HuggingFace Embeddings (sans API externe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLhEAgepnlxS"
      },
      "source": [
        "### DÃ©mo RAG : Retrieval (RÃ©cupÃ©ration de Documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regardons les dimensions de quelques modÃ¨les disponibles :"
      ],
      "metadata": {
        "id": "jnOyi9Ripm6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "vec = model.encode(\"test\", convert_to_numpy=True)\n",
        "print(\"sentence-transformers/all-mpnet-base-v2\",\"embedding shape:\", vec.shape)  # ex: (384,) ou (768,)\n"
      ],
      "metadata": {
        "id": "-hLxcy_cMaZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vec = model.encode(\"test\", convert_to_numpy=True)\n",
        "print(\"sentence-transformers/all-MiniLM-L6-v2\",\"embedding shape:\", vec.shape)  # ex: (384,) ou (768,)\n"
      ],
      "metadata": {
        "id": "vRtI_D-tRXVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DÃ©finition du RAG avec le modÃ¨le **sentence-transformers/all-MiniLM-L6-v2**"
      ],
      "metadata": {
        "id": "8TsEdobbqCa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_rag_with_huggingface(kb, persist_dir=\"./chroma_db\"):\n",
        "    \"\"\"\n",
        "    Configure RAG avec HuggingFace embeddings.\n",
        "    kb : liste de textes (knowledge base)\n",
        "    persist_dir : chemin oÃ¹ persister la base Chroma\n",
        "    \"\"\"\n",
        "    import os\n",
        "    #from langchain_community.vectorstores import Chroma\n",
        "    from langchain_chroma import Chroma\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    from langchain_core.documents import Document\n",
        "\n",
        "    try:\n",
        "        # Choisir un modÃ¨le et l'utiliser partout\n",
        "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        print(f\"ğŸ“¦ ModÃ¨le sÃ©lectionnÃ©: {model_name}\")\n",
        "\n",
        "        # Charger les embeddings HF\n",
        "        print(\"ğŸ“š Chargement des embeddings HuggingFace...\")\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "        # VÃ©rifier si la base existe dÃ©jÃ \n",
        "        if os.path.exists(persist_dir) and os.listdir(persist_dir):\n",
        "            print(f\"âœ“ Base Chroma existante dÃ©tectÃ©e: {persist_dir}\")\n",
        "            print(\"ğŸ“‚ Chargement de la base existante...\")\n",
        "            db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
        "        else:\n",
        "            print(\"ğŸ†• Aucune base existante, crÃ©ation d'une nouvelle base...\")\n",
        "\n",
        "            # CrÃ©er les documents\n",
        "            docs = [Document(page_content=x) for x in kb]\n",
        "\n",
        "            # (Debug) vÃ©rifier la dimension d'embedding si l'API le permet\n",
        "            try:\n",
        "                sample = embeddings.embed_query(\"test\")\n",
        "                print(\"ğŸ” Embedding sample length:\", len(sample))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # Construire la base vectorielle (Chroma)\n",
        "            print(\"ğŸ”¨ Construction de la base vectorielle...\")\n",
        "            db = Chroma.from_documents(docs, embeddings, persist_directory=persist_dir)\n",
        "            print(f\"ğŸ’¾ Base sauvegardÃ©e dans: {persist_dir}\")\n",
        "\n",
        "        # CrÃ©er un retriever\n",
        "        # retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
        "        # on rÃ©cupÃ¨re les 2ers documents correspondants :\n",
        "        retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "        return retriever, db\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Erreur lors de la configuration RAG: {e}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "C6jWi5U9rXMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1er exemple basique : Q&A RAG simple sans LLM externe**\n",
        "  - Les informations sont passÃ©es dans le contexte\n",
        "  - kb1 : liste de textes (knowledge base)\n",
        "  - questions1 : liste de chaÃ®nes (questions)\n",
        "    "
      ],
      "metadata": {
        "id": "MqXpBgVZihiq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYfaP_pWnlxS"
      },
      "outputs": [],
      "source": [
        "# Fonction pour effectuer Q&A RAG simple sans LLM externe\n",
        "def simple_rag_qa():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SIMPLE RAG Q&A (Retrieval-based)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    retriever, _ = setup_rag_with_huggingface(kb)\n",
        "\n",
        "    if not retriever:\n",
        "        print(\"âš ï¸  Impossible de configurer le retriever.\")\n",
        "        return\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for question in questions:\n",
        "        docs = retriever.invoke(question)\n",
        "\n",
        "        # Combiner les documents rÃ©cupÃ©rÃ©s comme contexte\n",
        "        context = \"\\n\".join([f\"- {doc.page_content}\" for doc in docs])\n",
        "\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"context\": context,\n",
        "            \"source_docs\": len(docs)\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"\\nâ“ Q: {question}\")\n",
        "        print(f\"ğŸ“– Context Retrieved ({result['source_docs']} doc):\")\n",
        "        print(f\"   {context}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ExÃ©cuter la dÃ©mo\n",
        "rag_results = simple_rag_qa()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2nd exemple plus rÃ©aliste : Q&A RAG avec LLM externe**\n",
        "  - kb : liste de textes (knowledge base)\n",
        "  - questions : liste de chaÃ®nes (questions)\n",
        "    "
      ],
      "metadata": {
        "id": "4v536x_qi-Ub"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5JjwYx0nlxS"
      },
      "outputs": [],
      "source": [
        "# RAG avec LLM HuggingFace : 1 Ã¨re question pour tester le fonctionnement :\n",
        "retriever, db = setup_rag_with_huggingface(kb)\n",
        "\n",
        "if retriever:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RAG RETRIEVAL DEMO\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Tester la rÃ©cupÃ©ration\n",
        "    test_query = \"What are the components of Agentic AI?\"\n",
        "    print(f\"\\nğŸ“ Question: {test_query}\")\n",
        "\n",
        "    retrieved_docs = retriever.invoke(test_query)\n",
        "\n",
        "    print(f\"\\nğŸ“– Documents rÃ©cupÃ©rÃ©s (top {len(retrieved_docs)}):\")\n",
        "    for i, doc in enumerate(retrieved_docs, 1):\n",
        "        print(f\"  [{i}] {doc.page_content}\")\n",
        "else:\n",
        "    print(\"âš ï¸  RAG n'a pas pu Ãªtre configurÃ©. VÃ©rifiez votre connexion.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG avec LLM HuggingFace : on soumet toutes nos questions enregistrÃ©es au dÃ©but du notebook :\n",
        "retriever, db = setup_rag_with_huggingface(kb)\n",
        "if retriever:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RAG RETRIEVAL DEMO\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "   # Liste de questions Ã  exÃ©cuter :\n",
        "   # questions = [\n",
        "   #     \"What are the components of Agentic AI?\",\n",
        "   #     \"How does RAG work?\",\n",
        "   #     \"What are the benefits of using LLMs?\"\n",
        "   #     # Ajoutez vos questions ici\n",
        "   # ]\n",
        "\n",
        "    # ExÃ©cuter chaque question successivement\n",
        "    for idx, test_query in enumerate(questions, 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Question {idx}/{len(questions)}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"\\nğŸ“ Question: {test_query}\")\n",
        "\n",
        "        retrieved_docs = retriever.invoke(test_query)\n",
        "        print(f\"\\nğŸ“– Documents rÃ©cupÃ©rÃ©s (top {len(retrieved_docs)}):\")\n",
        "        for i, doc in enumerate(retrieved_docs, 1):\n",
        "            print(f\"  [{i}] {doc.page_content}\")\n",
        "else:\n",
        "    print(\"âš ï¸  RAG n'a pas pu Ãªtre configurÃ©. VÃ©rifiez votre connexion.\")"
      ],
      "metadata": {
        "id": "1ylCRi9Glo3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ6tuzS_nlxS"
      },
      "source": [
        "## Ã‰tape 6 : Comparaison des RÃ©sultats (5 min)\n",
        "\n",
        "CrÃ©ons un tableau comparatif des trois approches :"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# CrÃ©er le tableau comparatif\n",
        "comparison_data = {\n",
        "    \"Aspect\": [\n",
        "        \"Accuracy\",\n",
        "        \"Cost\",\n",
        "        \"Training Time\",\n",
        "        \"Update Frequency\",\n",
        "        \"Knowledge Freshness\",\n",
        "        \"Scalability\",\n",
        "        \"Complexity\",\n",
        "        \"Infrastructure\"\n",
        "    ],\n",
        "    \"Fine-Tuning\": [\n",
        "        \"â­â­â­â­â­\",\n",
        "        \"ğŸ’°ğŸ’°ğŸ’°ğŸ’°\",\n",
        "        \"ğŸ• Hours/Days\",\n",
        "        \"Monthly/Quarterly\",\n",
        "        \"Stale (until retrained)\",\n",
        "        \"Fair (one model per domain)\",\n",
        "        \"ğŸ”´ High\",\n",
        "        \"GPUs required\"\n",
        "    ],\n",
        "    \"Adapters/LoRA\": [\n",
        "        \"â­â­â­â­\",\n",
        "        \"ğŸ’°ğŸ’°\",\n",
        "        \"ğŸ• Minutes/Hours\",\n",
        "        \"Weekly\",\n",
        "        \"Semi-fresh\",\n",
        "        \"Good (modular)\",\n",
        "        \"ğŸŸ¡ Medium\",\n",
        "        \"GPUs (lighter)\"\n",
        "    ],\n",
        "    \"RAG\": [\n",
        "        \"â­â­â­\",\n",
        "        \"ğŸ’°\",\n",
        "        \"âš¡ Real-time\",\n",
        "        \"Real-time\",\n",
        "        \"âœ¨ Always Fresh\",\n",
        "        \"Excellent\",\n",
        "        \"ğŸŸ¢ Low\",\n",
        "        \"Vector DB + API\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# CrÃ©ation du HTML stylisÃ©\n",
        "html = \"\"\"\n",
        "<style>\n",
        "    .comparison-table {\n",
        "        width: 100%;\n",
        "        max-width: 1200px;\n",
        "        margin: 20px auto;\n",
        "        background: linear-gradient(to bottom right, #f8fafc, #eff6ff);\n",
        "        border-radius: 8px;\n",
        "        overflow: hidden;\n",
        "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    .table-header {\n",
        "        background: linear-gradient(to right, #2563eb, #4f46e5);\n",
        "        padding: 20px;\n",
        "        text-align: center;\n",
        "    }\n",
        "    .table-header h1 {\n",
        "        color: white;\n",
        "        font-size: 24px;\n",
        "        font-weight: bold;\n",
        "        margin: 0;\n",
        "    }\n",
        "    .styled-table {\n",
        "        width: 100%;\n",
        "        border-collapse: collapse;\n",
        "        background: white;\n",
        "    }\n",
        "    .styled-table thead tr {\n",
        "        background-color: #f1f5f9;\n",
        "        border-bottom: 2px solid #cbd5e1;\n",
        "    }\n",
        "    .styled-table th {\n",
        "        padding: 16px;\n",
        "        text-align: left;\n",
        "        font-weight: 600;\n",
        "        color: #334155;\n",
        "        width: 25%;\n",
        "    }\n",
        "    .styled-table td {\n",
        "        padding: 16px;\n",
        "        color: #475569;\n",
        "        border-bottom: 1px solid #e2e8f0;\n",
        "    }\n",
        "    .styled-table tbody tr:nth-child(even) {\n",
        "        background-color: #f8fafc;\n",
        "    }\n",
        "    .styled-table tbody tr:hover {\n",
        "        background-color: #dbeafe;\n",
        "        transition: background-color 0.2s;\n",
        "    }\n",
        "    .styled-table tbody tr td:first-child {\n",
        "        font-weight: 500;\n",
        "        color: #1e293b;\n",
        "    }\n",
        "    .table-footer {\n",
        "        background-color: #f8fafc;\n",
        "        padding: 20px;\n",
        "        border-top: 1px solid #e2e8f0;\n",
        "    }\n",
        "    .footer-grid {\n",
        "        display: grid;\n",
        "        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n",
        "        gap: 16px;\n",
        "        font-size: 14px;\n",
        "    }\n",
        "    .footer-item {\n",
        "        display: flex;\n",
        "        gap: 8px;\n",
        "        color: #1e40af;  /* Bleu foncÃ© pour tout le texte */\n",
        "    }\n",
        "    .footer-item strong {\n",
        "        color: #1e3a8a;  /* Bleu foncÃ© */\n",
        "    }\n",
        "    .footer-item span:first-child {\n",
        "        font-size: 18px;\n",
        "    }\n",
        "</style>\n",
        "\n",
        "<div class=\"comparison-table\">\n",
        "    <div class=\"table-header\">\n",
        "        <h1>Tableau Comparatif: Fine-Tuning vs Adapters/LoRA vs RAG</h1>\n",
        "    </div>\n",
        "\n",
        "    <table class=\"styled-table\">\n",
        "        <thead>\n",
        "            <tr>\n",
        "                <th>Aspect</th>\n",
        "                <th>Fine-Tuning</th>\n",
        "                <th>Adapters/LoRA</th>\n",
        "                <th>RAG</th>\n",
        "            </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "\"\"\"\n",
        "\n",
        "# Ajouter les lignes du tableau\n",
        "for _, row in df.iterrows():\n",
        "    html += f\"\"\"\n",
        "            <tr>\n",
        "                <td>{row['Aspect']}</td>\n",
        "                <td>{row['Fine-Tuning']}</td>\n",
        "                <td>{row['Adapters/LoRA']}</td>\n",
        "                <td>{row['RAG']}</td>\n",
        "            </tr>\n",
        "\"\"\"\n",
        "\n",
        "html += \"\"\"\n",
        "        </tbody>\n",
        "    </table>\n",
        "\n",
        "    <div class=\"table-footer\">\n",
        "        <div class=\"footer-grid\">\n",
        "            <div class=\"footer-item\">\n",
        "                <span>ğŸ’¡</span>\n",
        "                <div><strong>Fine-Tuning:</strong> Meilleure prÃ©cision mais coÃ»teux</div>\n",
        "            </div>\n",
        "            <div class=\"footer-item\">\n",
        "                <span>ğŸ’¡</span>\n",
        "                <div><strong>Adapters/LoRA:</strong> Bon compromis coÃ»t/performance</div>\n",
        "            </div>\n",
        "            <div class=\"footer-item\">\n",
        "                <span>ğŸ’¡</span>\n",
        "                <div><strong>RAG:</strong> DonnÃ©es fraÃ®ches, faible complexitÃ©</div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "# Afficher le tableau stylisÃ©\n",
        "display(HTML(html))"
      ],
      "metadata": {
        "id": "PKmcHDrbA48S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diGmb8p2nlxS"
      },
      "source": [
        "### Tableau DÃ©taillÃ© des Cas d'Usage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "# Create the DataFrame\n",
        "df_usecases = pd.DataFrame({\n",
        "    'Method': [\n",
        "        'Fine-Tuning',\n",
        "        'Adapters (LoRA)',\n",
        "        'RAG'\n",
        "    ],\n",
        "    'Pros': [\n",
        "        'âœ“ Highly accurate\\nâœ“ Baked-in knowledge\\nâœ“ Very fast inference',\n",
        "        'âœ“ Cheap fine-tuning\\nâœ“ Modular/multi-domain\\nâœ“ Fast training',\n",
        "        'âœ“ Flexible\\nâœ“ Real-time updates\\nâœ“ No retraining'\n",
        "    ],\n",
        "    'Cons': [\n",
        "        'âœ— Expensive\\nâœ— Rigid (hard to update)\\nâœ— Retrain per domain',\n",
        "        'âœ— Still needs training\\nâœ— Slightly lower accuracy\\nâœ— Framework dependent',\n",
        "        'âœ— Retriever quality matters\\nâœ— Latency (retrieval step)\\nâœ— Context window limits'\n",
        "    ],\n",
        "    'Best Use Case': [\n",
        "        'Applications Ã  domaine restreint (par exemple : chatbot mÃ©dical)',\n",
        "        'Adaptation au domaine (tÃ¢ches spÃ©cialisÃ©es multiples)',\n",
        "        'Connaissances dynamiques (actualitÃ©s, FAQ, donnÃ©es en temps rÃ©el)'\n",
        "    ]\n",
        "})\n",
        "\n",
        "\n",
        "html_table = \"\"\"\n",
        "<table style=\"width:100%; border-collapse: collapse; font-family: monospace;\">\n",
        "<tr style=\"background-color: #aaaaaa;\">\n",
        "    <th style=\"border: 1px solid #ddd; padding: 16px; color: Black; text-align: left;\">MÃ©thode</th>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 16px; color: black; text-align: left;\">Avantages</th>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 16px; color: black; text-align: left;\">InconvÃ©nients</th>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 16px; color: black; text-align: left;\">Cas d'usage</th>\n",
        "</tr>\n",
        "\"\"\"\n",
        "\n",
        "for idx, row in df_usecases.iterrows():\n",
        "    pros_html = \"<br>\".join(row['Pros'].split('\\n'))\n",
        "    cons_html = \"<br>\".join(row['Cons'].split('\\n'))\n",
        "    use_html = \"<br>\".join(row['Best Use Case'].split('\\n'))\n",
        "\n",
        "    html_table += f\"\"\"\n",
        "<tr>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; color: white; font-weight: bold;\">{row['Method']}</td>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; color: green;\">{pros_html}</td>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; color: red;\">{cons_html}</td>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px;\">{use_html}</td>\n",
        "</tr>\n",
        "\"\"\"\n",
        "\n",
        "html_table += \"</table>\"\n",
        "display(HTML(html_table))\n"
      ],
      "metadata": {
        "id": "Oscs-aztCvVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGiC4SfWnlxT"
      },
      "source": [
        "## RÃ©sumÃ© et Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWh7Oau8nlxT"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘              LAB03 - CONCLUSIONS & KEY TAKEAWAYS                              â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "ğŸ¯ TROIS STRATÃ‰GIES POUR ADAPTER LES LLMs Ã€ UN DOMAINE:\n",
        "\n",
        "1ï¸âƒ£  FINE-TUNING\n",
        "   â””â”€ EntraÃ®ner le modÃ¨le sur de nouvelles donnÃ©es\n",
        "   â””â”€ âœ… TrÃ¨s prÃ©cis | âŒ CoÃ»teux, rigide\n",
        "   â””â”€ ğŸ’¡ Meilleur pour: Applications de niche\n",
        "\n",
        "2ï¸âƒ£  ADAPTERS / LoRA\n",
        "   â””â”€ EntraÃ®ner seulement de petits modules paramÃ©triques\n",
        "   â””â”€ âœ… Ã‰conomique, modulaire | âŒ NÃ©cessite toujours du training\n",
        "   â””â”€ ğŸ’¡ Meilleur pour: Adaptation multi-domaine\n",
        "\n",
        "3ï¸âƒ£  RAG (Retrieval-Augmented Generation)\n",
        "   â””â”€ RÃ©cupÃ©rer des connaissances externes en temps rÃ©el\n",
        "   â””â”€ âœ… Flexible, mises Ã  jour faciles | âŒ DÃ©pend de la qualitÃ© du retriever\n",
        "   â””â”€ ğŸ’¡ Meilleur pour: Connaissances dynamiques, agents IA\n",
        "\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                    ğŸš€ RECOMMANDATION POUR LES AGENTS IA                       â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "Pour les Agentic AI systems (LangChain, CrewAI, etc.):\n",
        "\n",
        "   â¡ï¸  RAG est souvent le GO-TO car:\n",
        "      â€¢ Les agents ont besoin de connaissances Ã  jour\n",
        "      â€¢ Les tools et l'itÃ©ration demandent de la flexibilitÃ©\n",
        "      â€¢ Les coÃ»ts sont raisonnables vs fine-tuning\n",
        "\n",
        "   â¡ï¸  Combinaison optimale:\n",
        "      â€¢ RAG pour les connaissances mÃ©tier dynamiques\n",
        "      â€¢ Fine-tuning/Adapters pour le format/style du modÃ¨le\n",
        "      â€¢ Tool calling pour les actions externes\n",
        "\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                         âœ… NEXT STEPS                                         â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "1. Pratiquez RAG avec votre propre base de connaissances\n",
        "2. Explorez les frameworks (LangChain, Llama Index, etc.)\n",
        "3. Optimisez le retriever avec diffÃ©rentes embeddings\n",
        "4. Testez les agents IA intÃ©grant ces stratÃ©gies\n",
        "5. Mesurez la qualitÃ© des rÃ©ponses (eval)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ‰ LAB03 COMPLETE!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogPnvje6nlxT"
      },
      "source": [
        "## Resources SupplÃ©mentaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thW41_X9nlxT"
      },
      "outputs": [],
      "source": [
        "resources = \"\"\"\n",
        "ğŸ“š RESSOURCES RECOMMANDÃ‰ES:\n",
        "\n",
        "RAG & LangChain:\n",
        "  â€¢ https://python.langchain.com/\n",
        "  â€¢ https://js.langchain.com/\n",
        "  â€¢ RAG Best Practices: https://docs.llamaindex.ai/\n",
        "\n",
        "Fine-Tuning & LoRA:\n",
        "  â€¢ HuggingFace Fine-Tuning: https://huggingface.co/docs/transformers/training\n",
        "  â€¢ PEFT (LoRA): https://github.com/huggingface/peft\n",
        "  â€¢ OpenAI Fine-Tuning: https://platform.openai.com/docs/guides/fine-tuning\n",
        "\n",
        "Vector Databases:\n",
        "  â€¢ Chroma: https://www.trychroma.com/\n",
        "  â€¢ Pinecone: https://www.pinecone.io/\n",
        "  â€¢ Weaviate: https://weaviate.io/\n",
        "\n",
        "Frameworks Agentic AI:\n",
        "  â€¢ LangChain: https://python.langchain.com/\n",
        "  â€¢ CrewAI: https://www.crewai.com/\n",
        "  â€¢ Llama Index: https://www.llamaindex.ai/\n",
        "\n",
        "Ã‰valuation des systÃ¨mes RAG:\n",
        "  â€¢ RAGAS: https://github.com/explodinggradients/ragas\n",
        "  â€¢ DeepEval: https://www.deepeval.com/\n",
        "\"\"\"\n",
        "print(resources)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}