{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/RAG/blob/main/lab01_visualisation_de_la_tokenisation_et_des_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-GS-JotvfUK"
      },
      "source": [
        "---\n",
        "# Lab 1: Visualisation de la tokenisation et des embeddings\n",
        "---\n",
        "\n",
        "**Objectif:**\n",
        "- Comprendre comment un texte est segmentÃ© en tokens et converti en vecteurs numÃ©riques(embeddings).\n",
        "- Apprendre Ã  visualiser et interprÃ©ter ces structures.\n",
        "\n",
        "**DurÃ©e estimÃ©e:**\n",
        "- 90â€“120 minutes\n",
        "\n",
        "**Livrable :**\n",
        "- Notebook avec graphiques prÃ©sentant la segmentation en tokens et une visualisation 2D de vecteurs numÃ©riques.\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIEgIbxrvfUN"
      },
      "source": [
        "## Step 1: PrÃ©-requis (5 min)\n",
        "\n",
        "Installation des librairies nÃ©cessaires :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tCH_RXrvfUN"
      },
      "outputs": [],
      "source": [
        "!pip install openai tiktoken matplotlib scikit-learn python-dotenv -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV1D8fCnvfUO"
      },
      "source": [
        "### Configuration de la clÃ© \"OpenAI API Key\" :\n",
        "\n",
        "Puisque nous faisons ce notebook dans Google Colab, nos allons utiliser `userdata.get()` pour rÃ©cupÃ©rer l'API key en toute sÃ©curitÃ© :\n",
        "\n",
        "Si cela n'a pas dÃ©jÃ  Ã©tÃ© fait :\n",
        "- Ouvre Google Colab â†’ File â†’ Upload notebook\n",
        "- Ajoute ta clÃ© API OpenAI :\n",
        "  - Clique sur ðŸ”‘ (Secrets Manager)\n",
        "  - Puis sur \"+ Ajouter un secret\"\n",
        "  - Ajoute OPENAI_API_KEY et la valeur associÃ©e\n",
        "  - Veiile Ã  rendre le secret accessible depuis le notebook ( active \"accÃ¨s depuis le notebook\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aEBqZgEvfUP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get API key from Colab Secrets (add it in the Secrets manager: ðŸ”‘ icon on left panel)\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Or if running locally with .env file:\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "\n",
        "print(\"âœ“ API Key configured successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zsR2w_HvfUP"
      },
      "source": [
        "---\n",
        "## Step 2: Inspect Tokenization (15 min)\n",
        "\n",
        "Use tiktoken (OpenAI's tokenizer) to break text into tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgw-boT4vfUP"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "# Initialize tokenizer for GPT-4o mini\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"Agentic AI agents can plan, reason, and use tools.\"\n",
        "\n",
        "# Tokenize\n",
        "tokens = enc.encode(sample_text)\n",
        "\n",
        "# Display results\n",
        "print(\"=\"*70)\n",
        "print(f\"Original text: {sample_text}\")\n",
        "print(f\"Number of tokens: {len(tokens)}\")\n",
        "print(f\"Token IDs: {tokens}\")\n",
        "print(\"\\nDecoded tokens (individual):\")\n",
        "for i, token_id in enumerate(tokens, 1):\n",
        "    decoded = enc.decode([token_id])\n",
        "    print(f\"  {i}. ID {token_id}: '{decoded}'\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGg45DLMvfUP"
      },
      "source": [
        "### Experiment with Different Text Types\n",
        "\n",
        "Observe how punctuation, spaces, emojis, and special characters affect tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykU9428wvfUQ"
      },
      "outputs": [],
      "source": [
        "test_texts = [\n",
        "    \"Hello, World!\",\n",
        "    \"HelloWorld\",\n",
        "    \"hello world\",\n",
        "    \"AI is ðŸ”¥\",\n",
        "    \"def my_function():\\n    return True\",\n",
        "    \"AGENTIC\",\n",
        "    \"agentic\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TOKENIZATION ANALYSIS ACROSS DIFFERENT TEXT TYPES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for text in test_texts:\n",
        "    tokens = enc.encode(text)\n",
        "    decoded = [enc.decode([t]) for t in tokens]\n",
        "    print(f\"\\nText: {repr(text)}\")\n",
        "    print(f\"Tokens: {len(tokens)} â†’ {tokens}\")\n",
        "    print(f\"Decoded: {decoded}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnX7tXjVvfUQ"
      },
      "source": [
        "---\n",
        "## Step 3: Compare Token Length Across Texts (10 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM6rOl2vvfUQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentences = [\n",
        "    \"AI is amazing.\",\n",
        "    \"Artificial Intelligence is amazing.\",\n",
        "    \"AI is ðŸ”¥.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\",\n",
        "]\n",
        "\n",
        "# Collect data\n",
        "data = []\n",
        "for sentence in sentences:\n",
        "    token_list = enc.encode(sentence)\n",
        "    data.append({\n",
        "        \"Text\": sentence,\n",
        "        \"Character Count\": len(sentence),\n",
        "        \"Token Count\": len(token_list),\n",
        "        \"Ratio (Chars/Token)\": len(sentence) / len(token_list)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"TOKEN LENGTH COMPARISON\")\n",
        "print(\"=\"*100)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\"*100)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart: Character vs Token count\n",
        "x = range(len(df))\n",
        "axes[0].bar([i - 0.2 for i in x], df['Character Count'], width=0.4, label='Characters', color='steelblue')\n",
        "axes[0].bar([i + 0.2 for i in x], df['Token Count'], width=0.4, label='Tokens', color='coral')\n",
        "axes[0].set_xlabel('Sentence Index', fontsize=11, fontweight='bold')\n",
        "axes[0].set_ylabel('Count', fontsize=11, fontweight='bold')\n",
        "axes[0].set_title('Character Count vs Token Count', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Line chart: Char/Token ratio\n",
        "axes[1].plot(x, df['Ratio (Chars/Token)'], marker='o', linewidth=2, markersize=8, color='green')\n",
        "axes[1].set_xlabel('Sentence Index', fontsize=11, fontweight='bold')\n",
        "axes[1].set_ylabel('Ratio (Chars/Token)', fontsize=11, fontweight='bold')\n",
        "axes[1].set_title('Character-to-Token Ratio', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ’¡ Insight: Emojis and special symbols tend to use more tokens per character!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfhZjB3jvfUQ"
      },
      "source": [
        "---\n",
        "## Step 4: Generate Embeddings (20 min)\n",
        "\n",
        "Use OpenAI's `text-embedding-3-small` model to create numerical vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oF3c5N5vfUR"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Texts to embed\n",
        "texts = [\n",
        "    \"Agentic AI\",\n",
        "    \"Autonomous agents\",\n",
        "    \"Bananas are yellow\",\n",
        "    \"Machine learning models\",\n",
        "    \"Fruit is delicious\",\n",
        "]\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING EMBEDDINGS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for text in texts:\n",
        "    resp = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=text\n",
        "    )\n",
        "    vector = resp.data[0].embedding\n",
        "    embeddings.append(vector)\n",
        "    print(f\"âœ“ '{text}' â†’ Vector length: {len(vector)}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nðŸ“Š Embedding Dimension: {len(embeddings[0])}\")\n",
        "print(f\"Number of texts embedded: {len(embeddings)}\")\n",
        "\n",
        "# Show a sample embedding (first 20 values)\n",
        "print(f\"\\nSample embedding (first 20 values) for '{texts[0]}':\")\n",
        "print(embeddings[0][:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFM2sJJbvfUR"
      },
      "source": [
        "---\n",
        "## Step 5: Dimensionality Reduction & Plotting (25 min)\n",
        "\n",
        "Reduce 1536-dimensional embeddings to 2D for visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUPXAUMVvfUR"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Apply PCA to reduce to 2D\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Calculate explained variance\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "total_var = sum(explained_var)\n",
        "\n",
        "print(f\"\\nðŸ“ˆ PCA Explained Variance:\")\n",
        "print(f\"  PC1: {explained_var[0]:.4f} ({explained_var[0]*100:.2f}%)\")\n",
        "print(f\"  PC2: {explained_var[1]:.4f} ({explained_var[1]*100:.2f}%)\")\n",
        "print(f\"  Total: {total_var:.4f} ({total_var*100:.2f}%)\")\n",
        "\n",
        "# Create visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
        "\n",
        "for i, (txt, color) in enumerate(zip(texts, colors)):\n",
        "    x, y = embeddings_2d[i]\n",
        "    plt.scatter(x, y, s=300, alpha=0.7, color=color, edgecolors='black', linewidth=2)\n",
        "    plt.text(x + 0.05, y + 0.05, txt, fontsize=10, fontweight='bold',\n",
        "             bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.3))\n",
        "\n",
        "plt.xlabel(f'PC1 ({explained_var[0]*100:.1f}%)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel(f'PC2 ({explained_var[1]*100:.1f}%)', fontsize=12, fontweight='bold')\n",
        "plt.title('Embedding Visualization (PCA 2D)', fontsize=14, fontweight='bold')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.axhline(y=0, color='k', linewidth=0.5)\n",
        "plt.axvline(x=0, color='k', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ’¡ Observation: Semantically similar texts should be closer together!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLk3USSrvfUR"
      },
      "source": [
        "### Interactive Embedding Explorer\n",
        "\n",
        "Add more sentences and visualize clusters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JezWUZbrvfUR"
      },
      "outputs": [],
      "source": [
        "# Expand with more related texts\n",
        "extended_texts = texts + [\n",
        "    \"AI agents with reasoning\",\n",
        "    \"Yellow banana fruit\",\n",
        "    \"Deep learning neural networks\",\n",
        "    \"Apples are red\",\n",
        "]\n",
        "\n",
        "# Generate embeddings for new texts\n",
        "extended_embeddings = []\n",
        "\n",
        "print(\"\\nGenerating embeddings for extended set...\")\n",
        "for text in extended_texts:\n",
        "    resp = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=text\n",
        "    )\n",
        "    extended_embeddings.append(resp.data[0].embedding)\n",
        "\n",
        "# Apply PCA\n",
        "pca_ext = PCA(n_components=2)\n",
        "embeddings_2d_ext = pca_ext.fit_transform(extended_embeddings)\n",
        "\n",
        "# Categorize texts\n",
        "categories = {\n",
        "    'AI/Agents': [0, 1, 3, 5, 6],\n",
        "    'Food/Fruit': [2, 4, 7, 8],\n",
        "}\n",
        "\n",
        "category_colors = {'AI/Agents': '#FF6B6B', 'Food/Fruit': '#4ECDC4'}\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(12, 9))\n",
        "\n",
        "for category, indices in categories.items():\n",
        "    for idx in indices:\n",
        "        x, y = embeddings_2d_ext[idx]\n",
        "        ax.scatter(x, y, s=400, alpha=0.7, color=category_colors[category],\n",
        "                  edgecolors='black', linewidth=2, label=category if idx == indices[0] else '')\n",
        "        ax.text(x + 0.08, y + 0.08, extended_texts[idx], fontsize=9, fontweight='bold',\n",
        "               bbox=dict(boxstyle='round,pad=0.3', facecolor=category_colors[category], alpha=0.3))\n",
        "\n",
        "ax.set_xlabel(f'PC1 ({pca_ext.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel(f'PC2 ({pca_ext.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Extended Embedding Visualization - Semantic Clusters', fontsize=14, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "ax.axhline(y=0, color='k', linewidth=0.5)\n",
        "ax.axvline(x=0, color='k', linewidth=0.5)\n",
        "\n",
        "# Remove duplicate labels\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "by_label = dict(zip(labels, handles))\n",
        "ax.legend(by_label.values(), by_label.keys(), fontsize=11, loc='best')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKbhp1ktvfUR"
      },
      "source": [
        "---\n",
        "## Step 6: Experiment with Semantic Similarity (15 min)\n",
        "\n",
        "Compute cosine similarity between embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bXQKzBCvfUR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Use original 5 texts for clarity\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "similarity_df = pd.DataFrame(\n",
        "    similarity_matrix,\n",
        "    index=texts,\n",
        "    columns=texts\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"COSINE SIMILARITY MATRIX\")\n",
        "print(\"=\"*100)\n",
        "print(similarity_df.round(4))\n",
        "print(\"=\"*100)\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm', vmin=0, vmax=1)\n",
        "\n",
        "ax.set_xticks(range(len(texts)))\n",
        "ax.set_yticks(range(len(texts)))\n",
        "ax.set_xticklabels(texts, rotation=45, ha='right')\n",
        "ax.set_yticklabels(texts)\n",
        "\n",
        "# Add values to heatmap\n",
        "for i in range(len(texts)):\n",
        "    for j in range(len(texts)):\n",
        "        text = ax.text(j, i, f'{similarity_matrix[i, j]:.3f}',\n",
        "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "\n",
        "ax.set_title('Cosine Similarity Between Embeddings', fontsize=14, fontweight='bold', pad=20)\n",
        "cbar = plt.colorbar(im, ax=ax)\n",
        "cbar.set_label('Similarity Score', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find most and least similar pairs\n",
        "print(\"\\nðŸ” Key Similarities:\")\n",
        "similarities_list = []\n",
        "for i in range(len(texts)):\n",
        "    for j in range(i+1, len(texts)):\n",
        "        similarities_list.append((texts[i], texts[j], similarity_matrix[i, j]))\n",
        "\n",
        "similarities_list.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(\"\\n  ðŸ† Most Similar:\")\n",
        "for text1, text2, score in similarities_list[:3]:\n",
        "    print(f\"    '{text1}' â†” '{text2}': {score:.4f}\")\n",
        "\n",
        "print(\"\\n  â„ï¸  Least Similar:\")\n",
        "for text1, text2, score in similarities_list[-3:]:\n",
        "    print(f\"    '{text1}' â†” '{text2}': {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKnc4iKuvfUS"
      },
      "source": [
        "---\n",
        "## Step 7: Mini-Project - Token & Embedding Explorer (15â€“20 min)\n",
        "\n",
        "Build a reusable function to analyze any text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAc7-059vfUS"
      },
      "outputs": [],
      "source": [
        "class TextAnalyzer:\n",
        "    \"\"\"Comprehensive text tokenization and embedding analyzer.\"\"\"\n",
        "\n",
        "    def __init__(self, reference_text=None):\n",
        "        self.enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "        self.client = OpenAI()\n",
        "        self.reference_text = reference_text\n",
        "        self.reference_embedding = None\n",
        "\n",
        "        if reference_text:\n",
        "            resp = self.client.embeddings.create(\n",
        "                model=\"text-embedding-3-small\",\n",
        "                input=reference_text\n",
        "            )\n",
        "            self.reference_embedding = resp.data[0].embedding\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize text and return tokens with decoded values.\"\"\"\n",
        "        token_ids = self.enc.encode(text)\n",
        "        token_strs = [self.enc.decode([t]) for t in token_ids]\n",
        "        return token_ids, token_strs\n",
        "\n",
        "    def embed(self, text):\n",
        "        \"\"\"Generate embedding for text.\"\"\"\n",
        "        resp = self.client.embeddings.create(\n",
        "            model=\"text-embedding-3-small\",\n",
        "            input=text\n",
        "        )\n",
        "        return resp.data[0].embedding\n",
        "\n",
        "    def analyze(self, text):\n",
        "        \"\"\"Comprehensive analysis of input text.\"\"\"\n",
        "        token_ids, token_strs = self.tokenize(text)\n",
        "        embedding = self.embed(text)\n",
        "\n",
        "        result = {\n",
        "            'text': text,\n",
        "            'char_count': len(text),\n",
        "            'token_count': len(token_ids),\n",
        "            'tokens': list(zip(token_ids, token_strs)),\n",
        "            'embedding': embedding,\n",
        "            'embedding_dim': len(embedding)\n",
        "        }\n",
        "\n",
        "        # Compute similarity to reference if available\n",
        "        if self.reference_embedding:\n",
        "            similarity = cosine_similarity(\n",
        "                [embedding],\n",
        "                [self.reference_embedding]\n",
        "            )[0][0]\n",
        "            result['similarity_to_reference'] = similarity\n",
        "\n",
        "        return result\n",
        "\n",
        "    def display_analysis(self, text):\n",
        "        \"\"\"Print formatted analysis.\"\"\"\n",
        "        analysis = self.analyze(text)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"TEXT: '{analysis['text']}'\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Character Count: {analysis['char_count']}\")\n",
        "        print(f\"Token Count: {analysis['token_count']}\")\n",
        "        print(f\"Embedding Dimension: {analysis['embedding_dim']}\")\n",
        "\n",
        "        if 'similarity_to_reference' in analysis:\n",
        "            print(f\"\\nSimilarity to Reference ('{self.reference_text}'): {analysis['similarity_to_reference']:.4f}\")\n",
        "\n",
        "        print(\"\\nTokens:\")\n",
        "        for i, (token_id, token_str) in enumerate(analysis['tokens'], 1):\n",
        "            print(f\"  {i:2d}. ID {token_id:5d} â†’ '{token_str}'\")\n",
        "\n",
        "        print(f\"\\nEmbedding Preview (first 10 values): {analysis['embedding'][:10]}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "# Initialize analyzer with a reference text\n",
        "analyzer = TextAnalyzer(reference_text=\"AI and agents\")\n",
        "\n",
        "# Test with different texts\n",
        "test_sentences = [\n",
        "    \"Agentic AI systems are intelligent.\",\n",
        "    \"Machine learning requires data.\",\n",
        "    \"I love bananas ðŸŒ\",\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    analyzer.display_analysis(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4TAzF_FvfUS"
      },
      "source": [
        "### Test with Your Own Texts\n",
        "\n",
        "Try adding your custom sentences below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGcUWZR6vfUS"
      },
      "outputs": [],
      "source": [
        "# Add your own sentences here for analysis!\n",
        "custom_texts = [\n",
        "    \"Your sentence here\",\n",
        "    \"Another sentence\",\n",
        "    \"Add as many as you want\",\n",
        "]\n",
        "\n",
        "for text in custom_texts:\n",
        "    analyzer.display_analysis(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeKKwvFIvfUS"
      },
      "source": [
        "---\n",
        "## Step 8: Advanced Visualization - Similarity Network\n",
        "\n",
        "Create a network graph showing similarity relationships:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84U_PfL8vfUS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "import numpy as np\n",
        "\n",
        "# Use extended_texts from earlier\n",
        "similarity_matrix_ext = cosine_similarity(extended_embeddings)\n",
        "\n",
        "# Create network visualization\n",
        "fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "# Use 2D embedding positions as node positions\n",
        "positions = embeddings_2d_ext\n",
        "\n",
        "# Draw edges (similarity links) - only if similarity > threshold\n",
        "threshold = 0.7\n",
        "for i in range(len(extended_texts)):\n",
        "    for j in range(i+1, len(extended_texts)):\n",
        "        sim = similarity_matrix_ext[i, j]\n",
        "        if sim > threshold:\n",
        "            x1, y1 = positions[i]\n",
        "            x2, y2 = positions[j]\n",
        "            # Line opacity and width based on similarity\n",
        "            alpha = (sim - threshold) / (1 - threshold) * 0.8\n",
        "            width = 1 + (sim - threshold) / (1 - threshold) * 3\n",
        "            ax.plot([x1, x2], [y1, y2], 'k-', alpha=alpha, linewidth=width)\n",
        "\n",
        "# Draw nodes\n",
        "for idx, (text, (x, y)) in enumerate(zip(extended_texts, positions)):\n",
        "    category = 'AI/Agents' if idx in categories['AI/Agents'] else 'Food/Fruit'\n",
        "    color = category_colors[category]\n",
        "    ax.scatter(x, y, s=500, alpha=0.8, color=color, edgecolors='black', linewidth=2, zorder=3)\n",
        "    ax.text(x, y, str(idx), ha='center', va='center', fontsize=9, fontweight='bold', zorder=4)\n",
        "\n",
        "# Add legend with text labels\n",
        "ax.text(0.02, 0.98, 'Node Labels:', transform=ax.transAxes, fontsize=11, fontweight='bold',\n",
        "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "label_text = '\\n'.join([f\"{i}: {text}\" for i, text in enumerate(extended_texts)])\n",
        "ax.text(0.02, 0.92, label_text, transform=ax.transAxes, fontsize=8, verticalalignment='top',\n",
        "        family='monospace', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.3))\n",
        "\n",
        "# Add category legend\n",
        "ai_patch = mpatches.Patch(color='#FF6B6B', label='AI/Agents', edgecolor='black')\n",
        "food_patch = mpatches.Patch(color='#4ECDC4', label='Food/Fruit', edgecolor='black')\n",
        "ax.legend(handles=[ai_patch, food_patch], loc='upper right', fontsize=11, framealpha=0.9)\n",
        "\n",
        "ax.set_xlabel(f'PC1 ({pca_ext.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel(f'PC2 ({pca_ext.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title(f'Semantic Similarity Network (threshold > {threshold})', fontsize=14, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "ax.axhline(y=0, color='k', linewidth=0.5)\n",
        "ax.axvline(x=0, color='k', linewidth=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nðŸ“Š Network Statistics (Threshold: {threshold}):\")\n",
        "connection_count = 0\n",
        "for i in range(len(extended_texts)):\n",
        "    for j in range(i+1, len(extended_texts)):\n",
        "        if similarity_matrix_ext[i, j] > threshold:\n",
        "            connection_count += 1\n",
        "            print(f\"  [{i}] '{extended_texts[i]}' â†” [{j}] '{extended_texts[j]}': {similarity_matrix_ext[i, j]:.4f}\")\n",
        "\n",
        "print(f\"\\nTotal connections: {connection_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxEuhYEjvfUT"
      },
      "source": [
        "---\n",
        "## Summary & Key Takeaways\n",
        "\n",
        "### What We Learned:\n",
        "\n",
        "1. **Tokenization** breaks text into discrete units that models understand\n",
        "   - Different texts have different token-to-character ratios\n",
        "   - Emojis and special characters use more tokens\n",
        "\n",
        "2. **Embeddings** convert tokens into high-dimensional numerical vectors\n",
        "   - OpenAI's `text-embedding-3-small` produces 1536-dimensional vectors\n",
        "   - These vectors capture semantic meaning\n",
        "\n",
        "3. **Dimensionality Reduction** (PCA) allows us to visualize embeddings\n",
        "   - We can reduce 1536D â†’ 2D while preserving semantic relationships\n",
        "   - Semantically similar texts cluster together\n",
        "\n",
        "4. **Cosine Similarity** measures semantic closeness\n",
        "   - Scores range from 0 (opposite) to 1 (identical)\n",
        "   - Used in RAG, semantic search, and LLM applications\n",
        "\n",
        "### Applications:\n",
        "\n",
        "- **RAG (Retrieval-Augmented Generation)**: Find relevant documents by embedding similarity\n",
        "- **Semantic Search**: Find similar texts without keyword matching\n",
        "- **Clustering**: Group semantically similar documents\n",
        "- **Recommendation Systems**: Suggest similar items based on embeddings\n",
        "- **LLM Reasoning**: Models use embeddings internally for reasoning\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Build a simple RAG pipeline with vector databases (Pinecone, Weaviate)\n",
        "- Experiment with different embedding models\n",
        "- Create semantic search applications\n",
        "- Explore advanced tokenization for multilingual texts\n",
        "\n",
        "---\n",
        "\n",
        "**ðŸŽ“ Lab Complete! You now understand how text becomes numbers in LLMs.**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}