{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/RAG/blob/main/LAB23_M%C3%A9moires_Short_Term_et_Long_Term.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckdbCGm_9FBh"
      },
      "source": [
        "# LAB23 : construction d'agents avec m√©moires \"Short_Term\" et \"Long_Term\"\n",
        "\n",
        "## Objectifs :\n",
        "Impl√©mentation de 2 types de m√©moire pour les agents IA :\n",
        "- **Short-term memory** ‚Üí context window (messages r√©cents)\n",
        "- **Long-term memory** ‚Üí knowledge base persistante (Vector DB)\n",
        "\n",
        "## Temps estim√© :\n",
        "- 20‚Äì30 minutes\n",
        "\n",
        "**Livrables :**\n",
        "- Notebook montrant comment un agent rappelle les conversations r√©centes et r√©cup√®re des faits stock√©s en m√©moire long terme."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY5iIp7b9FBj"
      },
      "source": [
        "## Step 1: Setup (5 min)\n",
        "\n",
        "Installation des pr√©requis et d√©pendances n√©cessaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEDykuf-9FBj"
      },
      "outputs": [],
      "source": [
        "# Installation des d√©pendances\n",
        "!pip install -q openai langchain chromadb python-dotenv\n",
        "!pip install -q -U langchain-chroma langchain-huggingface langchain-core langchain-openai\n",
        "!pip install -q datasets\n",
        "!pip install -q transformers\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain-community\n",
        "\n",
        "\n",
        "print(\"‚úì D√©pendances install√©es avec succ√®s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IacpFfDb9FBk"
      },
      "source": [
        "### Configuration de l'API OpenAI\n",
        "\n",
        "Dans Google Colab, vous pouvez d√©finir votre cl√© API de deux fa√ßons :\n",
        "1. **M√©thode s√©curis√©e (recommand√©e)** : Utiliser `google.colab.userdata`\n",
        "2. **M√©thode alternative** : D√©finir directement en variable d'environnement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6mO9h7k9FBk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# R√©cup√©rer la cl√© API depuis les secrets Colab\n",
        "# Pour ajouter : cliquez sur üîë dans le panneau de gauche\n",
        "try:\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "    print(\"‚úì Cl√© API OpenAI charg√©e depuis les secrets Colab\")\n",
        "except:\n",
        "    print(\"‚ö† Secrets Colab non configur√©s. Veuillez ajouter OPENAI_API_KEY.\")\n",
        "    print(\"Instructions : Cliquez sur üîë dans le panneau gauche > Ajouter un nouveau secret\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQw8aTJb9FBk"
      },
      "source": [
        "## Step 2: Short-Term Memory (Context Window) (5 min)\n",
        "\n",
        "La m√©moire court-terme consiste √† maintenir l'historique de conversation dans le prompt.\n",
        "\n",
        "### Concept cl√© :\n",
        "- Sans contexte ‚Üí le mod√®le \"oublie\"\n",
        "- Avec historique ‚Üí il peut se rappeler des informations pr√©c√©demment donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMB4jRXk9FBl"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
        "\n",
        "def chat_with_memory(messages):\n",
        "    \"\"\"\n",
        "    Fonction pour communiquer avec le mod√®le en gardant l'historique.\n",
        "\n",
        "    Args:\n",
        "        messages: Liste de dictionnaires avec 'role' et 'content'\n",
        "\n",
        "    Returns:\n",
        "        str: R√©ponse du mod√®le\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "print(\"‚úì Fonction chat_with_memory d√©finie\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NidW4mrk9FBl"
      },
      "source": [
        "### Exp√©rience 1 : Conversation avec m√©moire\n",
        "\n",
        "Simulons une conversation o√π l'agent doit se souvenir du nom de l'utilisateur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0OtPFu19FBl"
      },
      "outputs": [],
      "source": [
        "# Initialisation une conversation\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful tutor. Be concise and friendly.\"},\n",
        "    {\"role\": \"user\", \"content\": \"My name is Stephane.\"}\n",
        "]\n",
        "\n",
        "# Premier √©change\n",
        "print(\"üó£Ô∏è User: My name is Stephane.\")\n",
        "reply1 = chat_with_memory(conversation)\n",
        "print(f\"ü§ñ Assistant: {reply1}\")\n",
        "\n",
        "# Ajout de la r√©ponse √† l'historique\n",
        "conversation.append({\"role\": \"assistant\", \"content\": reply1})\n",
        "\n",
        "# Deuxi√®me message - test de m√©moire court-terme\n",
        "conversation.append({\"role\": \"user\", \"content\": \"What is my name?\"})\n",
        "print(\"\\nüó£Ô∏è User: What is my name?\")\n",
        "reply2 = chat_with_memory(conversation)\n",
        "print(f\"ü§ñ Assistant: {reply2}\")\n",
        "\n",
        "conversation.append({\"role\": \"assistant\", \"content\": reply2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZyu1Wmi9FBl"
      },
      "source": [
        "### Exp√©rience 2 : Sans contexte (oubli)\n",
        "\n",
        "Demandons le nom du utilisateur **SANS l'historique** pour constater qu'effectivement le mod√®le oublie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzL4I4lZ9FBm"
      },
      "outputs": [],
      "source": [
        "# Cr√©er une nouvelle conversation SANS contexte pr√©c√©dent\n",
        "conversation_without_context = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful tutor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
        "]\n",
        "\n",
        "print(\"üó£Ô∏è User: What is my name?\")\n",
        "print(\"\\n(Note: Sans contexte pr√©c√©dent dans la conversation)\")\n",
        "reply_without_context = chat_with_memory(conversation_without_context)\n",
        "print(f\"ü§ñ Assistant: {reply_without_context}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSE:\")\n",
        "print(f\"‚úì AVEC contexte: L'agent se souvient du nom (Stephane)\")\n",
        "print(f\"‚úó SANS contexte: L'agent ne peut pas savoir le nom\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_r67xBP9FBm"
      },
      "source": [
        "## Step 3: Long-Term Memory (Vector Store) (10 min)\n",
        "\n",
        "Persistance d'informations y compris apr√®s  la fin des sessions de conversation.\n",
        "\n",
        "### Concept cl√© :\n",
        "- Vector DB (Chroma) stocke les embeddings des documents et persiste les informations\n",
        "- Lors d'un √©change, on effectue une recherche s√©mantique : \"agent memory\" ‚Üí r√©cup√®re les documents pertinents\n",
        "-  ‚Üí Les informations sont persist√©es entre les sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlxOWroO9FBm"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "#from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Initialiser les embeddings OpenAI\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "print(\"‚úì OpenAIEmbeddings initialis√©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfLRobho9FBm"
      },
      "source": [
        "### Cr√©er une base de connaissances\n",
        "\n",
        "Nous allons stocker plusieurs faits sur les agents IA et LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGbY7ljK9FBm"
      },
      "outputs": [],
      "source": [
        "# Les 6 documents de notre base de connaissances\n",
        "knowledge_base = [\n",
        "    Document(\n",
        "        page_content=\"Agentic AI agents use tools and memory to accomplish complex tasks autonomously.\",\n",
        "        metadata={\"source\": \"agentic_ai_basics\", \"type\": \"definition\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"LangChain is a framework that helps build autonomous agents with memory, tools, and chains.\",\n",
        "        metadata={\"source\": \"langchain_intro\", \"type\": \"framework\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"RAG (Retrieval-Augmented Generation) improves accuracy by retrieving relevant knowledge before generating responses.\",\n",
        "        metadata={\"source\": \"rag_concept\", \"type\": \"technique\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Short-term memory in agents stores recent conversation context in the prompt window.\",\n",
        "        metadata={\"source\": \"agent_memory\", \"type\": \"memory_type\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Long-term memory uses vector databases to persistently store and retrieve semantic knowledge.\",\n",
        "        metadata={\"source\": \"agent_memory\", \"type\": \"memory_type\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"CrewAI orchestrates multiple AI agents to collaborate on complex workflows.\",\n",
        "        metadata={\"source\": \"crewai_framework\", \"type\": \"framework\"}\n",
        "    )\n",
        "]\n",
        "\n",
        "# On veille √† bien utiliser explicitement le nom de collection 'hanlab_long_term_memory'\n",
        "db = Chroma.from_documents(\n",
        "    documents=knowledge_base,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"hanlab_long_term_memory\",  # Nom de collection\n",
        "    persist_directory=\"./chroma_db\"  # Optionnel: persiste les donn√©es sur disque\n",
        ")\n",
        "\n",
        "print(f\"‚úì Base vectorielle cr√©√©e avec {len(knowledge_base)} documents\")\n",
        "print(\"‚úì Collection: 'hanlab_long_term_memory'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9UNdSe09FBm"
      },
      "source": [
        "### Exp√©rience 3 : Rechercher dans la m√©moire long-terme\n",
        "\n",
        "Testons une recherche s√©mantique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCIYklHn9FBn"
      },
      "outputs": [],
      "source": [
        "def recall_from_long_term_memory(query, k=2):\n",
        "    \"\"\"\n",
        "    R√©cup√®re les documents les plus pertinents de la m√©moire long-terme.\n",
        "\n",
        "    Args:\n",
        "        query: Question ou recherche √† effectuer\n",
        "        k: Nombre de documents √† retourner\n",
        "\n",
        "    Returns:\n",
        "        list: Documents pertinents\n",
        "    \"\"\"\n",
        "    results = db.similarity_search(query, k=k)\n",
        "    return results\n",
        "\n",
        "# Test de recherche\n",
        "query = \"What is agent memory?\"\n",
        "print(f\"üîç Query: {query}\\n\")\n",
        "\n",
        "results = recall_from_long_term_memory(query, k=2)\n",
        "\n",
        "print(\"R√©sultats r√©cup√©r√©s de la m√©moire long-terme:\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"\\n  [{i}] {doc.page_content}\")\n",
        "    print(f\"      (Source: {doc.metadata['source']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOPn80zg9FBn"
      },
      "source": [
        "### Exp√©rience 4 : Ajouter dynamiquement des nouveaux faits\n",
        "\n",
        "Les agents peuvent continuellement apprendre et √©tendre leur base de connaissances."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# On regarde avant enrichissement les faits r√©cup√©rables :\n",
        "query_kafka_avant = \"Tell me about data streaming platforms\"\n",
        "print(f\"üîç Query: {query_kafka_avant}\")\n",
        "print(\"\\nR√©sultats (incluant les nouveaux faits):\")\n",
        "results_kafka = recall_from_long_term_memory(query_kafka_avant, k=2)\n",
        "for i, doc in enumerate(results_kafka, 1):\n",
        "    print(f\"\\n  [{i}] {doc.page_content}\")\n",
        "    print(f\"      (Source: {doc.metadata['source']})\")"
      ],
      "metadata": {
        "id": "Qer6uI-5pfEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QyYnqN-9FBn"
      },
      "outputs": [],
      "source": [
        "# Nouveaux faits √† ajouter dynamiquement\n",
        "new_facts = [\n",
        "    Document(\n",
        "        page_content=\"Apache Kafka is a distributed streaming platform for building real-time data pipelines.\",\n",
        "        metadata={\"source\": \"kafka_platform\", \"type\": \"technology\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Prompt engineering involves carefully crafting instructions to optimize LLM outputs.\",\n",
        "        metadata={\"source\": \"prompt_engineering\", \"type\": \"technique\"}\n",
        "    )\n",
        "]\n",
        "\n",
        "# Ajouter les nouveaux documents √† la base\n",
        "db.add_documents(new_facts)\n",
        "\n",
        "print(f\"‚úì {len(new_facts)} nouveaux faits ajout√©s √† la m√©moire long-terme\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqNg9dua9FBn"
      },
      "outputs": [],
      "source": [
        "# V√©rifier que les nouveaux faits sont r√©cup√©rables\n",
        "query_kafka = \"Tell me about data streaming platforms\"\n",
        "print(f\"üîç Query: {query_kafka}\")\n",
        "print(\"\\nR√©sultats (incluant les nouveaux faits):\")\n",
        "results_kafka = recall_from_long_term_memory(query_kafka, k=2)\n",
        "for i, doc in enumerate(results_kafka, 1):\n",
        "    print(f\"\\n  [{i}] {doc.page_content}\")\n",
        "    print(f\"      (Source: {doc.metadata['source']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention aux d√©ppr√©ciations :\n",
        "\n",
        "| Aspect | RetrievalQA (Ancienne)                    | create_retrieval_chain() (Moderne) |\n",
        "| ------------ | ----------------------------------------- | ---------------------------------- |\n",
        "| Statut       |    Deprecated depuis v0.1.0python.langchain‚Äã | ‚úÖ Approche courante actuelle       |\n",
        "| Architecture | Wrapper h√©rit√©                            | LCEL natif (plus flexible)         |\n",
        "| Contr√¥le     | Limit√©                                    | Complet et composable              |\n",
        "| Maintenance  | Arr√™t√©e                                   | Active et am√©lior√©e                |\n",
        "| Performance  | OK                                        | Optimis√©e                          |"
      ],
      "metadata": {
        "id": "fMhjBqLu3tDN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smIzPk7H9FBn"
      },
      "source": [
        "## Step 4: Association de la m√©moire Short-Term et de la m√©moire Long-Term (10 min)\n",
        "\n",
        "Cela consiste √† combiner les deux syst√®mes de m√©moire : contexte conversationnel + connaissance persistante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwZFNEe59FBn"
      },
      "outputs": [],
      "source": [
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialiser le LLM et le retriever\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1.0)\n",
        "\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Cr√©ation de la cha√Æne RAG (Retrieval-Augmented Generation)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(\"‚úì Cha√Æne RetrievalQA cr√©√©e\")\n",
        "print(\"‚úì Combine short-term (contexte) + long-term (retriever) m√©moire\")\n",
        "conversation = [\n",
        "    {\"role\":\"system\",\"content\":\"You are a teaching AI agent.\"},\n",
        "    {\"role\":\"user\",\"content\":\"Remember my favorite framework is LangChain.\"},\n",
        "    {\"role\":\"assistant\",\"content\":\"Got it, your favorite framework is LangChain.\"}\n",
        "]\n",
        "print(\"\\nConversation:\", conversation)\n",
        "# User asks later\n",
        "conversation.append({\"role\":\"user\",\"content\":\"What's my favorite framework and how do agents use memory?\"})\n",
        "print(\"_______________________________________\")\n",
        "\n",
        "# La 1√®re r√©ponse utilise la m√©moire court-terme (contexte) :\n",
        "short_term_ans = chat_with_memory(conversation)\n",
        "print(\"\\nShort-term:\", short_term_ans)\n",
        "print(\"_______________________________________\")\n",
        "# La 2nde r√©ponse est enrichie grace √† la m√©moire long-terme (recherche dans vector DB) :\n",
        "# print(\"\\nLong-term:\", qa_chain.invoke({\"query\": \"How do agents use memory?\"}))\n",
        "long_term_response = qa_chain.invoke({\"query\": \"How do agents use memory?\"})\n",
        "print(\"\\nLong-term: How do agents use memory ?\\n\")\n",
        "print(long_term_response[\"result\"])\n",
        "print(\"_______________________________________\")\n",
        "# La 3√®me r√©ponse reprend la 1√®re question itiale sur le framework pr√©f√©r√© mais seulement sur la base de la recherche dans vector DB :\n",
        "# print(\"\\nLong-term:\", qa_chain.invoke({\"query\": \"How do agents use memory?\"}))\n",
        "long_term_response = qa_chain.invoke({\"query\": \"What's my favorite framework?\"})\n",
        "print(\"\\nLong-term: What's my favorite framework ?\\n\")\n",
        "print(long_term_response[\"result\"])\n",
        "print(\"_______________________________________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### On rajoute l'information dans la base vectorielle"
      ],
      "metadata": {
        "id": "5be9UY_8vy7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_preference = [\n",
        "    Document(\n",
        "        page_content=\"The user's favorite framework is LangChain because it helps build autonomous agents with memory, tools, and chains.\",\n",
        "        metadata={\"source\": \"user_preferences\", \"type\": \"user_preferences\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"The best framework is Hadoop.\",\n",
        "        metadata={\"source\": \"prompt_engineering\", \"type\": \"Fake\"}\n",
        "    )\n",
        "]\n",
        "\n",
        "# Ajouter les nouveaux documents √† la base\n",
        "db.add_documents(user_preference)\n",
        "\n",
        "print(f\"‚úì {len(new_facts)} nouveaux faits ajout√©s √† la m√©moire long-terme\")\n"
      ],
      "metadata": {
        "id": "Vgpd1l9ivx6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Et on repose la question :"
      ],
      "metadata": {
        "id": "WC6chZ_Lv-i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# La 4√®me r√©ponse reprend la 1√®re question initiale sur le framework pr√©f√©r√© toujours sur la base de la recherche dans vector DB :\n",
        "long_term_response = qa_chain.invoke({\"query\": \"What's my favorite framework?\"})\n",
        "print(\"\\nLong-term: What's my favorite framework ?\\n\")\n",
        "print(long_term_response[\"result\"])\n",
        "print(\"_______________________________________\")\n",
        "\n",
        "# La 5√®me r√©ponse interroge sur le meilleur framework :\n",
        "long_term_response = qa_chain.invoke({\"query\": \"What is the best's Framework?\"})\n",
        "print(\"\\nLong-term: What's my favorite framework ?\\n\")\n",
        "print(long_term_response[\"result\"])\n",
        "print(\"_______________________________________\")"
      ],
      "metadata": {
        "id": "XUWyvhXiwCsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to17fjna9FBo"
      },
      "source": [
        "### Exp√©rience 5 : Conversation avec hybridation m√©moire\n",
        "\n",
        "**Sc√©nario :** L'utilisateur indique quel est son framework favori (short-term), puis pose une question impliquant une connaissance long-terme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss8vULGB9FBo"
      },
      "outputs": [],
      "source": [
        "# Initialise la conversation\n",
        "hybrid_conversation = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are an expert teaching AI agent. You remember user preferences and use knowledge about AI frameworks and techniques.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"My favorite framework is LangChain because it helps build autonomous agents.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Premi√®re r√©ponse (short-term memory)\n",
        "print(\"üó£Ô∏è User: My favorite framework is LangChain because it helps build autonomous agents.\")\n",
        "reply_acknowledge = chat_with_memory(hybrid_conversation)\n",
        "print(f\"ü§ñ Assistant: {reply_acknowledge}\")\n",
        "\n",
        "hybrid_conversation.append({\"role\": \"assistant\", \"content\": reply_acknowledge})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FELVmtn99FBo"
      },
      "outputs": [],
      "source": [
        "# Question qui demande √† la fois short-term et long-term memory\n",
        "user_query = \"What's my favorite framework and how do agents use memory?\"\n",
        "hybrid_conversation.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "print(f\"\\nüó£Ô∏è User: {user_query}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"R√âPONSE 1: Utilisant SHORT-TERM MEMORY UNIQUEMENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# R√©ponse sans long-term memory (juste contexte)\n",
        "short_term_reply = chat_with_memory(hybrid_conversation)\n",
        "print(f\"\\nü§ñ Assistant:\\n{short_term_reply}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvWxBCk79FBo"
      },
      "source": [
        "### Exp√©rience 6 : Court-terme oublie, Long-terme se souvient\n",
        "\n",
        "**Scenario :** Apr√®s r√©initialisation de conversation, short-term memory oublie mais long-term memory persiste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOG4VfvU9FBo"
      },
      "outputs": [],
      "source": [
        "print(\"SCENARIO: R√©initialisation de conversation\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Conversation 1 : L'agent apprend une info\n",
        "conv_session1 = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I love Apache Kafka for real-time data processing.\"}\n",
        "]\n",
        "reply_session1 = chat_with_memory(conv_session1)\n",
        "print(f\"Session 1 - User: I love Apache Kafka for real-time data processing.\")\n",
        "print(f\"Session 1 - Assistant: {reply_session1}\\n\")\n",
        "\n",
        "# Conversation 2 : NOUVELLE CONVERSATION (conversation r√©initialis√©e)\n",
        "conv_session2 = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What do I love for data processing?\"}\n",
        "]\n",
        "reply_session2 = chat_with_memory(conv_session2)\n",
        "print(f\"Session 2 - User: What do I love for data processing?\")\n",
        "print(f\"Session 2 - Assistant: {reply_session2}\")\n",
        "print(f\"\\n‚ùå SHORT-TERM MEMORY: L'agent a OUBLI√â (conversation r√©initialis√©e)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx7j3O3_9FBo"
      },
      "source": [
        "### Exp√©rience 7 : Persistance avec long-term memory\n",
        "\n",
        "M√™me apr√®s r√©initialisation, la long-term memory persiste."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_preference = [\n",
        "    Document(\n",
        "        page_content=\"I love Apache Kafka for real-time data processing..\",\n",
        "        metadata={\"source\": \"user_preferences\", \"type\": \"user_preferences\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"One another best framework is Trunk Data Platform.\",\n",
        "        metadata={\"source\": \"prompt_engineering\", \"type\": \"Tech\"}\n",
        "    )\n",
        "]\n",
        "\n",
        "# Ajouter les nouveaux documents √† la base\n",
        "db.add_documents(user_preference)\n",
        "\n",
        "print(f\"‚úì {len(new_facts)} nouveaux faits ajout√©s √† la m√©moire long-terme\")"
      ],
      "metadata": {
        "id": "3uIhcC5C6zHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Red-K7yN9FBo"
      },
      "outputs": [],
      "source": [
        "print(\"SCENARIO: Requ√™te apr√®s r√©initialisation avec LONG-TERM MEMORY\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# M√™me question, mais avec RAG\n",
        "query_persistent = \"What technologies are mentioned for real-time processing?\"\n",
        "print(f\"Query: {query_persistent}\")\n",
        "\n",
        "# Cr√©ation de la cha√Æne RAG (Retrieval-Augmented Generation)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "qa_persistent = qa_chain.invoke({\"query\": query_persistent})\n",
        "result_persistent = qa_persistent[\"result\"]\n",
        "\n",
        "print(f\"\\nü§ñ Assistant (with long-term memory):\\n{result_persistent}\")\n",
        "print(f\"\\n‚úì LONG-TERM MEMORY: Les connaissances PERSISTENT m√™me apr√®s reset\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlKygKXf9FBp"
      },
      "source": [
        "### Exp√©rience 8 : Tester hallucination vs retrieval\n",
        "\n",
        "Comparer les r√©ponses avec et sans contexte de retrieval."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports corrects pour LangChain moderne\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Template pour le RAG\n",
        "template = \"\"\"You are an AI expert assistant. Use the following context to answer the question.\n",
        "If you don't know the answer based on the context, say so.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Fonction pour formater les documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Cr√©er la cha√Æne RAG avec LCEL (LangChain Expression Language)\n",
        "qa_chain_lcel = (\n",
        "    {\n",
        "        \"context\": retriever | format_docs,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Question qui pourrait causer une hallucination\n",
        "hallucination_query = \"What is the relationship between multi-agent systems and real-time data processing?\"\n",
        "\n",
        "print(f\"üîç Query: {hallucination_query}\\n\")\n",
        "print(\"=\"*60)\n",
        "print(\"R√âPONSE 1: Sans retrieval (risque de hallucination)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# R√©ponse directe (sans RAG)\n",
        "conv_no_rag = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an AI expert.\"},\n",
        "    {\"role\": \"user\", \"content\": hallucination_query}\n",
        "]\n",
        "reply_no_rag = chat_with_memory(conv_no_rag)\n",
        "print(f\"\\n{reply_no_rag}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"R√âPONSE 2: Avec retrieval (grounding knowledge)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Utiliser invoke()\n",
        "reply_rag = qa_chain_lcel.invoke(hallucination_query)\n",
        "print(f\"\\n{reply_rag}\")\n",
        "\n",
        "# Pour r√©cup√©rer aussi les documents sources\n",
        "docs = retriever.invoke(hallucination_query)\n",
        "print(f\"\\nDocuments utilis√©s comme source:\")\n",
        "for i, doc in enumerate(docs, 1):\n",
        "    print(f\"  [{i}] {doc.page_content[:200]}...\")\n"
      ],
      "metadata": {
        "id": "YyYQqAxh_kDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tra√ßons chaque √©tape\n",
        "question = \"What is the relationship between multi-agent systems and real-time data processing?\"\n",
        "\n",
        "# √âtape 1: retriever + format\n",
        "step1_context = retriever.invoke(question)\n",
        "print(\"STEP 1 - Documents r√©cup√©r√©s:\\n\")\n",
        "print(f\"  Nombre de docs: {len(step1_context)}\")\n",
        "\n",
        "step1_formatted = \"\\n\\n\".join(doc.page_content for doc in step1_context)\n",
        "print(f\"  Contexte format√©:\\n\\n{step1_formatted[:200]}...\\n\")\n",
        "\n",
        "# √âtape 2: prompt formatting\n",
        "step2_input = {\n",
        "    \"context\": step1_formatted,\n",
        "    \"question\": question\n",
        "}\n",
        "step2_prompt = prompt.format(**step2_input)\n",
        "print(\"STEP 2 - Prompt format√©:\\n\")\n",
        "print(f\"{step2_prompt}\\n\")\n",
        "\n",
        "# √âtape 3: LLM generation\n",
        "step3_llm_output = llm.invoke(step2_prompt)\n",
        "print(\"STEP 3 - R√©ponse du LLM (AIMessage):\")\n",
        "print(f\"  Type: {type(step3_llm_output)}\")\n",
        "print(f\"  Contenu: {step3_llm_output.content[:200]}...\\n\")\n",
        "\n",
        "# √âtape 4: Parser\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "step4_final = parser.invoke(step3_llm_output)  # Utiliser invoke() au lieu de parse()\n",
        "print(\"STEP 4 - R√©ponse finale (str):\")\n",
        "print(f\"  Type: {type(step4_final)}\")\n",
        "print(\"  Contenu:\")\n",
        "# Indenter chaque ligne de la r√©ponse\n",
        "for line in step4_final.split('\\n'):\n",
        "    print(f\"    {line}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZWNw9xB5A8Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prbdQPPp9FBp"
      },
      "source": [
        "### Exp√©rience 9 : Ajouter des faits dynamiquement et tester\n",
        "\n",
        "Simuler un agent qui apprend continuellement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bpd33Vo9FBp"
      },
      "outputs": [],
      "source": [
        "print(\"SCENARIO: Apprentissage continu (agent learns new facts)\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ajouter un nouveau fait important\n",
        "fact_to_learn =  [\n",
        "    Document (\n",
        "        page_content=\"Function calling enables AI agents to interact with external APIs and tools to perform real-world actions.\",\n",
        "        metadata={\"source\": \"agent_capabilities\", \"type\": \"technique\"}\n",
        ")\n",
        "]\n",
        "\n",
        "db.add_documents(fact_to_learn)\n",
        "print(\"‚úì Nouveau fait ajout√©: 'Function calling enables AI agents...'\")\n",
        "\n",
        "# Tester la r√©cup√©ration\n",
        "test_query = \"How can agents interact with external systems?\"\n",
        "print(f\"\\nüîç Query: {test_query}\")\n",
        "\n",
        "# Invoquer avec la cha√Æne directement\n",
        "qa_updated = qa_chain_lcel.invoke(test_query)\n",
        "print(f\"\\nü§ñ Assistant (with updated knowledge):\")\n",
        "print(qa_updated)\n",
        "\n",
        "# Afficher aussi les documents sources\n",
        "docs = retriever.invoke(test_query)\n",
        "print(f\"\\nüìö Documents utilis√©s ({len(docs)} trouv√©s):\")\n",
        "for i, doc in enumerate(docs, 1):\n",
        "    print(f\"  [{i}] {doc.page_content[:150]}...\")\n",
        "\n",
        "print(f\"\\n‚úì Agent a acc√®s au nouveau fait appris\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKsn_c-g9FBw"
      },
      "source": [
        "## Summary & Key Insights\n",
        "\n",
        "### R√©capitulatif de ce que vous avez appris :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz_9N2w39FBw"
      },
      "outputs": [],
      "source": [
        "summary = \"\"\"\\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë         SHORT-TERM vs LONG-TERM MEMORY IN AGENTIC AI              ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "üìå SHORT-TERM MEMORY (Context Window)\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "‚úì Stocke l'historique r√©cent de conversation\n",
        "‚úì Permet au mod√®le de r√©f√©rencer les messages pr√©c√©dents\n",
        "‚úì Limit√© par la taille de la context window (ex: 4K, 8K, 128K tokens)\n",
        "‚úó Se r√©initialise √† chaque nouvelle conversation\n",
        "‚úó Ne persiste pas entre les sessions\n",
        "\n",
        "Cas d'usage:\n",
        "‚Ä¢ Conversations interactives\n",
        "‚Ä¢ Contexte imm√©diat et r√©f√©rences\n",
        "‚Ä¢ Dialogue naturel\n",
        "\n",
        "---\n",
        "\n",
        "üíæ LONG-TERM MEMORY (Vector Database)\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "‚úì Stocke les connaissances de fa√ßon persistante\n",
        "‚úì R√©cup√©ration s√©mantique (recherche par similarit√©)\n",
        "‚úì Scalable √† de grands corpus de connaissances\n",
        "‚úì Persiste entre les sessions\n",
        "‚úì Am√©liore accuracy et r√©duit hallucination (RAG)\n",
        "‚úó N√©cessite une setup suppl√©mentaire\n",
        "‚úó Co√ªt en appels embeddings\n",
        "\n",
        "Cas d'usage:\n",
        "‚Ä¢ Knowledge bases persistantes\n",
        "‚Ä¢ Fact retrieval\n",
        "‚Ä¢ Context augmentation (RAG)\n",
        "‚Ä¢ Agents autonomes √† long terme\n",
        "\n",
        "---\n",
        "\n",
        "üîó HYBRID APPROACH (Short-term + Long-term)\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "‚úì Combine conversational context + persistent knowledge\n",
        "‚úì Meilleure compr√©hension du contexte utilisateur\n",
        "‚úì Acc√®s √† des faits pr√©cis stock√©s\n",
        "‚úì R√©duction des hallucinations\n",
        "‚úì Agents plus intelligents et contextuels\n",
        "\n",
        "Flux:\n",
        "1. Contexte court-terme: Lire l'historique conversation\n",
        "2. R√©cup√©ration long-terme: Chercher docs pertinents via RAG\n",
        "3. Fusion: Combiner contexte + docs pour meilleure r√©ponse\n",
        "\n",
        "---\n",
        "\n",
        "üöÄ NEXT STEPS\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "1. Essayez avec vos propres documents (PDFs, web pages)\n",
        "2. Tunerez embedding model et retriever (top_k, distance threshold)\n",
        "3. Impl√©mentez un agent loop complet (ReAct pattern)\n",
        "4. Explorez multi-agent orchestration (CrewAI)\n",
        "5. Comparez diff√©rentes Vector DBs: Chroma vs Pinecone vs Weaviate\n",
        "\"\"\"\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp9lwMiS9FBw"
      },
      "source": [
        "## üéì Challenges & Questions\n",
        "\n",
        "Essayez ces variations pour approfondir votre compr√©hension :\n",
        "\n",
        "1. **Challenge 1:** Ajoutez 5 nouveaux faits √† la base de connaissances. Testez les requ√™tes complexes qui les combinent.\n",
        "\n",
        "2. **Challenge 2:** Cr√©ez une conversation multi-tour o√π l'agent doit:\n",
        "   - Se rappeler le nom de l'utilisateur (short-term)\n",
        "   - R√©cup√©rer des faits sur les frameworks (long-term)\n",
        "   - G√©n√©rer une r√©ponse personnalis√©e\n",
        "\n",
        "3. **Challenge 3:** Comparez 2 diff√©rentes requ√™tes:\n",
        "   - L'une qui demande information dans la base (retrievable)\n",
        "   - L'une qui demande information hors de la base (hallucination risk)\n",
        "\n",
        "4. **Challenge 4:** Impl√©mentez un feedback loop:\n",
        "   - L'agent g√©n√®re une r√©ponse\n",
        "   - Vous √©valuez la confiance (hallucination ou pas)\n",
        "   - Ajoutez des documents pour am√©liorer futures requ√™tes\n",
        "\n",
        "5. **Challenge 5:** Explorez `similarity_search_with_score()` pour voir les distances d'embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJw23ONw9FBw"
      },
      "source": [
        "## üìñ Ressources suppl√©mentaires\n",
        "\n",
        "- [LangChain Documentation](https://python.langchain.com/)\n",
        "- [Chroma Vector Database](https://www.trychroma.com/)\n",
        "- [OpenAI Embeddings](https://platform.openai.com/docs/models/embeddings)\n",
        "- [RAG Pattern](https://python.langchain.com/docs/use_cases/question_answering/)\n",
        "- [CrewAI for Multi-Agent](https://crewai.io/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdY8KxrC9FBw"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüéâ Lab termin√©!\")\n",
        "print(\"‚úÖ Vous avez appris:\")\n",
        "print(\"   ‚Ä¢ Comment construire une m√©moire court-terme (context window)\")\n",
        "print(\"   ‚Ä¢ Comment construire une m√©moire long-terme (Vector DB)\")\n",
        "print(\"   ‚Ä¢ Comment combiner les deux pour des agents intelligents\")\n",
        "print(\"   ‚Ä¢ Comment √©viter les hallucinations avec RAG\")\n",
        "print(\"\\nüìù Sauvegardez ce notebook pour r√©f√©rences futures!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}