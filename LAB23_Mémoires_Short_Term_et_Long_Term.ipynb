{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/RAG/blob/main/LAB23_M%C3%A9moires_Short_Term_et_Long_Term.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckdbCGm_9FBh"
      },
      "source": [
        "# LAB23 : construction d'agents avec m√©moires \"Short_Term\" et \"Long_Term\"\n",
        "\n",
        "## Objectifs :\n",
        "Impl√©mentation de 2 types de m√©moire pour les agents IA :\n",
        "- **Short-term memory** ‚Üí context window (messages r√©cents)\n",
        "- **Long-term memory** ‚Üí knowledge base persistante (Vector DB)\n",
        "\n",
        "## Temps estim√© :\n",
        "- 20‚Äì30 minutes\n",
        "\n",
        "**Livrables :**\n",
        "- Notebook montrant comment un agent rappelle les conversations r√©centes et r√©cup√®re des faits stock√©s en m√©moire long terme."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY5iIp7b9FBj"
      },
      "source": [
        "## Step 1: Setup (5 min)\n",
        "\n",
        "Installation des pr√©requis et d√©pendances n√©cessaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEDykuf-9FBj",
        "outputId": "d98f9a7c-994b-4d98-ab89-5caf7de86be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/84.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úì D√©pendances install√©es avec succ√®s\n"
          ]
        }
      ],
      "source": [
        "# Installation des d√©pendances\n",
        "!pip install openai langchain chromadb python-dotenv -q\n",
        "!pip install -q -U langchain-chroma langchain-huggingface langchain-core langchain-openai\n",
        "!pip install -q datasets\n",
        "!pip install -q transformers\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain-community\n",
        "\n",
        "\n",
        "print(\"‚úì D√©pendances install√©es avec succ√®s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IacpFfDb9FBk"
      },
      "source": [
        "### Configuration de l'API OpenAI\n",
        "\n",
        "Dans Google Colab, vous pouvez d√©finir votre cl√© API de deux fa√ßons :\n",
        "1. **M√©thode s√©curis√©e (recommand√©e)** : Utiliser `google.colab.userdata`\n",
        "2. **M√©thode alternative** : D√©finir directement en variable d'environnement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6mO9h7k9FBk",
        "outputId": "a2a0dc62-5870-480a-f2ab-aa761ccc5193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Cl√© API OpenAI charg√©e depuis les secrets Colab\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# R√©cup√©rer la cl√© API depuis les secrets Colab\n",
        "# Pour ajouter : cliquez sur üîë dans le panneau de gauche\n",
        "try:\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "    print(\"‚úì Cl√© API OpenAI charg√©e depuis les secrets Colab\")\n",
        "except:\n",
        "    print(\"‚ö† Secrets Colab non configur√©s. Veuillez ajouter OPENAI_API_KEY.\")\n",
        "    print(\"Instructions : Cliquez sur üîë dans le panneau gauche > Ajouter un nouveau secret\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQw8aTJb9FBk"
      },
      "source": [
        "## Step 2: Short-Term Memory (Context Window) (5 min)\n",
        "\n",
        "La m√©moire court-terme consiste √† maintenir l'historique de conversation dans le prompt.\n",
        "\n",
        "### Concept cl√© :\n",
        "- Sans contexte ‚Üí le mod√®le \"oublie\"\n",
        "- Avec historique ‚Üí il peut se rappeler des informations pr√©c√©demment donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMB4jRXk9FBl",
        "outputId": "7fcad861-a7bf-455b-dbda-99486ebf42a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Fonction chat_with_memory d√©finie\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
        "\n",
        "def chat_with_memory(messages):\n",
        "    \"\"\"\n",
        "    Fonction pour communiquer avec le mod√®le en gardant l'historique.\n",
        "\n",
        "    Args:\n",
        "        messages: Liste de dictionnaires avec 'role' et 'content'\n",
        "\n",
        "    Returns:\n",
        "        str: R√©ponse du mod√®le\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "print(\"‚úì Fonction chat_with_memory d√©finie\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NidW4mrk9FBl"
      },
      "source": [
        "### Exp√©rience 1 : Conversation avec m√©moire\n",
        "\n",
        "Simulons une conversation o√π l'agent doit se souvenir du nom de l'utilisateur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0OtPFu19FBl",
        "outputId": "86bec2be-245c-4b05-d47b-aebbeaadd549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üó£Ô∏è User: My name is Stephane.\n",
            "ü§ñ Assistant: Nice to meet you, Stephane! How can I assist you today?\n",
            "\n",
            "üó£Ô∏è User: What is my name?\n",
            "ü§ñ Assistant: Your name is Stephane! How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "# Initialisation une conversation\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful tutor. Be concise and friendly.\"},\n",
        "    {\"role\": \"user\", \"content\": \"My name is Stephane.\"}\n",
        "]\n",
        "\n",
        "# Premier √©change\n",
        "print(\"üó£Ô∏è User: My name is Stephane.\")\n",
        "reply1 = chat_with_memory(conversation)\n",
        "print(f\"ü§ñ Assistant: {reply1}\")\n",
        "\n",
        "# Ajout de la r√©ponse √† l'historique\n",
        "conversation.append({\"role\": \"assistant\", \"content\": reply1})\n",
        "\n",
        "# Deuxi√®me message - test de m√©moire court-terme\n",
        "conversation.append({\"role\": \"user\", \"content\": \"What is my name?\"})\n",
        "print(\"\\nüó£Ô∏è User: What is my name?\")\n",
        "reply2 = chat_with_memory(conversation)\n",
        "print(f\"ü§ñ Assistant: {reply2}\")\n",
        "\n",
        "conversation.append({\"role\": \"assistant\", \"content\": reply2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZyu1Wmi9FBl"
      },
      "source": [
        "### Exp√©rience 2 : Sans contexte (oubli)\n",
        "\n",
        "Demandons le nom du utilisateur **SANS l'historique** pour constater qu'effectivement le mod√®le oublie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzL4I4lZ9FBm",
        "outputId": "f6a52a93-bc7e-415f-d630-2826fa822694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üó£Ô∏è User: What is my name?\n",
            "\n",
            "(Note: Sans contexte pr√©c√©dent dans la conversation)\n",
            "ü§ñ Assistant: I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. Therefore, I don't know your name. How can I assist you today?\n",
            "\n",
            "============================================================\n",
            "ANALYSE:\n",
            "‚úì AVEC contexte: L'agent se souvient du nom (Stephane)\n",
            "‚úó SANS contexte: L'agent ne peut pas savoir le nom\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Cr√©er une nouvelle conversation SANS contexte pr√©c√©dent\n",
        "conversation_without_context = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful tutor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
        "]\n",
        "\n",
        "print(\"üó£Ô∏è User: What is my name?\")\n",
        "print(\"\\n(Note: Sans contexte pr√©c√©dent dans la conversation)\")\n",
        "reply_without_context = chat_with_memory(conversation_without_context)\n",
        "print(f\"ü§ñ Assistant: {reply_without_context}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSE:\")\n",
        "print(f\"‚úì AVEC contexte: L'agent se souvient du nom (Stephane)\")\n",
        "print(f\"‚úó SANS contexte: L'agent ne peut pas savoir le nom\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_r67xBP9FBm"
      },
      "source": [
        "## Step 3: Long-Term Memory (Vector Store) (10 min)\n",
        "\n",
        "Persistance d'informations y compris apr√®s  la fin des sessions de conversation.\n",
        "\n",
        "### Concept cl√© :\n",
        "- Vector DB (Chroma) stocke les embeddings des documents et persiste les informations\n",
        "- Lors d'un √©change, on effectue une recherche s√©mantique : \"agent memory\" ‚Üí r√©cup√®re les documents pertinents\n",
        "-  ‚Üí Les informations sont persist√©es entre les sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlxOWroO9FBm",
        "outputId": "0ca5b62e-1a0e-43c2-b188-ad35be3b965f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì OpenAIEmbeddings initialis√©\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "#from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Initialiser les embeddings OpenAI\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "print(\"‚úì OpenAIEmbeddings initialis√©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfLRobho9FBm"
      },
      "source": [
        "### Cr√©er une base de connaissances\n",
        "\n",
        "Nous allons stocker plusieurs faits sur les agents IA et LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGbY7ljK9FBm",
        "outputId": "d5fa0b09-8df6-4ae6-d4fc-688d2ae21904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Base vectorielle cr√©√©e avec 6 documents\n",
            "‚úì Collection: 'hanlab_long_term_memory'\n"
          ]
        }
      ],
      "source": [
        "# Les 6 dDocuments de notre base de connaissances\n",
        "knowledge_base = [\n",
        "    Document(\n",
        "        page_content=\"Agentic AI agents use tools and memory to accomplish complex tasks autonomously.\",\n",
        "        metadata={\"source\": \"agentic_ai_basics\", \"type\": \"definition\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"LangChain is a framework that helps build autonomous agents with memory, tools, and chains.\",\n",
        "        metadata={\"source\": \"langchain_intro\", \"type\": \"framework\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"RAG (Retrieval-Augmented Generation) improves accuracy by retrieving relevant knowledge before generating responses.\",\n",
        "        metadata={\"source\": \"rag_concept\", \"type\": \"technique\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Short-term memory in agents stores recent conversation context in the prompt window.\",\n",
        "        metadata={\"source\": \"agent_memory\", \"type\": \"memory_type\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Long-term memory uses vector databases to persistently store and retrieve semantic knowledge.\",\n",
        "        metadata={\"source\": \"agent_memory\", \"type\": \"memory_type\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"CrewAI enables multi-agent orchestration where multiple agents collaborate to solve problems.\",\n",
        "        metadata={\"source\": \"crewai_intro\", \"type\": \"framework\"}\n",
        "    )\n",
        "]\n",
        "\n",
        "# Cr√©ation de la base vectorielle Chroma\n",
        "db = Chroma.from_documents(\n",
        "    documents=knowledge_base,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"hanlab_long_term_memory\",\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "print(f\"‚úì Base vectorielle cr√©√©e avec {len(knowledge_base)} documents\")\n",
        "print(f\"‚úì Collection: 'hanlab_long_term_memory'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si9CQuQZ9FBm"
      },
      "source": [
        "### Fonction de rappel (Recall) depuis la base de m√©moire long-terme\n",
        "\n",
        "R√©cup√©ration des documents pertinents √† partir d'une requ√™te."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMq79ANa9FBm",
        "outputId": "523542fe-f024-4fab-e7a9-f24aaf50eb54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Fonction recall_from_long_term_memory d√©finie\n"
          ]
        }
      ],
      "source": [
        "def recall_from_long_term_memory(query, k=3):\n",
        "    \"\"\"\n",
        "    R√©cup√©ration des faits pertinents dans la m√©moire long-terme.\n",
        "\n",
        "    Args:\n",
        "        query:  Cha√Æne de requ√™te\n",
        "        k:      Nombre de documents √† r√©cup√©rer\n",
        "\n",
        "    Returns:\n",
        "        Liste de documents pertinents\n",
        "    \"\"\"\n",
        "    results = db.similarity_search(query, k=k)\n",
        "    return results\n",
        "\n",
        "print(\"‚úì Fonction recall_from_long_term_memory d√©finie\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM5jlgmi9FBn"
      },
      "source": [
        "### Exp√©rience 3 : R√©cup√©ration de la m√©moire long-terme\n",
        "\n",
        "Tester la r√©cup√©ration s√©mantique de documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmjU3fvc9FBn",
        "outputId": "5b1a6e27-1649-4096-8257-376a81e32395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Query: How do agents use knowledge?\n",
            "\n",
            "R√©sultats de la m√©moire long-terme:\n",
            "\n",
            "  [1] Agentic AI agents use tools and memory to accomplish complex tasks autonomously.\n",
            "      (Source: agentic_ai_basics)\n",
            "\n",
            "  [2] RAG (Retrieval-Augmented Generation) improves accuracy by retrieving relevant knowledge before generating responses.\n",
            "      (Source: rag_concept)\n",
            "\n",
            "  [3] Short-term memory in agents stores recent conversation context in the prompt window.\n",
            "      (Source: agent_memory)\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Requ√™te 1\n",
        "query1 = \"How do agents use knowledge?\"\n",
        "print(f\"üîç Query: {query1}\")\n",
        "print(\"\\nR√©sultats de la m√©moire long-terme:\")\n",
        "results1 = recall_from_long_term_memory(query1, k=3)\n",
        "for i, doc in enumerate(results1, 1):\n",
        "    print(f\"\\n  [{i}] {doc.page_content}\")\n",
        "    print(f\"      (Source: {doc.metadata['source']})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eE-adyA9FBn",
        "outputId": "1e159aeb-b17a-4eaa-c6b0-69e9166c62d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Query: What frameworks help with agent building?\n",
            "\n",
            "R√©sultats de la m√©moire long-terme:\n",
            "\n",
            "  [1] Agentic AI agents use tools and memory to accomplish complex tasks autonomously.\n",
            "      (Source: agentic_ai_basics)\n",
            "\n",
            "  [2] LangChain is a framework that helps build autonomous agents with memory, tools, and chains.\n",
            "      (Source: langchain_intro)\n",
            "\n",
            "  [3] CrewAI enables multi-agent orchestration where multiple agents collaborate to solve problems.\n",
            "      (Source: crewai_intro)\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Requ√™te 2\n",
        "query2 = \"What frameworks help with agent building?\"\n",
        "print(f\"üîç Query: {query2}\")\n",
        "print(\"\\nR√©sultats de la m√©moire long-terme:\")\n",
        "results2 = recall_from_long_term_memory(query2, k=3)\n",
        "for i, doc in enumerate(results2, 1):\n",
        "    print(f\"\\n  [{i}] {doc.page_content}\")\n",
        "    print(f\"      (Source: {doc.metadata['source']})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otfWBSCV9FBn",
        "outputId": "6cb2e15e-dbe2-4529-a595-b098049cceca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Query: Tell me about multi-agent collaboration\n",
            "\n",
            "R√©sultats de la m√©moire long-terme:\n",
            "\n",
            "  [1] CrewAI enables multi-agent orchestration where multiple agents collaborate to solve problems.\n",
            "      (Source: crewai_intro)\n",
            "\n",
            "  [2] Agentic AI agents use tools and memory to accomplish complex tasks autonomously.\n",
            "      (Source: agentic_ai_basics)\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Requ√™te 3 - Multi-agent orchestration\n",
        "query3 = \"Tell me about multi-agent collaboration\"\n",
        "print(f\"üîç Query: {query3}\")\n",
        "print(\"\\nR√©sultats de la m√©moire long-terme:\")\n",
        "results3 = recall_from_long_term_memory(query3, k=2)\n",
        "for i, doc in enumerate(results3, 1):\n",
        "    print(f\"\\n  [{i}] {doc.page_content}\")\n",
        "    print(f\"      (Source: {doc.metadata['source']})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY-4GHwG9FBn"
      },
      "source": [
        "### Exp√©rience 4 : Ajouter dynamiquement de nouveaux faits\n",
        "\n",
        "La m√©moire long-terme est persistante et extensible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xOs5fUH9FBn",
        "outputId": "ec2a470c-8ed0-4ef3-942c-63f14da53640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì 2 nouveaux faits ajout√©s √† la m√©moire long-terme\n"
          ]
        }
      ],
      "source": [
        "# Ajouter de nouveaux documents dynamiquement\n",
        "new_facts = [\n",
        "    Document(\n",
        "        page_content=\"Apache Kafka is a distributed streaming platform for building real-time data pipelines.\",\n",
        "        metadata={\"source\": \"kafka_platform\", \"type\": \"technology\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Prompt engineering involves carefully crafting instructions to optimize LLM outputs.\",\n",
        "        metadata={\"source\": \"prompt_engineering\", \"type\": \"technique\"}\n",
        "    )\n",
        "]\n",
        "\n",
        "# Ajouter les nouveaux documents √† la base\n",
        "db.add_documents(new_facts)\n",
        "\n",
        "print(f\"‚úì {len(new_facts)} nouveaux faits ajout√©s √† la m√©moire long-terme\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqNg9dua9FBn",
        "outputId": "40ed92c5-a18b-4480-d3ab-b84d0f84aa14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Query: Tell me about data streaming platforms\n",
            "\n",
            "R√©sultats (incluant les nouveaux faits):\n",
            "\n",
            "  [1] Apache Kafka is a distributed streaming platform for building real-time data pipelines.\n",
            "      (Source: kafka_platform)\n",
            "\n",
            "  [2] Long-term memory uses vector databases to persistently store and retrieve semantic knowledge.\n",
            "      (Source: agent_memory)\n"
          ]
        }
      ],
      "source": [
        "# V√©rifier que les nouveaux faits sont r√©cup√©rables\n",
        "query_kafka = \"Tell me about data streaming platforms\"\n",
        "print(f\"üîç Query: {query_kafka}\")\n",
        "print(\"\\nR√©sultats (incluant les nouveaux faits):\")\n",
        "results_kafka = recall_from_long_term_memory(query_kafka, k=2)\n",
        "for i, doc in enumerate(results_kafka, 1):\n",
        "    print(f\"\\n  [{i}] {doc.page_content}\")\n",
        "    print(f\"      (Source: {doc.metadata['source']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smIzPk7H9FBn"
      },
      "source": [
        "## Step 4: Association de la m√©moire Short-Term et de la m√©moire Long-Term (10 min)\n",
        "\n",
        "Cela consiste √† combiner les deux syst√®mes de m√©moire : contexte conversationnel + connaissance persistante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwZFNEe59FBn",
        "outputId": "b07d4bb6-44b7-40e7-e36e-a99dec17c4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Cha√Æne RetrievalQA cr√©√©e\n",
            "‚úì Combine short-term (contexte) + long-term (retriever) m√©moire\n",
            "_______________________________________\n",
            "\n",
            "Short-term: Your favorite framework is LangChain. In LangChain, agents can use memory to maintain context and state across interactions. Memory allows agents to remember past interactions, user preferences, and relevant information, enabling them to provide more personalized and context-aware responses.\n",
            "\n",
            "There are different types of memory implementations in LangChain, such as:\n",
            "\n",
            "1. **Conversation Memory**: Stores the history of the conversation, allowing the agent to reference previous messages or user inputs.\n",
            "2. **Long-term Memory**: Keeps track of user-specific information over longer periods, which can help the agent recall important details in future interactions.\n",
            "3. **Short-term Memory**: Useful for temporary context within a single session, helping the agent respond based on recent interactions without retaining that information long-term.\n",
            "\n",
            "By utilizing memory effectively, agents can enhance user experience by providing more coherent and relevant interactions. If you have any specific questions about implementing memory in LangChain, feel free to ask!\n",
            "_______________________________________\n",
            "\n",
            "Short+Long-term: {'query': 'How do agents use memory?', 'result': 'Agents use memory in two main ways: short-term memory and long-term memory. \\n\\nShort-term memory stores recent conversation context within the prompt window, which allows agents to keep track of ongoing interactions and maintain coherence in conversations.\\n\\nLong-term memory utilizes vector databases to persistently store and retrieve semantic knowledge, enabling agents to remember information over time and apply it in future interactions. This helps agents accomplish complex tasks autonomously by leveraging both recent context and stored knowledge.', 'source_documents': [Document(id='179aacf5-025e-4925-86dd-af6e0201a66b', metadata={'type': 'definition', 'source': 'agentic_ai_basics'}, page_content='Agentic AI agents use tools and memory to accomplish complex tasks autonomously.'), Document(id='5f861d9c-9c66-470a-9a8b-cba2c632b1de', metadata={'type': 'memory_type', 'source': 'agent_memory'}, page_content='Short-term memory in agents stores recent conversation context in the prompt window.'), Document(id='af14b115-46d0-4837-99cd-75e07f56f1a6', metadata={'type': 'memory_type', 'source': 'agent_memory'}, page_content='Long-term memory uses vector databases to persistently store and retrieve semantic knowledge.')]}\n",
            "_______________________________________\n"
          ]
        }
      ],
      "source": [
        "# 1√®re M√©thode (d√©ppr√©ci√©e) :\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Initialiser le LLM et le retriever\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Cr√©ation de la cha√Æne RAG (Retrieval-Augmented Generation)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(\"‚úì Cha√Æne RetrievalQA cr√©√©e\")\n",
        "print(\"‚úì Combine short-term (contexte) + long-term (retriever) m√©moire\")\n",
        "conversation = [\n",
        "    {\"role\":\"system\",\"content\":\"You are a teaching AI agent.\"},\n",
        "    {\"role\":\"user\",\"content\":\"Remember my favorite framework is LangChain.\"},\n",
        "    {\"role\":\"assistant\",\"content\":\"Got it, your favorite framework is LangChain.\"}\n",
        "]\n",
        "\n",
        "# User asks later\n",
        "conversation.append({\"role\":\"user\",\"content\":\"What‚Äôs my favorite framework and how do agents use memory?\"})\n",
        "print(\"_______________________________________\")\n",
        "# La 1√®re r√©ponse utilise la m√©moire court-terme (short-term)\n",
        "short_term_ans = chat_with_memory(conversation)\n",
        "print(\"\\nShort-term:\", short_term_ans)\n",
        "print(\"_______________________________________\")\n",
        "# La seconde r√©ponse est augment√©e grace √† la m√©moire long-terme\n",
        "print(\"\\nShort+Long-term:\", qa_chain.invoke(\"How do agents use memory?\"))\n",
        "print(\"_______________________________________\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nde M√©thode (LCEL) conseill√©e d√©sormais :\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# create_retrieval_chain :        Orchestre la r√©cup√©ration (retrieval) + g√©n√©ration\n",
        "# create_stuff_documents_chain :  Combine les documents r√©cup√©r√©s dans un prompt\n",
        "# ChatPromptTemplate :            Cr√©e un template de prompt structur√©\n",
        "# ChatOpenAI :                    Interface pour appeler GPT (ici GPT-4o-mini)\n",
        "\n",
        "# Configuration\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "# Configure GPT-4o-mini comme mod√®le de langue\n",
        "# temperature=0.7 : Balance entre cr√©ativit√© (1.0) et d√©terminisme (0.0)\n",
        "#                   0.7 = r√©ponses mod√©r√©ment cr√©atives mais coh√©rentes\n",
        "\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
        "# Convertit db (une base de donn√©es vectorielle Chroma/Pinecone/etc.) en retriever\n",
        "# search_kwargs={\"k\": 3} : R√©cup√®re les 3 documents les plus pertinents\n",
        "# Le retriever cherchera les chunks s√©mantiquement proches du query\n",
        "\n",
        "# D√©finir le prompt\n",
        "system_prompt = \"\"\"Utilisez le contexte fourni pour r√©pondre √† la question.\n",
        "Si vous ne connaissez pas la r√©ponse, dites-le simplement.\n",
        "\n",
        "Contexte:\n",
        "{context}\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Syst√®me   : Instructions au mod√®le (utiliser le contexte, avouer l'ignorance)\n",
        "# Humain    : Placeholder {input} = la question de l'utilisateur\n",
        "# Contexte  : Placeholder {context} = sera remplac√© par les 3 documents r√©cup√©r√©s\n",
        "\n",
        "# Cr√©er la cha√Æne moderne\n",
        "stuff_chain = create_stuff_documents_chain(llm, prompt)\n",
        "# stuff_chain :\n",
        "# - Combine tous les documents dans un seul prompt\n",
        "# - Envoie au LLM : [system_prompt] + [documents] + [question]\n",
        "# - C'est le pattern \"stuff\" = on met tout dans le contexte\n",
        "\n",
        "qa_chain = create_retrieval_chain(retriever, stuff_chain)\n",
        "# Workflow complet :\n",
        "# - Re√ßoit la question utilisateur\n",
        "# - Appelle retriever ‚Üí r√©cup√®re 3 documents\n",
        "# - Envoie {context} + {input} au stuff_chain\n",
        "# - Le LLM g√©n√®re la r√©ponse avec le contexte\n",
        "\n",
        "print(\"‚úì Cha√Æne RAG moderne cr√©√©e avec LCEL\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eRDTJ-wGON5",
        "outputId": "00ce0a72-dae0-49e3-9729-cde6739080a2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Cha√Æne RAG moderne cr√©√©e avec LCEL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to17fjna9FBo"
      },
      "source": [
        "### Exp√©rience 5 : Conversation avec hybridation m√©moire\n",
        "\n",
        "**Sc√©nario :** L'utilisateur indique quel est son framework favori (short-term), puis pose une question impliquant une connaissance long-terme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss8vULGB9FBo",
        "outputId": "d533884c-ca38-4aa2-b29c-bd44e8bda495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üó£Ô∏è User: My favorite framework is LangChain because it helps build autonomous agents.\n",
            "ü§ñ Assistant: That's great to hear! LangChain is indeed a powerful framework for building autonomous agents, especially in the context of natural language processing and interaction. It allows you to create complex workflows by chaining together different components, making it easier to manage state and incorporate various data sources.\n",
            "\n",
            "If you have any specific questions about LangChain, whether it's about implementation, best practices, or specific features, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "# Initialiser une conversation\n",
        "hybrid_conversation = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are an expert teaching AI agent. You remember user preferences and use knowledge about AI frameworks and techniques.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"My favorite framework is LangChain because it helps build autonomous agents.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Premi√®re r√©ponse (short-term memory)\n",
        "print(\"üó£Ô∏è User: My favorite framework is LangChain because it helps build autonomous agents.\")\n",
        "reply_acknowledge = chat_with_memory(hybrid_conversation)\n",
        "print(f\"ü§ñ Assistant: {reply_acknowledge}\")\n",
        "\n",
        "hybrid_conversation.append({\"role\": \"assistant\", \"content\": reply_acknowledge})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FELVmtn99FBo",
        "outputId": "f8d59540-513c-4c2b-cb2d-daa6c7252ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üó£Ô∏è User: What's my favorite framework and how do agents use memory?\n",
            "\n",
            "============================================================\n",
            "R√âPONSE 1: Utilisant SHORT-TERM MEMORY UNIQUEMENT\n",
            "============================================================\n",
            "\n",
            "ü§ñ Assistant:\n",
            "Your favorite framework is LangChain. \n",
            "\n",
            "In LangChain, agents can use memory to retain information across interactions, enabling them to act more intelligently and contextually. Memory allows agents to remember facts, previous interactions, user preferences, and other relevant data, which helps create a more coherent and personalized experience. \n",
            "\n",
            "There are different types of memory that agents can utilize in LangChain:\n",
            "\n",
            "1. **Short-Term Memory**: This allows the agent to remember information only for the duration of a single session. It's useful for maintaining context during an ongoing conversation.\n",
            "\n",
            "2. **Long-Term Memory**: This type of memory enables agents to store information persistently across sessions. It can be useful for remembering user preferences, previous conversations, and important facts that should influence future interactions.\n",
            "\n",
            "3. **Custom Memory**: You can implement custom memory mechanisms tailored to your specific use case, allowing you to manage how data is stored and retrieved based on your application's requirements.\n",
            "\n",
            "By incorporating memory, agents can provide more relevant responses and adapt their behavior based on the user's history and preferences. If you want to dive deeper into a specific aspect of memory in LangChain or any implementation details, let me know!\n"
          ]
        }
      ],
      "source": [
        "# Question qui demande √† la fois short-term et long-term memory\n",
        "user_query = \"What's my favorite framework and how do agents use memory?\"\n",
        "hybrid_conversation.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "print(f\"\\nüó£Ô∏è User: {user_query}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"R√âPONSE 1: Utilisant SHORT-TERM MEMORY UNIQUEMENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# R√©ponse sans long-term memory (juste contexte)\n",
        "short_term_reply = chat_with_memory(hybrid_conversation)\n",
        "print(f\"\\nü§ñ Assistant:\\n{short_term_reply}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlB_P2o69FBo",
        "outputId": "9e4e7b91-db88-4fb1-9503-f5b05bfb746d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "R√âPONSE 2: Utilisant SHORT-TERM + LONG-TERM MEMORY (RAG)\n",
            "============================================================\n",
            "\n",
            "ü§ñ Assistant (enrichie avec RAG):\n",
            "I don't know what your favorite framework is. However, agents in LangChain use memory to store recent conversation context in the prompt window, which helps them accomplish complex tasks autonomously.\n",
            "\n",
            "üìö Documents r√©cup√©r√©s de la m√©moire long-terme:\n",
            "  [1] LangChain is a framework that helps build autonomous agents with memory, tools, ...\n",
            "      (Source: langchain_intro)\n",
            "  [2] Agentic AI agents use tools and memory to accomplish complex tasks autonomously....\n",
            "      (Source: agentic_ai_basics)\n",
            "  [3] Short-term memory in agents stores recent conversation context in the prompt win...\n",
            "      (Source: agent_memory)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"R√âPONSE 2: Utilisant SHORT-TERM + LONG-TERM MEMORY (RAG)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# R√©ponse avec long-term memory retrieval\n",
        "# D√©ppr√©ci√© : qa_result = qa_chain({\"query\": user_query})\n",
        "qa_result = qa_chain.invoke({\"query\": user_query})\n",
        "\n",
        "long_term_reply = qa_result[\"result\"]\n",
        "\n",
        "print(f\"\\nü§ñ Assistant (enrichie avec RAG):\\n{long_term_reply}\")\n",
        "\n",
        "# Afficher les sources r√©cup√©r√©es\n",
        "print(\"\\nüìö Documents r√©cup√©r√©s de la m√©moire long-terme:\")\n",
        "for i, doc in enumerate(qa_result.get(\"source_documents\", []), 1):\n",
        "    print(f\"  [{i}] {doc.page_content[:80]}...\")\n",
        "    print(f\"      (Source: {doc.metadata['source']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h_q0nqL9FBo"
      },
      "source": [
        "## Step 5: Reflection & Experiment (15‚Äì20 min)\n",
        "\n",
        "Exp√©riences avanc√©es pour comprendre les limites et avantages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFWQEo1K9FBo"
      },
      "source": [
        "### Exp√©rience 6 : Oubli avec r√©initialisation de conversation (short-term)\n",
        "\n",
        "D√©montrer que la m√©moire court-terme se perd lors du reset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAFGXvTa9FBo",
        "outputId": "36daf1df-3966-474a-fd52-e361eda6d11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCENARIO: R√©initialisation de conversation\n",
            "\n",
            "============================================================\n",
            "Session 1 - User: I love Apache Kafka for real-time data processing.\n",
            "Session 1 - Assistant: That's great to hear! Apache Kafka is a powerful distributed event streaming platform that excels in real-time data processing. It allows you to build robust data pipelines and stream applications. Some of the key features and benefits of Kafka include:\n",
            "\n",
            "1. **Scalability**: Kafka can handle large volumes of data, making it suitable for high-throughput applications.\n",
            "\n",
            "2. **Fault Tolerance**: Kafka replicates data across multiple brokers, ensuring that your data is safe even in the event of hardware failures.\n",
            "\n",
            "3. **Durability**: Messages in Kafka are persisted to disk, which means you can replay them if needed.\n",
            "\n",
            "4. **High Performance**: Kafka is designed for high-throughput and low-latency, making it ideal for real-time analytics and data processing.\n",
            "\n",
            "5. **Decoupling of Systems**: Kafka allows producers and consumers to operate independently, enabling a more flexible architecture.\n",
            "\n",
            "6. **Rich Ecosystem**: It integrates well with various big data technologies, including Apache Spark, Apache Flink, and Hadoop.\n",
            "\n",
            "7. **Stream Processing Capabilities**: With Kafka Streams and ksqlDB, you can perform real-time processing and analytics directly on streaming data.\n",
            "\n",
            "If you have any specific questions about Kafka or need help with certain features or concepts, feel free to ask!\n",
            "\n",
            "Session 2 - User: What do I love for data processing?\n",
            "Session 2 - Assistant: It sounds like you might be referring to the programming language \"R\" and the data manipulation package \"dplyr,\" which is part of the tidyverse ecosystem. Both are widely used for data processing, analysis, and visualization in various fields. \n",
            "\n",
            "If you're looking for specific tools or libraries for data processing, here are some popular options:\n",
            "\n",
            "1. **R and dplyr**: R is a powerful language for statistical computing, and dplyr provides a set of functions for data manipulation that are easy to use and understand.\n",
            "\n",
            "2. **Python and Pandas**: Python is another popular language for data analysis, and Pandas is a powerful library that provides data structures and functions for data manipulation and analysis.\n",
            "\n",
            "3. **SQL**: Structured Query Language (SQL) is essential for working with databases and performing data processing tasks such as querying, filtering, and aggregating data.\n",
            "\n",
            "4. **Apache Spark**: For big data processing, Apache Spark is a powerful distributed processing engine that can handle large datasets across clusters.\n",
            "\n",
            "5. **Apache Hadoop**: This framework allows for the distributed processing of large data sets across clusters of computers using simple programming models.\n",
            "\n",
            "6. **Excel**: While not as powerful for large datasets, Excel is widely used for data processing and analysis, especially for smaller datasets.\n",
            "\n",
            "7. **Data Visualization Tools**: Tools like Tableau or Power BI can be used for data processing and visualization, making it easier to analyze and present data.\n",
            "\n",
            "If you meant something else by \"data processing,\" please provide more context!\n",
            "\n",
            "‚ùå SHORT-TERM MEMORY: L'agent a OUBLI√â (conversation r√©initialis√©e)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"SCENARIO: R√©initialisation de conversation\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Conversation 1 : L'agent apprend une info\n",
        "conv_session1 = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I love Apache Kafka for real-time data processing.\"}\n",
        "]\n",
        "reply_session1 = chat_with_memory(conv_session1)\n",
        "print(f\"Session 1 - User: I love Apache Kafka for real-time data processing.\")\n",
        "print(f\"Session 1 - Assistant: {reply_session1}\\n\")\n",
        "\n",
        "# Conversation 2 : NOUVELLE CONVERSATION (conversation r√©initialis√©e)\n",
        "conv_session2 = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What do I love for data processing?\"}\n",
        "]\n",
        "reply_session2 = chat_with_memory(conv_session2)\n",
        "print(f\"Session 2 - User: What do I love for data processing?\")\n",
        "print(f\"Session 2 - Assistant: {reply_session2}\")\n",
        "print(f\"\\n‚ùå SHORT-TERM MEMORY: L'agent a OUBLI√â (conversation r√©initialis√©e)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx7j3O3_9FBo"
      },
      "source": [
        "### Exp√©rience 7 : Persistance avec long-term memory\n",
        "\n",
        "M√™me apr√®s r√©initialisation, la long-term memory persiste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Red-K7yN9FBo",
        "outputId": "ae2b17eb-b0c3-464d-8821-32c4fafa250c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCENARIO: Requ√™te apr√®s r√©initialisation avec LONG-TERM MEMORY\n",
            "\n",
            "============================================================\n",
            "Query: What technologies are mentioned for real-time processing?\n",
            "\n",
            "ü§ñ Assistant (with long-term memory):\n",
            "I don't know.\n",
            "\n",
            "‚úì LONG-TERM MEMORY: Les connaissances PERSISTENT m√™me apr√®s reset\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"SCENARIO: Requ√™te apr√®s r√©initialisation avec LONG-TERM MEMORY\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# M√™me question, mais avec RAG\n",
        "query_persistent = \"What technologies are mentioned for real-time processing?\"\n",
        "print(f\"Query: {query_persistent}\")\n",
        "\n",
        "# Utiliser le RAG chain\n",
        "qa_persistent = qa_chain.invoke({\"query\": query_persistent})\n",
        "result_persistent = qa_persistent[\"result\"]\n",
        "\n",
        "print(f\"\\nü§ñ Assistant (with long-term memory):\\n{result_persistent}\")\n",
        "print(f\"\\n‚úì LONG-TERM MEMORY: Les connaissances PERSISTENT m√™me apr√®s reset\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlKygKXf9FBp"
      },
      "source": [
        "### Exp√©rience 8 : Tester hallucination vs retrieval\n",
        "\n",
        "Comparer les r√©ponses avec et sans contexte de retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqqW77tg9FBp"
      },
      "outputs": [],
      "source": [
        "# Question qui pourrait causer une hallucination\n",
        "hallucination_query = \"What is the relationship between multi-agent systems and real-time data processing?\"\n",
        "\n",
        "print(f\"üîç Query: {hallucination_query}\\n\")\n",
        "print(\"=\"*60)\n",
        "print(\"R√âPONSE 1: Sans retrieval (risque de hallucination)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# R√©ponse directe (sans RAG)\n",
        "conv_no_rag = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an AI expert.\"},\n",
        "    {\"role\": \"user\", \"content\": hallucination_query}\n",
        "]\n",
        "reply_no_rag = chat_with_memory(conv_no_rag)\n",
        "print(f\"\\n{reply_no_rag}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"R√âPONSE 2: Avec retrieval (grounding knowledge)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# R√©ponse avec RAG\n",
        "qa_grounded = qa_chain({\"query\": hallucination_query})\n",
        "reply_rag = qa_grounded[\"result\"]\n",
        "print(f\"\\n{reply_rag}\")\n",
        "print(f\"\\nDocuments utilis√©s comme source:\")\n",
        "for i, doc in enumerate(qa_grounded[\"source_documents\"], 1):\n",
        "    print(f\"  [{i}] {doc.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prbdQPPp9FBp"
      },
      "source": [
        "### Exp√©rience 9 : Ajouter des faits dynamiquement et tester\n",
        "\n",
        "Simuler un agent qui apprend continuellement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bpd33Vo9FBp"
      },
      "outputs": [],
      "source": [
        "print(\"SCENARIO: Apprentissage continu (agent learns new facts)\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ajouter un nouveau fait important\n",
        "fact_to_learn = Document(\n",
        "    page_content=\"Function calling enables AI agents to interact with external APIs and tools to perform real-world actions.\",\n",
        "    metadata={\"source\": \"agent_capabilities\", \"type\": \"technique\"}\n",
        ")\n",
        "\n",
        "db.add_documents([fact_to_learn])\n",
        "print(\"‚úì Nouveau fait ajout√©: 'Function calling enables AI agents...'\")\n",
        "\n",
        "# Tester la r√©cup√©ration\n",
        "test_query = \"How can agents interact with external systems?\"\n",
        "print(f\"\\nüîç Query: {test_query}\")\n",
        "\n",
        "qa_updated = qa_chain({\"query\": test_query})\n",
        "print(f\"\\nü§ñ Assistant (with updated knowledge):\\n{qa_updated['result']}\")\n",
        "\n",
        "print(f\"\\n‚úì Agent a acc√®s au nouveau fait appris\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKsn_c-g9FBw"
      },
      "source": [
        "## Summary & Key Insights\n",
        "\n",
        "### R√©capitulatif de ce que vous avez appris :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz_9N2w39FBw"
      },
      "outputs": [],
      "source": [
        "summary = \"\"\"\\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë         SHORT-TERM vs LONG-TERM MEMORY IN AGENTIC AI              ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "üìå SHORT-TERM MEMORY (Context Window)\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "‚úì Stocke l'historique r√©cent de conversation\n",
        "‚úì Permet au mod√®le de r√©f√©rencer les messages pr√©c√©dents\n",
        "‚úì Limit√© par la taille de la context window (ex: 4K, 8K, 128K tokens)\n",
        "‚úó Se r√©initialise √† chaque nouvelle conversation\n",
        "‚úó Ne persiste pas entre les sessions\n",
        "\n",
        "Cas d'usage:\n",
        "‚Ä¢ Conversations interactives\n",
        "‚Ä¢ Contexte imm√©diat et r√©f√©rences\n",
        "‚Ä¢ Dialogue naturel\n",
        "\n",
        "---\n",
        "\n",
        "üíæ LONG-TERM MEMORY (Vector Database)\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "‚úì Stocke les connaissances de fa√ßon persistante\n",
        "‚úì R√©cup√©ration s√©mantique (recherche par similarit√©)\n",
        "‚úì Scalable √† de grands corpus de connaissances\n",
        "‚úì Persiste entre les sessions\n",
        "‚úì Am√©liore accuracy et r√©duit hallucination (RAG)\n",
        "‚úó N√©cessite une setup suppl√©mentaire\n",
        "‚úó Co√ªt en appels embeddings\n",
        "\n",
        "Cas d'usage:\n",
        "‚Ä¢ Knowledge bases persistantes\n",
        "‚Ä¢ Fact retrieval\n",
        "‚Ä¢ Context augmentation (RAG)\n",
        "‚Ä¢ Agents autonomes √† long terme\n",
        "\n",
        "---\n",
        "\n",
        "üîó HYBRID APPROACH (Short-term + Long-term)\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "‚úì Combine conversational context + persistent knowledge\n",
        "‚úì Meilleure compr√©hension du contexte utilisateur\n",
        "‚úì Acc√®s √† des faits pr√©cis stock√©s\n",
        "‚úì R√©duction des hallucinations\n",
        "‚úì Agents plus intelligents et contextuels\n",
        "\n",
        "Flux:\n",
        "1. Contexte court-terme: Lire l'historique conversation\n",
        "2. R√©cup√©ration long-terme: Chercher docs pertinents via RAG\n",
        "3. Fusion: Combiner contexte + docs pour meilleure r√©ponse\n",
        "\n",
        "---\n",
        "\n",
        "üöÄ NEXT STEPS\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "1. Essayez avec vos propres documents (PDFs, web pages)\n",
        "2. Tunerez embedding model et retriever (top_k, distance threshold)\n",
        "3. Impl√©mentez un agent loop complet (ReAct pattern)\n",
        "4. Explorez multi-agent orchestration (CrewAI)\n",
        "5. Comparez diff√©rentes Vector DBs: Chroma vs Pinecone vs Weaviate\n",
        "\"\"\"\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp9lwMiS9FBw"
      },
      "source": [
        "## üéì Challenges & Questions\n",
        "\n",
        "Essayez ces variations pour approfondir votre compr√©hension :\n",
        "\n",
        "1. **Challenge 1:** Ajoutez 5 nouveaux faits √† la base de connaissances. Testez les requ√™tes complexes qui les combinent.\n",
        "\n",
        "2. **Challenge 2:** Cr√©ez une conversation multi-tour o√π l'agent doit:\n",
        "   - Se rappeler le nom de l'utilisateur (short-term)\n",
        "   - R√©cup√©rer des faits sur les frameworks (long-term)\n",
        "   - G√©n√©rer une r√©ponse personnalis√©e\n",
        "\n",
        "3. **Challenge 3:** Comparez 2 diff√©rentes requ√™tes:\n",
        "   - L'une qui demande information dans la base (retrievable)\n",
        "   - L'une qui demande information hors de la base (hallucination risk)\n",
        "\n",
        "4. **Challenge 4:** Impl√©mentez un feedback loop:\n",
        "   - L'agent g√©n√®re une r√©ponse\n",
        "   - Vous √©valuez la confiance (hallucination ou pas)\n",
        "   - Ajoutez des documents pour am√©liorer futures requ√™tes\n",
        "\n",
        "5. **Challenge 5:** Explorez `similarity_search_with_score()` pour voir les distances d'embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJw23ONw9FBw"
      },
      "source": [
        "## üìñ Ressources suppl√©mentaires\n",
        "\n",
        "- [LangChain Documentation](https://python.langchain.com/)\n",
        "- [Chroma Vector Database](https://www.trychroma.com/)\n",
        "- [OpenAI Embeddings](https://platform.openai.com/docs/models/embeddings)\n",
        "- [RAG Pattern](https://python.langchain.com/docs/use_cases/question_answering/)\n",
        "- [CrewAI for Multi-Agent](https://crewai.io/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdY8KxrC9FBw"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüéâ Lab termin√©!\")\n",
        "print(\"‚úÖ Vous avez appris:\")\n",
        "print(\"   ‚Ä¢ Comment construire une m√©moire court-terme (context window)\")\n",
        "print(\"   ‚Ä¢ Comment construire une m√©moire long-terme (Vector DB)\")\n",
        "print(\"   ‚Ä¢ Comment combiner les deux pour des agents intelligents\")\n",
        "print(\"   ‚Ä¢ Comment √©viter les hallucinations avec RAG\")\n",
        "print(\"\\nüìù Sauvegardez ce notebook pour r√©f√©rences futures!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}