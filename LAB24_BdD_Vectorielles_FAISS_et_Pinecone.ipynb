{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/RAG/blob/main/LAB24_BdD_Vectorielles_FAISS_et_Pinecone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Lab 2: Vector Database Setup with FAISS and Pinecone\n",
        "\n",
        "**Objectif:** CrÃ©er des embeddings, les indexer avec FAISS localement, puis les dÃ©ployer sur une base vectorielle managÃ©e (Pinecone) pour un usage de niveau production.\n",
        "\n",
        "**DurÃ©e estimÃ©e:** 20â€“30 minutes\n",
        "\n",
        "**Livrable:** Scripts/notebook qui construisent, interrogent et persistent un index FAISS ; avec en plus un index Pinecone fonctionnel avec upsert/query.\n",
        "\n",
        "---\n",
        "\n",
        "## Table des matiÃ¨res\n",
        "1. Installation et configuration de l'environnement\n",
        "2. Corpus de documents exemple\n",
        "3. GÃ©nÃ©ration des embeddings (OpenAI)\n",
        "4. FAISS: construction, recherche, persistance (local)\n",
        "5. Pinecone: crÃ©ation d'index, upsert, query (managÃ©)\n",
        "6. (Optionnel) IntÃ©gration LangChain avec Pinecone\n",
        "7. Prochaines Ã©tapes et expÃ©rimentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section1"
      },
      "source": [
        "## 1. Installation et Configuration de l'Environnement\n",
        "\n",
        "â±ï¸ **Temps estimÃ©:** 5â€“10 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21451468-ec4e-4d19-bed3-2de5140a792a"
      },
      "source": [
        "# Installation des dÃ©pendances\n",
        "!pip install -q openai tiktoken faiss-cpu pinecone-client langchain langchain-openai langchain-pinecone chromadb python-dotenv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m259.3/259.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "prophet 1.2.2 requires numpy<2.4.0,>=1.15.4, but you have numpy 2.4.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env-setup"
      },
      "source": [
        "### Configuration des clÃ©s API\n",
        "\n",
        "**Option 1 (RecommandÃ©e pour Colab):** Utiliser les Secrets de Colab\n",
        "- Cliquez sur l'icÃ´ne ğŸ”‘ dans la barre latÃ©rale gauche\n",
        "- Ajoutez `OPENAI_API_KEY` et `PINECONE_API_KEY`\n",
        "- Activez l'accÃ¨s au notebook\n",
        "\n",
        "**Option 2:** DÃ©finir directement dans le code (âš ï¸ Ne pas partager le notebook publiquement)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "env-config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e543ed7-a1d0-4c84-a3fe-99cf28696cb7"
      },
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Option 1: RÃ©cupÃ©rer depuis les Secrets Colab\n",
        "try:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    os.environ[\"PINECONE_API_KEY\"] = userdata.get('PINECONE_API_KEY')\n",
        "    print(\"âœ… ClÃ©s API chargÃ©es depuis les Secrets Colab\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Secrets non trouvÃ©s: {e}\")\n",
        "    print(\"Veuillez configurer manuellement ci-dessous...\\n\")\n",
        "\n",
        "    # Option 2: Configuration manuelle (dÃ©commenter et remplir)\n",
        "    # os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Votre clÃ© OpenAI\n",
        "    # os.environ[\"PINECONE_API_KEY\"] = \"...\"   # Votre clÃ© Pinecone\n",
        "\n",
        "# Configuration du nom de l'index Pinecone\n",
        "os.environ[\"PINECONE_INDEX\"] = \"agentic-ai-lab\"\n",
        "\n",
        "# VÃ©rification\n",
        "if \"OPENAI_API_KEY\" in os.environ and \"PINECONE_API_KEY\" in os.environ:\n",
        "    print(\"âœ… Configuration complÃ¨te\")\n",
        "    print(f\"   - Index Pinecone: {os.environ['PINECONE_INDEX']}\")\n",
        "else:\n",
        "    print(\"âŒ Configuration incomplÃ¨te - veuillez dÃ©finir vos clÃ©s API\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ClÃ©s API chargÃ©es depuis les Secrets Colab\n",
            "âœ… Configuration complÃ¨te\n",
            "   - Index Pinecone: agentic-ai-lab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section2"
      },
      "source": [
        "## 2. Ensemble de documents servant d'exemple\n",
        "\n",
        "Nous utilisons un petit ensemble de donnÃ©es sur l'IA agentique.\n",
        "\n",
        "On pourra le remplacer par d'autres donnÃ©es plus tard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "corpus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e99d778-c9bc-4513-99c2-827af3ab67e7"
      },
      "source": [
        "# Documents exemple\n",
        "docs = [\n",
        "    {\"id\": \"d1\", \"text\": \"Agentic AI agents use tools, memory, and goals to act.\"},\n",
        "    {\"id\": \"d2\", \"text\": \"LangChain and CrewAI help orchestrate multi-agent workflows.\"},\n",
        "    {\"id\": \"d3\", \"text\": \"RAG retrieves external knowledge to improve answer accuracy.\"},\n",
        "    {\"id\": \"d4\", \"text\": \"Vector databases enable fast similarity search over embeddings.\"},\n",
        "    {\"id\": \"d5\", \"text\": \"Planning loops and ReAct improve reasoning in complex tasks.\"},\n",
        "]\n",
        "\n",
        "# RequÃªtes de test\n",
        "queries = [\n",
        "    \"How do agents use memory?\",\n",
        "    \"Name a framework for multi-agent orchestration.\",\n",
        "    \"Why is RAG useful?\"\n",
        "]\n",
        "\n",
        "print(f\"ğŸ“š Corpus: {len(docs)} documents\")\n",
        "print(f\"ğŸ” RequÃªtes: {len(queries)} questions\\n\")\n",
        "\n",
        "for doc in docs:\n",
        "    print(f\"  [{doc['id']}] {doc['text']}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“š Corpus: 5 documents\n",
            "ğŸ” RequÃªtes: 3 questions\n",
            "\n",
            "  [d1] Agentic AI agents use tools, memory, and goals to act.\n",
            "  [d2] LangChain and CrewAI help orchestrate multi-agent workflows.\n",
            "  [d3] RAG retrieves external knowledge to improve answer accuracy.\n",
            "  [d4] Vector databases enable fast similarity search over embeddings.\n",
            "  [d5] Planning loops and ReAct improve reasoning in complex tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section3"
      },
      "source": [
        "## 3. GÃ©nÃ©ration des Embeddings (OpenAI)\n",
        "\n",
        "Nous utilisons le modÃ¨le `text-embedding-3-small` d'OpenAI (rapide, 1536 dimensions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "embeddings",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e68d95-f282-4146-fb4c-16172dbbc303"
      },
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Initialisation du client OpenAI\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "EMB_MODEL = \"text-embedding-3-small\"  # Bon compromis performance/coÃ»t, 1536-d\n",
        "\n",
        "def get_embedding(text: str):\n",
        "    \"\"\"GÃ©nÃ¨re un embedding pour un texte donnÃ©.\"\"\"\n",
        "    response = client.embeddings.create(model=EMB_MODEL, input=text)\n",
        "    return response.data[0].embedding\n",
        "\n",
        "print(\"ğŸ”„ GÃ©nÃ©ration des embeddings...\\n\")\n",
        "\n",
        "# Embeddings des documents\n",
        "X = [get_embedding(d[\"text\"]) for d in docs]\n",
        "print(f\"âœ… Documents encodÃ©s: {len(X)} vecteurs\")\n",
        "\n",
        "# Embeddings des requÃªtes\n",
        "Q = [get_embedding(q) for q in queries]\n",
        "print(f\"âœ… RequÃªtes encodÃ©es: {len(Q)} vecteurs\")\n",
        "\n",
        "# Dimension des vecteurs\n",
        "dim = len(X[0])\n",
        "print(f\"\\nğŸ“Š Dimension des embeddings: {dim}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ GÃ©nÃ©ration des embeddings...\n",
            "\n",
            "âœ… Documents encodÃ©s: 5 vecteurs\n",
            "âœ… RequÃªtes encodÃ©es: 3 vecteurs\n",
            "\n",
            "ğŸ“Š Dimension des embeddings: 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section4"
      },
      "source": [
        "## 4. FAISS: construction, recherche, persistance (en local)\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) est une bibliothÃ¨que optimisÃ©e pour la recherche de similaritÃ©."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section4a"
      },
      "source": [
        "### 4a. Construction de l'index FAISS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faiss-build",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df6e90d-34d1-459e-f0bb-701dc32aa8d7"
      },
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Conversion en array NumPy\n",
        "xb = np.array(X, dtype=\"float32\")\n",
        "\n",
        "# CrÃ©ation d'un index avec produit scalaire (Inner Product)\n",
        "# Note: normalisation des vecteurs pour que IP â‰ˆ similaritÃ© cosinus\n",
        "index = faiss.IndexFlatIP(dim)\n",
        "\n",
        "# Normalisation L2 des vecteurs (pour similaritÃ© cosinus)\n",
        "faiss.normalize_L2(xb)\n",
        "\n",
        "# Ajout des vecteurs Ã  l'index\n",
        "index.add(xb)\n",
        "\n",
        "print(f\"âœ… Index FAISS crÃ©Ã©\")\n",
        "print(f\"ğŸ“Š Vecteurs indexÃ©s: {index.ntotal}\")\n",
        "print(f\"ğŸ“ Dimension: {index.d}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Index FAISS crÃ©Ã©\n",
            "ğŸ“Š Vecteurs indexÃ©s: 5\n",
            "ğŸ“ Dimension: 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section4b"
      },
      "source": [
        "### 4b. Recherche dans l'index FAISS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faiss-search",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de42efe-cf28-42fb-f6d7-43fa9381c3b6"
      },
      "source": [
        "def faiss_search(query_vec, k=3):\n",
        "    \"\"\"\n",
        "    Recherche les k vecteurs les plus similaires.\n",
        "\n",
        "    Args:\n",
        "        query_vec: Vecteur de requÃªte\n",
        "        k: Nombre de rÃ©sultats Ã  retourner\n",
        "\n",
        "    Returns:\n",
        "        D: Distances (scores de similaritÃ©)\n",
        "        I: Indices des documents correspondants\n",
        "    \"\"\"\n",
        "    q = np.array([query_vec], dtype=\"float32\")\n",
        "    faiss.normalize_L2(q)\n",
        "    D, I = index.search(q, k)  # distances, indices\n",
        "    return D[0], I[0]\n",
        "\n",
        "# Test des requÃªtes\n",
        "print(\"ğŸ” RÃ©sultats de recherche FAISS:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for qi, qv in enumerate(Q):\n",
        "    D, I = faiss_search(qv, k=2)\n",
        "    print(f\"\\nâ“ Query: {queries[qi]}\")\n",
        "    print(\"-\" * 80)\n",
        "    for rank, (score, idx) in enumerate(zip(D, I), 1):\n",
        "        print(f\"  {rank}. [{docs[idx]['id']}] score={round(float(score), 4)}\")\n",
        "        print(f\"     â†’ {docs[idx]['text']}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” RÃ©sultats de recherche FAISS:\n",
            "\n",
            "================================================================================\n",
            "\n",
            "â“ Query: How do agents use memory?\n",
            "--------------------------------------------------------------------------------\n",
            "  1. [d1] score=0.6091\n",
            "     â†’ Agentic AI agents use tools, memory, and goals to act.\n",
            "  2. [d2] score=0.3129\n",
            "     â†’ LangChain and CrewAI help orchestrate multi-agent workflows.\n",
            "\n",
            "â“ Query: Name a framework for multi-agent orchestration.\n",
            "--------------------------------------------------------------------------------\n",
            "  1. [d2] score=0.5671\n",
            "     â†’ LangChain and CrewAI help orchestrate multi-agent workflows.\n",
            "  2. [d1] score=0.3924\n",
            "     â†’ Agentic AI agents use tools, memory, and goals to act.\n",
            "\n",
            "â“ Query: Why is RAG useful?\n",
            "--------------------------------------------------------------------------------\n",
            "  1. [d3] score=0.6225\n",
            "     â†’ RAG retrieves external knowledge to improve answer accuracy.\n",
            "  2. [d5] score=0.3436\n",
            "     â†’ Planning loops and ReAct improve reasoning in complex tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section4c"
      },
      "source": [
        "### 4c. Sauvegarde et Chargement de l'index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faiss-persist",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5535c96-161d-4e0a-f90e-59d845231430"
      },
      "source": [
        "# Sauvegarde de l'index\n",
        "index_file = \"faiss_agentic.index\"\n",
        "faiss.write_index(index, index_file)\n",
        "print(f\"ğŸ’¾ Index sauvegardÃ©: {index_file}\")\n",
        "\n",
        "# Rechargement de l'index\n",
        "index2 = faiss.read_index(index_file)\n",
        "print(f\"ğŸ“‚ Index rechargÃ©: {index2.ntotal} vecteurs\")\n",
        "\n",
        "# VÃ©rification que l'index rechargÃ© fonctionne\n",
        "test_query = Q[0]\n",
        "q_test = np.array([test_query], dtype=\"float32\")\n",
        "faiss.normalize_L2(q_test)\n",
        "D_test, I_test = index2.search(q_test, 1)\n",
        "print(f\"\\nâœ… Test de l'index rechargÃ©:\")\n",
        "print(f\"   Meilleur rÃ©sultat: [{docs[I_test[0][0]]['id']}] score={round(float(D_test[0][0]), 4)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Index sauvegardÃ©: faiss_agentic.index\n",
            "ğŸ“‚ Index rechargÃ©: 5 vecteurs\n",
            "\n",
            "âœ… Test de l'index rechargÃ©:\n",
            "   Meilleur rÃ©sultat: [d1] score=0.6091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section4d"
      },
      "source": [
        "### 4d. Mapping des MÃ©tadonnÃ©es\n",
        "\n",
        "âš ï¸ **Important:** FAISS ne stocke que les vecteurs. Vous devez maintenir un mapping sÃ©parÃ© IDâ†’mÃ©tadonnÃ©es."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faiss-metadata",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686b400f-0cb7-4e52-d40d-ed791305753f"
      },
      "source": [
        "import json\n",
        "import pickle\n",
        "\n",
        "# CrÃ©ation du mapping index â†’ document\n",
        "id_map = {i: d for i, d in enumerate(docs)}\n",
        "\n",
        "print(\"ğŸ“‹ Mapping ID â†’ MÃ©tadonnÃ©es:\")\n",
        "for idx, doc in id_map.items():\n",
        "    print(f\"  Index {idx} â†’ Document '{doc['id']}'\")\n",
        "\n",
        "# Sauvegarde du mapping (JSON)\n",
        "with open(\"faiss_metadata.json\", \"w\") as f:\n",
        "    json.dump(id_map, f, indent=2)\n",
        "print(\"\\nğŸ’¾ MÃ©tadonnÃ©es sauvegardÃ©es: faiss_metadata.json\")\n",
        "\n",
        "# Alternative: sauvegarde en pickle (plus rapide pour gros volumes)\n",
        "with open(\"faiss_metadata.pkl\", \"wb\") as f:\n",
        "    pickle.dump(id_map, f)\n",
        "print(\"ğŸ’¾ MÃ©tadonnÃ©es sauvegardÃ©es: faiss_metadata.pkl\")\n",
        "\n",
        "# Rechargement\n",
        "with open(\"faiss_metadata.json\", \"r\") as f:\n",
        "    id_map_loaded = json.load(f)\n",
        "    # Conversion des clÃ©s de string Ã  int\n",
        "    id_map_loaded = {int(k): v for k, v in id_map_loaded.items()}\n",
        "\n",
        "print(f\"\\nâœ… MÃ©tadonnÃ©es rechargÃ©es: {len(id_map_loaded)} entrÃ©es\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‹ Mapping ID â†’ MÃ©tadonnÃ©es:\n",
            "  Index 0 â†’ Document 'd1'\n",
            "  Index 1 â†’ Document 'd2'\n",
            "  Index 2 â†’ Document 'd3'\n",
            "  Index 3 â†’ Document 'd4'\n",
            "  Index 4 â†’ Document 'd5'\n",
            "\n",
            "ğŸ’¾ MÃ©tadonnÃ©es sauvegardÃ©es: faiss_metadata.json\n",
            "ğŸ’¾ MÃ©tadonnÃ©es sauvegardÃ©es: faiss_metadata.pkl\n",
            "\n",
            "âœ… MÃ©tadonnÃ©es rechargÃ©es: 5 entrÃ©es\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section5"
      },
      "source": [
        "## 5. Pinecone: CrÃ©ation d'Index, Upsert, Query (ManagÃ©)\n",
        "\n",
        "Pinecone est une base vectorielle managÃ©e optimisÃ©e pour la production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section5-init"
      },
      "source": [
        "### Initialisation et crÃ©ation de l'Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pinecone-init",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5d56f6a-3b0e-4a37-e1df-23f479bcda80"
      },
      "source": [
        "import time\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Initialisation du client Pinecone (v3)\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "index_name = os.getenv(\"PINECONE_INDEX\", \"agentic-ai-lab\")\n",
        "\n",
        "print(f\"ğŸ”— Connexion Ã  Pinecone...\")\n",
        "print(f\"ğŸ“‡ Index cible: {index_name}\\n\")\n",
        "\n",
        "# Liste des index existants\n",
        "existing_indexes = [i[\"name\"] for i in pc.list_indexes()]\n",
        "print(f\"ğŸ“‹ Index existants: {existing_indexes}\")\n",
        "\n",
        "# CrÃ©ation de l'index s'il n'existe pas\n",
        "if index_name not in existing_indexes:\n",
        "    print(f\"\\nğŸ—ï¸ CrÃ©ation de l'index '{index_name}'...\")\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=dim,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",\n",
        "            region=\"us-east-1\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Attente que l'index soit prÃªt\n",
        "    print(\"â³ Attente de l'initialisation de l'index...\")\n",
        "    while True:\n",
        "        desc = pc.describe_index(index_name)\n",
        "        if desc.status[\"ready\"]:\n",
        "            print(\"âœ… Index prÃªt!\")\n",
        "            break\n",
        "        print(\"   Toujours en cours...\")\n",
        "        time.sleep(2)\n",
        "else:\n",
        "    print(f\"âœ… Index '{index_name}' existe dÃ©jÃ \")\n",
        "\n",
        "# Connexion Ã  l'index\n",
        "index_pinecone = pc.Index(index_name)\n",
        "stats = index_pinecone.describe_index_stats()\n",
        "print(f\"\\nğŸ“Š Statistiques de l'index:\")\n",
        "print(f\"   - Vecteurs: {stats['total_vector_count']}\")\n",
        "print(f\"   - Dimension: {stats.get('dimension', 'N/A')}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”— Connexion Ã  Pinecone...\n",
            "ğŸ“‡ Index cible: agentic-ai-lab\n",
            "\n",
            "ğŸ“‹ Index existants: ['agentic-ai-lab']\n",
            "âœ… Index 'agentic-ai-lab' existe dÃ©jÃ \n",
            "\n",
            "ğŸ“Š Statistiques de l'index:\n",
            "   - Vecteurs: 0\n",
            "   - Dimension: 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section5a"
      },
      "source": [
        "### 5a. Upsert des Vecteurs avec MÃ©tadonnÃ©es"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pinecone-upsert",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62da231b-2325-4474-8cad-9c7161714708"
      },
      "source": [
        "def normalize(v):\n",
        "    \"\"\"Normalise un vecteur (optionnel pour Pinecone cosine, mais recommandÃ© pour la cohÃ©rence.\"\"\"\n",
        "    v = np.array(v, dtype=\"float32\")\n",
        "    n = np.linalg.norm(v)\n",
        "    return (v / n).tolist() if n > 0 else v.tolist()\n",
        "\n",
        "# PrÃ©paration des vecteurs avec mÃ©tadonnÃ©es\n",
        "vectors = [\n",
        "    {\n",
        "        \"id\": d[\"id\"],\n",
        "        \"values\": normalize(vec),\n",
        "        \"metadata\": {\"text\": d[\"text\"]}\n",
        "    }\n",
        "    for d, vec in zip(docs, X)\n",
        "]\n",
        "\n",
        "print(f\"ğŸ“¤ Upsert de {len(vectors)} vecteurs dans Pinecone...\\n\")\n",
        "\n",
        "# Upsert dans Pinecone\n",
        "upsert_response = index_pinecone.upsert(vectors=vectors)\n",
        "\n",
        "print(f\"âœ… Upsert terminÃ©\")\n",
        "print(f\"   - Vecteurs insÃ©rÃ©s: {upsert_response['upserted_count']}\")\n",
        "\n",
        "# Attente de la synchronisation\n",
        "time.sleep(2)\n",
        "\n",
        "# VÃ©rification\n",
        "stats = index_pinecone.describe_index_stats()\n",
        "print(f\"\\nğŸ“Š Statistiques mises Ã  jour:\")\n",
        "print(f\"   - Total vecteurs: {stats['total_vector_count']}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¤ Upsert de 5 vecteurs dans Pinecone...\n",
            "\n",
            "âœ… Upsert terminÃ©\n",
            "   - Vecteurs insÃ©rÃ©s: 5\n",
            "\n",
            "ğŸ“Š Statistiques mises Ã  jour:\n",
            "   - Total vecteurs: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section5b"
      },
      "source": [
        "### 5b. Recherche dans Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pinecone-query",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a09557-8a66-4a79-ae08-88cea4dad3c1"
      },
      "source": [
        "def pinecone_search(query_vec, top_k=3):\n",
        "    \"\"\"\n",
        "    Recherche les k vecteurs les plus similaires dans Pinecone.\n",
        "\n",
        "    Args:\n",
        "        query_vec: Vecteur de requÃªte\n",
        "        top_k: Nombre de rÃ©sultats Ã  retourner\n",
        "\n",
        "    Returns:\n",
        "        RÃ©sultats de la requÃªte Pinecone\n",
        "    \"\"\"\n",
        "    res = index_pinecone.query(\n",
        "        vector=normalize(query_vec),\n",
        "        top_k=top_k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    return res\n",
        "\n",
        "# Test des requÃªtes\n",
        "print(\"ğŸ” RÃ©sultats de recherche Pinecone:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for qi, qv in enumerate(Q):\n",
        "    res = pinecone_search(qv, top_k=2)\n",
        "    print(f\"\\nâ“ Query: {queries[qi]}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for rank, match in enumerate(res[\"matches\"], 1):\n",
        "        print(f\"  {rank}. [{match['id']}] score={round(match['score'], 4)}\")\n",
        "        print(f\"     â†’ {match['metadata']['text']}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” RÃ©sultats de recherche Pinecone:\n",
            "\n",
            "================================================================================\n",
            "\n",
            "â“ Query: How do agents use memory?\n",
            "--------------------------------------------------------------------------------\n",
            "  1. [d1] score=0.6094\n",
            "     â†’ Agentic AI agents use tools, memory, and goals to act.\n",
            "  2. [d2] score=0.313\n",
            "     â†’ LangChain and CrewAI help orchestrate multi-agent workflows.\n",
            "\n",
            "â“ Query: Name a framework for multi-agent orchestration.\n",
            "--------------------------------------------------------------------------------\n",
            "  1. [d2] score=0.5674\n",
            "     â†’ LangChain and CrewAI help orchestrate multi-agent workflows.\n",
            "  2. [d1] score=0.3923\n",
            "     â†’ Agentic AI agents use tools, memory, and goals to act.\n",
            "\n",
            "â“ Query: Why is RAG useful?\n",
            "--------------------------------------------------------------------------------\n",
            "  1. [d3] score=0.6228\n",
            "     â†’ RAG retrieves external knowledge to improve answer accuracy.\n",
            "  2. [d5] score=0.344\n",
            "     â†’ Planning loops and ReAct improve reasoning in complex tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section5c"
      },
      "source": [
        "### 5c. (Optionnel) Suppression et Nettoyage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pinecone-cleanup"
      },
      "source": [
        "# âš ï¸ ATTENTION: DÃ©commenter uniquement si vous voulez supprimer des donnÃ©es\n",
        "\n",
        "# # Suppression d'un seul vecteur\n",
        "# print(\"ğŸ—‘ï¸ Suppression du vecteur 'd5'...\")\n",
        "# index_pinecone.delete(ids=[\"d5\"])\n",
        "# print(\"âœ… Vecteur supprimÃ©\")\n",
        "\n",
        "# # Suppression de TOUS les vecteurs de l'index (mais garde l'index)\n",
        "# print(\"ğŸ—‘ï¸ Suppression de TOUS les vecteurs...\")\n",
        "# index_pinecone.delete(delete_all=True)\n",
        "# print(\"âœ… Index vidÃ©\")\n",
        "\n",
        "# # Suppression complÃ¨te de l'index (âš ï¸ DANGER!)\n",
        "# # print(f\"ğŸ—‘ï¸ SUPPRESSION DE L'INDEX '{index_name}'...\")\n",
        "# # pc.delete_index(index_name)\n",
        "# # print(\"âœ… Index supprimÃ©\")\n",
        "\n",
        "print(\"â„¹ï¸ Section cleanup - dÃ©commenter le code pour activer les suppressions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6"
      },
      "source": [
        "## 6. (Optionnel) IntÃ©gration LangChain avec Pinecone\n",
        "\n",
        "Si vous prÃ©voyez d'utiliser des chaÃ®nes/agents LangChain, voici comment intÃ©grer Pinecone comme retriever."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "langchain-integration",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559c9d07-b6fb-4213-d3e0-48ed11f09ec0"
      },
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "print(\"ğŸ”— Initialisation du retriever LangChain avec Pinecone...\\n\")\n",
        "\n",
        "# CrÃ©ation de l'objet embeddings LangChain\n",
        "lc_embeddings = OpenAIEmbeddings(model=EMB_MODEL)\n",
        "\n",
        "# CrÃ©ation du vector store LangChain\n",
        "store = PineconeVectorStore(\n",
        "    index_name=index_name,\n",
        "    embedding=lc_embeddings\n",
        ")\n",
        "\n",
        "print(\"âœ… Vector store LangChain crÃ©Ã©\\n\")\n",
        "\n",
        "# Test de recherche avec LangChain\n",
        "print(\"ğŸ” Test de recherche LangChain:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_query = \"How do agents use memory?\"\n",
        "print(f\"â“ Query: {test_query}\\n\")\n",
        "\n",
        "docs_found = store.similarity_search(test_query, k=2)\n",
        "\n",
        "for i, doc in enumerate(docs_found, 1):\n",
        "    print(f\"{i}. {doc.page_content}\")\n",
        "    if doc.metadata:\n",
        "        print(f\"   Metadata: {doc.metadata}\")\n",
        "\n",
        "print(\"\\nâœ… IntÃ©gration LangChain opÃ©rationnelle\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”— Initialisation du retriever LangChain avec Pinecone...\n",
            "\n",
            "âœ… Vector store LangChain crÃ©Ã©\n",
            "\n",
            "ğŸ” Test de recherche LangChain:\n",
            "\n",
            "================================================================================\n",
            "â“ Query: How do agents use memory?\n",
            "\n",
            "1. Agentic AI agents use tools, memory, and goals to act.\n",
            "2. LangChain and CrewAI help orchestrate multi-agent workflows.\n",
            "\n",
            "âœ… IntÃ©gration LangChain opÃ©rationnelle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section7"
      },
      "source": [
        "## 7. ExpÃ©rimentations et Prochaines Ã‰tapes\n",
        "\n",
        "### ğŸ§ª Que tester maintenant?\n",
        "\n",
        "1. **MÃ©triques de distance:**\n",
        "   - Comparer cosine â†” dot product (ajuster la normalisation)\n",
        "   - Tester avec/sans normalisation sur Pinecone\n",
        "\n",
        "2. **Performance et scalabilitÃ©:**\n",
        "   - Augmenter Ã  plusieurs milliers de documents\n",
        "   - Mesurer la latence FAISS vs Pinecone\n",
        "   - Tester diffÃ©rents types d'index FAISS (IVF, HNSW)\n",
        "\n",
        "3. **MÃ©tadonnÃ©es enrichies:**\n",
        "   - Ajouter: titre, URL, tags, timestamp, catÃ©gorie\n",
        "   - Utiliser les filtres de mÃ©tadonnÃ©es dans Pinecone\n",
        "   - Exemple: `filter={\"category\": \"technical\"}`\n",
        "\n",
        "4. **IntÃ©gration RAG:**\n",
        "   - Connecter ce retriever Ã  une chaÃ®ne RAG (Semaine 9)\n",
        "   - Construire un agent avec accÃ¨s Ã  cette base de connaissances\n",
        "   - ImplÃ©menter un systÃ¨me de question-rÃ©ponse complet\n",
        "\n",
        "5. **Corpus personnalisÃ©:**\n",
        "   - Remplacer les documents exemple par vos propres donnÃ©es\n",
        "   - Charger depuis des fichiers (PDF, TXT, JSON)\n",
        "   - ImplÃ©menter du chunking pour documents longs\n",
        "\n",
        "### ğŸ“š Ressources supplÃ©mentaires\n",
        "\n",
        "- [FAISS Documentation](https://github.com/facebookresearch/faiss/wiki)\n",
        "- [Pinecone Documentation](https://docs.pinecone.io/)\n",
        "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
        "- [LangChain Vector Stores](https://python.langchain.com/docs/modules/data_connection/vectorstores/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "experiments"
      },
      "source": [
        "### Cellule d'expÃ©rimentation libre\n",
        "\n",
        "Utilisez cette cellule pour vos propres tests et expÃ©rimentations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "playground"
      },
      "source": [
        "# ğŸ§ª Zone d'expÃ©rimentation\n",
        "# Testez vos propres requÃªtes et documents ici\n",
        "\n",
        "# Exemple: Test avec une nouvelle requÃªte\n",
        "# new_query = \"Your question here\"\n",
        "# new_query_vec = get_embedding(new_query)\n",
        "\n",
        "# # FAISS\n",
        "# D, I = faiss_search(new_query_vec, k=3)\n",
        "# print(\"FAISS Results:\")\n",
        "# for score, idx in zip(D, I):\n",
        "#     print(f\"  {docs[idx]['id']}: {docs[idx]['text']} (score: {score:.4f})\")\n",
        "\n",
        "# # Pinecone\n",
        "# results = pinecone_search(new_query_vec, top_k=3)\n",
        "# print(\"\\nPinecone Results:\")\n",
        "# for match in results['matches']:\n",
        "#     print(f\"  {match['id']}: {match['metadata']['text']} (score: {match['score']:.4f})\")\n",
        "\n",
        "print(\"ğŸ’¡ DÃ©commentez le code ci-dessus pour tester vos propres requÃªtes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## ğŸ“‹ RÃ©capitulatif\n",
        "\n",
        "### âœ… Ce que vous avez accompli:\n",
        "\n",
        "1. âœ… Installation et configuration de l'environnement (OpenAI + Pinecone)\n",
        "2. âœ… GÃ©nÃ©ration d'embeddings avec OpenAI `text-embedding-3-small`\n",
        "3. âœ… Construction et interrogation d'un index FAISS local\n",
        "4. âœ… Persistance et rechargement d'index FAISS\n",
        "5. âœ… Gestion des mÃ©tadonnÃ©es pour FAISS\n",
        "6. âœ… CrÃ©ation et configuration d'un index Pinecone\n",
        "7. âœ… Upsert de vecteurs avec mÃ©tadonnÃ©es dans Pinecone\n",
        "8. âœ… Recherche de similaritÃ© dans Pinecone\n",
        "9. âœ… IntÃ©gration avec LangChain\n",
        "\n",
        "### ğŸ¯ Prochaines Ã©tapes:\n",
        "\n",
        "- ExpÃ©rimenter avec vos propres donnÃ©es\n",
        "- Optimiser les performances pour des corpus plus larges\n",
        "- IntÃ©grer dans un systÃ¨me RAG complet\n",
        "- Explorer les filtres de mÃ©tadonnÃ©es avancÃ©s\n",
        "\n",
        "### ğŸ’¾ Fichiers gÃ©nÃ©rÃ©s:\n",
        "\n",
        "- `faiss_agentic.index` - Index FAISS persistÃ©\n",
        "- `faiss_metadata.json` - MÃ©tadonnÃ©es des documents (JSON)\n",
        "- `faiss_metadata.pkl` - MÃ©tadonnÃ©es des documents (Pickle)\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ‰ FÃ©licitations! Vous avez maintenant un workflow local FAISS pour le prototypage rapide et un index Pinecone managÃ© prÃªt pour la production!**"
      ]
    }
  ]
}