{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/RAG/blob/main/rag_langchain_2026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nj_V2UhXWWY"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/docling-project/docling/blob/main/docs/examples/rag_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZkHldTkXWWZ"
      },
      "source": [
        "# RAG with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdRs9WNhXWWZ"
      },
      "source": [
        "| Step | Tech | Execution |\n",
        "| --- | --- | --- |\n",
        "| Embedding | Hugging Face / Sentence Transformers | ðŸ’» Local |\n",
        "| Vector store | Milvus | ðŸ’» Local |\n",
        "| Gen AI | Hugging Face Inference API | ðŸŒ Remote |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWWQhkFXWWZ"
      },
      "source": [
        "Cet exemple exploite l'intÃ©gration\n",
        "[LangChain Docling](../../integrations/langchain/), ainsi qu'une base de donnÃ©es vectorielles Milvus,\n",
        "et des sentence-transformers embeddings.\n",
        "\n",
        "Le composant `DoclingLoader` permetÂ :\n",
        "\n",
        "- d'utiliser facilement et rapidement diffÃ©rents types de documents dans les applications LLM, et\n",
        "- d'exploiter le format riche de Docling pour un ancrage avancÃ© et natif du document.\n",
        "\n",
        "`DoclingLoader` prend en charge deux modes d'exportationÂ :\n",
        "\n",
        "- `ExportType.MARKDOWN`Â : si on souhaite collecter chaque document d'entrÃ©e comme un document LangChain distinct,\n",
        "\n",
        "ou\n",
        "- `ExportType.DOC_CHUNKS` (par dÃ©faut)Â : si on souhaite dÃ©couper chaque document d'entrÃ©e en shards (segments) et\n",
        "\n",
        "capturer ensuite chaque shard comme un document LangChain distinct.\n",
        "\n",
        "L'exemple permet d'explorer les deux modes via le paramÃ¨tre `EXPORT_TYPE`Â ; selon la valeur dÃ©finie, le pipeline de l'exemple est configurÃ© en consÃ©quence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKrhoOqSXWWa"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMGZ3DZBXWWa"
      },
      "source": [
        "ðŸ‘‰ Pour une vitesse de conversion optimale, utilisez l'accÃ©lÃ©ration GPU quand c'est possibleÂ ; par exemple, si vous utilisez Colab, utilisez un environnement d'exÃ©cution compatible GPU.\n",
        "\n",
        "Ce notebook utilise l'API d'infÃ©rence de Hugging FaceÂ ; pour augmenter le quota LLM, on peut fournir un jeton via la variable d'environnement HF_TOKEN.\n",
        "\n",
        "Les dÃ©pendances peuvent Ãªtre installÃ©es comme indiquÃ© ci-dessous (l'option `--no-warn-conflicts` est destinÃ©e Ã  l'environnement Python prÃ©configurÃ© de ColabÂ ; on peut la supprimer pour une utilisation plus stricte)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtKF4ID7XWWa"
      },
      "outputs": [],
      "source": [
        "%pip install -q --progress-bar off --no-warn-conflicts  langchain-classic langchain-docling langchain-core langchain-huggingface langchain_milvus pymilvus[milvus_lite] langchain python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksptp-d7XWWb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_docling.loader import ExportType\n",
        "\n",
        "\n",
        "def _get_env_from_colab_or_os(key):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "\n",
        "        try:\n",
        "            return userdata.get(key)\n",
        "        except userdata.SecretNotFoundError:\n",
        "            pass\n",
        "    except ImportError:\n",
        "        pass\n",
        "    return os.getenv(key)\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# https://github.com/huggingface/transformers/issues/5486:\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "HF_TOKEN = _get_env_from_colab_or_os(\"HF_TOKEN\")\n",
        "FILE_PATH = [\"https://arxiv.org/pdf/2408.09869\"]  # Docling Technical Report\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "#GEN_MODEL_ID = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "GEN_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "EXPORT_TYPE = ExportType.DOC_CHUNKS\n",
        "QUESTION = \"Which are the main AI models in Docling?\"\n",
        "PROMPT = PromptTemplate.from_template(\n",
        "    \"Context information is below.\\n---------------------\\n{context}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {input}\\nAnswer:\\n\",\n",
        ")\n",
        "TOP_K = 3\n",
        "MILVUS_URI = str(Path(mkdtemp()) / \"docling.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoJR3FpuXWWb"
      },
      "source": [
        "## Chargement des documents\n",
        "\n",
        "Nous pouvons maintenant instancier notre chargeur et charger les documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg0jatz5XWWc"
      },
      "outputs": [],
      "source": [
        "from langchain_docling import DoclingLoader\n",
        "\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "loader = DoclingLoader(\n",
        "    file_path=FILE_PATH,\n",
        "    export_type=EXPORT_TYPE,\n",
        "    chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        ")\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb1ozPHeXWWc"
      },
      "source": [
        "> RemarqueÂ : le message indiquant  `\"Token indices sequence length is longer than the specified\n",
        "maximum sequence length...\"`  c'est-Ã -dire  `\"La longueur de la sÃ©quence des indices de jetons est supÃ©rieure Ã  la\n",
        "longueur de sÃ©quence maximale spÃ©cifiÃ©eâ€¦Â \"`  peut Ãªtre ignorÃ© dans ce casÂ â€” dÃ©tails\n",
        "\n",
        "[ici](https://github.com/docling-project/docling-core/issues/119#issuecomment-2577418826)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJcjaooVXWWc"
      },
      "source": [
        "DÃ©termination des splits (divisions) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmHhCAy8XWWc"
      },
      "outputs": [],
      "source": [
        "if EXPORT_TYPE == ExportType.DOC_CHUNKS:\n",
        "    splits = docs\n",
        "elif EXPORT_TYPE == ExportType.MARKDOWN:\n",
        "    from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "\n",
        "    splitter = MarkdownHeaderTextSplitter(\n",
        "        headers_to_split_on=[\n",
        "            (\"#\", \"Header_1\"),\n",
        "            (\"##\", \"Header_2\"),\n",
        "            (\"###\", \"Header_3\"),\n",
        "        ],\n",
        "    )\n",
        "    splits = [split for doc in docs for split in splitter.split_text(doc.page_content)]\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected export type: {EXPORT_TYPE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCion89TXWWc"
      },
      "source": [
        "Regardons quelques splits exemple :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6Y3-NahXWWc"
      },
      "outputs": [],
      "source": [
        "for d in splits[:3]:\n",
        "    print(f\"- {d.page_content=}\")\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kz367ehXWWd"
      },
      "source": [
        "## Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd1XeFATXWWd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_milvus import Milvus\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "\n",
        "milvus_uri = str(Path(mkdtemp()) / \"docling.db\")  # or set as needed\n",
        "vectorstore = Milvus.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    collection_name=\"docling_demo\",\n",
        "    connection_args={\"uri\": milvus_uri},\n",
        "    index_params={\"index_type\": \"FLAT\"},\n",
        "    drop_old=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcc9yphlXWWd"
      },
      "source": [
        "## RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFn3Z9eZXWWd"
      },
      "outputs": [],
      "source": [
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
        "\n",
        "# CrÃ©er l'endpoint avec task conversational\n",
        "endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=GEN_MODEL_ID,\n",
        "    huggingfacehub_api_token=HF_TOKEN,\n",
        "    task=\"conversational\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "# Wrapper pour compatibilitÃ© avec les chaÃ®nes LangChain\n",
        "llm = ChatHuggingFace(llm=endpoint)\n",
        "\n",
        "\n",
        "def clip_text(text, threshold=100):\n",
        "    return f\"{text[:threshold]}...\" if len(text) > threshold else text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7W3EUWzXWWd"
      },
      "outputs": [],
      "source": [
        "question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "resp_dict = rag_chain.invoke({\"input\": QUESTION})\n",
        "\n",
        "clipped_answer = clip_text(resp_dict[\"answer\"], threshold=200)\n",
        "print(f\"Question:\\n{resp_dict['input']}\\n\\nAnswer:\\n{clipped_answer}\")\n",
        "for i, doc in enumerate(resp_dict[\"context\"]):\n",
        "    print()\n",
        "    print(f\"Source {i + 1}:\")\n",
        "    print(f\"  text: {json.dumps(clip_text(doc.page_content, threshold=350))}\")\n",
        "    for key in doc.metadata:\n",
        "        if key != \"pk\":\n",
        "            val = doc.metadata.get(key)\n",
        "            clipped_val = clip_text(val) if isinstance(val, str) else val\n",
        "            print(f\"  {key}: {clipped_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-YrkCtiXWWd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}