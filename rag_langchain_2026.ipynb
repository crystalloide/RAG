{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/RAG/blob/main/rag_langchain_2026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZkHldTkXWWZ"
      },
      "source": [
        "# RAG with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdRs9WNhXWWZ"
      },
      "source": [
        "| Step | Tech | Execution |\n",
        "| --- | --- | --- |\n",
        "| Embedding | Hugging Face / Sentence Transformers | üíª Local |\n",
        "| Vector store | Milvus | üíª Local |\n",
        "| Gen AI | Hugging Face Inference API | üåê Remote |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vue d'ensemble\n",
        "Ce notebook impl√©mente un syst√®me RAG qui permet de poser des questions sur des documents PDF en utilisant :\n",
        "\n",
        "- Docling pour extraire et d√©couper les documents\n",
        "- Milvus comme base de donn√©es vectorielle\n",
        "- Hugging Face pour les embeddings et le mod√®le de g√©n√©ration"
      ],
      "metadata": {
        "id": "Ub-E3rrjgXze"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWWQhkFXWWZ"
      },
      "source": [
        "Cet exemple exploite l'int√©gration\n",
        "[LangChain Docling](../../integrations/langchain/), ainsi qu'une base de donn√©es vectorielles Milvus,\n",
        "et des sentence-transformers embeddings.\n",
        "\n",
        "Le composant `DoclingLoader` permet¬†:\n",
        "\n",
        "- d'utiliser facilement et rapidement diff√©rents types de documents dans les applications LLM, et\n",
        "- d'exploiter le format riche de Docling pour un ancrage avanc√© et natif du document.\n",
        "\n",
        "`DoclingLoader` prend en charge deux modes d'exportation¬†:\n",
        "\n",
        "- `ExportType.MARKDOWN`¬†: si on souhaite collecter chaque document d'entr√©e comme un document LangChain distinct,\n",
        "\n",
        "ou\n",
        "- `ExportType.DOC_CHUNKS` (par d√©faut)¬†: si on souhaite d√©couper chaque document d'entr√©e en shards (segments) et\n",
        "\n",
        "capturer ensuite chaque shard comme un document LangChain distinct.\n",
        "\n",
        "L'exemple permet d'explorer les deux modes via le param√®tre `EXPORT_TYPE`¬†; selon la valeur d√©finie, le pipeline de l'exemple est configur√© en cons√©quence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKrhoOqSXWWa"
      },
      "source": [
        "## Configuration :"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Le notebook configure l'environnement avec :\n",
        "- Les d√©pendances n√©cessaires (LangChain, Docling, Milvus, etc.)\n",
        "- Les param√®tres cl√©s :\n",
        "  - FILE_PATH : Le document PDF √† analyser (ici, le rapport technique de Docling)\n",
        "  - EMBED_MODEL_ID : Le mod√®le pour cr√©er les embeddings des textes\n",
        "  - GEN_MODEL_ID : Le mod√®le LLM pour g√©n√©rer les r√©ponses (Mistral-7B)\n",
        "  - EXPORT_TYPE : Mode de d√©coupage des documents (DOC_CHUNKS ou MARKDOWN)\n",
        "  - QUESTION : La question √† poser au syst√®me"
      ],
      "metadata": {
        "id": "10BCElSsgjfQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMGZ3DZBXWWa"
      },
      "source": [
        "üëâ Pour une vitesse de conversion optimale, utilisez l'acc√©l√©ration GPU quand c'est possible¬†; par exemple, si vous utilisez Colab, utilisez un environnement d'ex√©cution compatible GPU.\n",
        "\n",
        "Ce notebook utilise l'API d'inf√©rence de Hugging Face¬†; pour augmenter le quota LLM, on peut fournir un jeton via la variable d'environnement HF_TOKEN.\n",
        "\n",
        "Les d√©pendances peuvent √™tre install√©es comme indiqu√© ci-dessous (l'option `--no-warn-conflicts` est destin√©e √† l'environnement Python pr√©configur√© de Colab¬†; on peut la supprimer pour une utilisation plus stricte)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtKF4ID7XWWa"
      },
      "outputs": [],
      "source": [
        "%pip install -q --progress-bar off --no-warn-conflicts  langchain-classic langchain-docling langchain-core langchain-huggingface langchain_milvus pymilvus[milvus_lite] langchain python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksptp-d7XWWb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_docling.loader import ExportType\n",
        "\n",
        "\n",
        "def _get_env_from_colab_or_os(key):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "\n",
        "        try:\n",
        "            return userdata.get(key)\n",
        "        except userdata.SecretNotFoundError:\n",
        "            pass\n",
        "    except ImportError:\n",
        "        pass\n",
        "    return os.getenv(key)\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# https://github.com/huggingface/transformers/issues/5486:\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "HF_TOKEN = _get_env_from_colab_or_os(\"HF_TOKEN\")\n",
        "FILE_PATH = [\"https://arxiv.org/pdf/2408.09869\"]  # Docling Technical Report\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "#GEN_MODEL_ID = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "GEN_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "EXPORT_TYPE = ExportType.DOC_CHUNKS\n",
        "QUESTION = \"Which are the main AI models in Docling?\"\n",
        "PROMPT = PromptTemplate.from_template(\n",
        "    \"Context information is below.\\n---------------------\\n{context}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {input}\\nAnswer:\\n\",\n",
        ")\n",
        "TOP_K = 3\n",
        "MILVUS_URI = str(Path(mkdtemp()) / \"docling.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoJR3FpuXWWb"
      },
      "source": [
        "## Chargement des documents\n",
        "\n",
        "Nous pouvons maintenant instancier notre chargeur et charger les documents."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DoclingLoader t√©l√©charge et parse le PDF\n",
        "\n",
        "Le document est automatiquement d√©coup√© en \"chunks\" (morceaux) selon la strat√©gie choisie\n",
        "- Deux modes possibles :\n",
        "  - DOC_CHUNKS : D√©coupage intelligent en segments\n",
        "  - MARKDOWN : Conversion en Markdown puis d√©coupage par en-t√™tes"
      ],
      "metadata": {
        "id": "eDMSKGRcg5er"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg0jatz5XWWc"
      },
      "outputs": [],
      "source": [
        "from langchain_docling import DoclingLoader\n",
        "\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "loader = DoclingLoader(\n",
        "    file_path=FILE_PATH,\n",
        "    export_type=EXPORT_TYPE,\n",
        "    chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        ")\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb1ozPHeXWWc"
      },
      "source": [
        "> Remarque¬†: le message indiquant  `\"Token indices sequence length is longer than the specified\n",
        "maximum sequence length...\"`  c'est-√†-dire  `\"La longueur de la s√©quence des indices de jetons est sup√©rieure √† la\n",
        "longueur de s√©quence maximale sp√©cifi√©e‚Ä¶¬†\"`  peut √™tre ignor√© dans ce cas¬†‚Äî d√©tails\n",
        "\n",
        "[ici](https://github.com/docling-project/docling-core/issues/119#issuecomment-2577418826)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJcjaooVXWWc"
      },
      "source": [
        "D√©termination des splits (d√©coupage ou splitting) :\n",
        "\n",
        "Selon le mode choisi :\n",
        "- Mode DOC_CHUNKS : Les chunks sont les docs (d√©j√† pr√™ts)\n",
        "- Mode MARKDOWN : Utilise MarkdownHeaderTextSplitter pour d√©couper selon les titres (#, ##, ###)\n",
        "\n",
        "Cela cr√©e une liste de splits - des petits morceaux de texte qui seront index√©s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmHhCAy8XWWc"
      },
      "outputs": [],
      "source": [
        "if EXPORT_TYPE == ExportType.DOC_CHUNKS:\n",
        "    splits = docs\n",
        "elif EXPORT_TYPE == ExportType.MARKDOWN:\n",
        "    from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "\n",
        "    splitter = MarkdownHeaderTextSplitter(\n",
        "        headers_to_split_on=[\n",
        "            (\"#\", \"Header_1\"),\n",
        "            (\"##\", \"Header_2\"),\n",
        "            (\"###\", \"Header_3\"),\n",
        "        ],\n",
        "    )\n",
        "    splits = [split for doc in docs for split in splitter.split_text(doc.page_content)]\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected export type: {EXPORT_TYPE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCion89TXWWc"
      },
      "source": [
        "Regardons quelques splits exemple :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6Y3-NahXWWc"
      },
      "outputs": [],
      "source": [
        "for d in splits[:3]:\n",
        "    print(f\"- {d.page_content=}\")\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kz367ehXWWd"
      },
      "source": [
        "## Ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cette √©tape est cruciale :\n",
        "\n",
        "- Chaque split est transform√© en vecteur num√©rique (embedding) via le mod√®le sentence-transformers\n",
        "- Ces vecteurs sont stock√©s dans Milvus (base de donn√©es vectorielle locale)\n",
        "\n",
        "Cela permet de faire des recherches par similarit√© s√©mantique"
      ],
      "metadata": {
        "id": "vl0TjuyChi3v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd1XeFATXWWd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_milvus import Milvus\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "\n",
        "milvus_uri = str(Path(mkdtemp()) / \"docling.db\")  # or set as needed\n",
        "vectorstore = Milvus.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    collection_name=\"docling_demo\",\n",
        "    connection_args={\"uri\": milvus_uri},\n",
        "    index_params={\"index_type\": \"FLAT\"},\n",
        "    drop_old=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcc9yphlXWWd"
      },
      "source": [
        "## RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La cha√Æne RAG fonctionne ainsi :\n",
        "\n",
        "- Retrieval (R√©cup√©ration) :\n",
        "  - La question est convertie en vecteur\n",
        "  - Les TOP_K (3) chunks les plus similaires sont r√©cup√©r√©s de Milvus\n",
        "\n",
        "\n",
        "- Augmentation :\n",
        "  - Ces chunks sont inject√©s dans un prompt avec la question\n",
        "  - Le prompt structure : \"Voici le contexte... R√©ponds √† la question...\"\n",
        "\n",
        "\n",
        "- Generation :\n",
        "  - Le LLM (Mistral-7B via l'API Hugging Face) g√©n√®re une r√©ponse bas√©e sur le contexte fourni"
      ],
      "metadata": {
        "id": "p-f4f6-DhsNm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFn3Z9eZXWWd"
      },
      "outputs": [],
      "source": [
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
        "\n",
        "# Cr√©er l'endpoint avec task conversational\n",
        "endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=GEN_MODEL_ID,\n",
        "    huggingfacehub_api_token=HF_TOKEN,\n",
        "    task=\"conversational\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "# Wrapper pour compatibilit√© avec les cha√Ænes LangChain\n",
        "llm = ChatHuggingFace(llm=endpoint)\n",
        "\n",
        "\n",
        "def clip_text(text, threshold=100):\n",
        "    return f\"{text[:threshold]}...\" if len(text) > threshold else text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## En retour, le syst√®me :\n",
        "- Pose la question \"Which are the main AI models in Docling?\"\n",
        "- R√©cup√®re les passages pertinents du PDF\n",
        "- G√©n√®re une r√©ponse bas√©e sur ces passages\n",
        "- Affiche la r√©ponse + les sources utilis√©es avec leurs m√©tadonn√©es"
      ],
      "metadata": {
        "id": "K_82LsP6iJ8D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7W3EUWzXWWd"
      },
      "outputs": [],
      "source": [
        "question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "resp_dict = rag_chain.invoke({\"input\": QUESTION})\n",
        "\n",
        "clipped_answer = clip_text(resp_dict[\"answer\"], threshold=200)\n",
        "print(f\"Question:\\n{resp_dict['input']}\\n\\nAnswer:\\n{clipped_answer}\")\n",
        "for i, doc in enumerate(resp_dict[\"context\"]):\n",
        "    print()\n",
        "    print(f\"Source {i + 1}:\")\n",
        "    print(f\"  text: {json.dumps(clip_text(doc.page_content, threshold=350))}\")\n",
        "    for key in doc.metadata:\n",
        "        if key != \"pk\":\n",
        "            val = doc.metadata.get(key)\n",
        "            clipped_val = clip_text(val) if isinstance(val, str) else val\n",
        "            print(f\"  {key}: {clipped_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avantages de cette approche :\n",
        "\n",
        "‚úÖ R√©pond avec des informations sp√©cifiques au document\n",
        "\n",
        "‚úÖ Cite les sources utilis√©es\n",
        "\n",
        "‚úÖ √âvite les hallucinations en se basant sur du contenu r√©el\n",
        "\n",
        "‚úÖ Peut traiter des documents longs sans d√©passer les limites de contexte du LLM\n"
      ],
      "metadata": {
        "id": "AWDpAFfSiQjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-YrkCtiXWWd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}