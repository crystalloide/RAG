{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/RAG/blob/main/LAB01_Visualisation_tokenisation_embeddings_open_source2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztcfeu9czNOd"
      },
      "source": [
        "# Lab 1: Visualisation de la tokenisation et des embeddings (Open Source)\n",
        "\n",
        "---\n",
        "\n",
        "**Objectif:**\n",
        "- Comprendre comment un texte est segment√© en tokens et converti en vecteurs num√©riques (embeddings).\n",
        "- Apprendre √† visualiser et interpr√©ter ces structures.\n",
        "- **Utiliser uniquement des mod√®les open source gratuits !**\n",
        "\n",
        "**Dur√©e estim√©e:**\n",
        "- 90‚Äì120 minutes\n",
        "\n",
        "**Livrable :**\n",
        "- Notebook avec graphiques pr√©sentant la segmentation en tokens et une visualisation 2D de vecteurs num√©riques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbi2llxSzNOe"
      },
      "source": [
        "## Step 1: Pr√©-requis (5 min)\n",
        "\n",
        "Installation des librairies n√©cessaires :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVI0XFCJzNOe"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentence-transformers matplotlib scikit-learn pandas numpy networkx scipy -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L_6MD3VzNOe"
      },
      "source": [
        "## Step 2: Inspection de la tokenisation (15 min)\n",
        "\n",
        "Utilisation de `transformers` et `AutoTokenizer` pour d√©couper le texte en tokens :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Ejn395zNOe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize tokenizer with a popular open-source model (DistilBERT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"Agentic AI agents can plan, reason, and use tools.\"\n",
        "\n",
        "# Tokenize\n",
        "tokens = tokenizer.encode(sample_text)\n",
        "tokens_decoded = tokenizer.tokenize(sample_text)\n",
        "\n",
        "# Display results\n",
        "print(\"=\"*70)\n",
        "print(f\"Texte originel: {sample_text}\")\n",
        "print(f\"Nombre de tokens: {len(tokens)}\")\n",
        "print(f\"Token IDs: {tokens}\")\n",
        "print(\"\\nTokens d√©cod√©s (individuellement):\")\n",
        "for i, token in enumerate(tokens_decoded, 1):\n",
        "    print(f\" {i}. '{token}'\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwxLNHipzNOf"
      },
      "source": [
        "### Exp√©rimentez avec diff√©rents types de texte\n",
        "\n",
        "Regardons comment la ponctuation, les espaces, et les caract√®res sp√©ciaux affectent la tokenisation :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppzOr5U3zNOf"
      },
      "outputs": [],
      "source": [
        "test_texts = [\n",
        "    \"Hello, World!\",\n",
        "    \"HelloWorld\",\n",
        "    \"hello world\",\n",
        "    \"AI is üî•.\",\n",
        "    \"def my_function():\\n return True\",\n",
        "    \"AGENTIC\",\n",
        "    \"agentic\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Analyse de la Tokenisation sur diff√©rents exemples de texte\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for text in test_texts:\n",
        "    tokens = tokenizer.encode(text)\n",
        "    decoded = tokenizer.tokenize(text)\n",
        "    print(f\"\\nText: {repr(text)}\")\n",
        "    print(f\"Tokens: {len(tokens)} ‚Üí {tokens}\")\n",
        "    print(f\"Decoded: {decoded}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucx-k72QzNOf"
      },
      "source": [
        "## Step 3 : Comparaison de la longueur des jetons dans les textes (10 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snMzaTF7zNOf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentences = [\n",
        "    \"AI is amazing.\",\n",
        "    \"Artificial Intelligence is amazing.\",\n",
        "    \"AI is üî•.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\",\n",
        "]\n",
        "\n",
        "# Collect data\n",
        "data = []\n",
        "for sentence in sentences:\n",
        "    token_list = tokenizer.encode(sentence)\n",
        "    data.append({\n",
        "        \"Text\": sentence,\n",
        "        \"Character Count\": len(sentence),\n",
        "        \"Token Count\": len(token_list),\n",
        "        \"Ratio (Chars/Token)\": len(sentence) / len(token_list)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"Comparaison de la longueur des Tokens\")\n",
        "print(\"=\"*100)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\"*100)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart: Character vs Token count\n",
        "x = range(len(df))\n",
        "axes[0].bar([i - 0.2 for i in x], df['Character Count'], width=0.4, label='Characters', color='steelblue')\n",
        "axes[0].bar([i + 0.2 for i in x], df['Token Count'], width=0.4, label='Tokens', color='coral')\n",
        "axes[0].set_xlabel('Sentence Index', fontsize=11, fontweight='bold')\n",
        "axes[0].set_ylabel('Count', fontsize=11, fontweight='bold')\n",
        "axes[0].set_title('Character Count vs Token Count', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Line chart: Char/Token ratio\n",
        "axes[1].plot(x, df['Ratio (Chars/Token)'], marker='o', linewidth=2, markersize=8, color='green')\n",
        "axes[1].set_xlabel('Sentence Index', fontsize=11, fontweight='bold')\n",
        "axes[1].set_ylabel('Ratio (Chars/Token)', fontsize=11, fontweight='bold')\n",
        "axes[1].set_title('Character-to-Token Ratio', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Remarque : Les caract√®res sp√©ciaux ont tendance √† utiliser plus de jetons par caract√®re !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR9ITGcuzNOf"
      },
      "source": [
        "## Step 4: G√©n√©rer des embeddings (20 min)\n",
        "\n",
        "Utilisons le mod√®le open source `sentence-transformers` pour cr√©er des vecteurs num√©riques :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttJr-RoQzNOf"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Load a pre-trained model (free and open-source)\n",
        "# Options: 'all-MiniLM-L6-v2', 'all-mpnet-base-v2', 'distiluse-base-multilingual-cased-v2'\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Texts to embed\n",
        "texts = [\n",
        "    \"Agentic AI\",\n",
        "    \"Autonomous agents\",\n",
        "    \"Bananas are yellow\",\n",
        "    \"Machine learning models\",\n",
        "    \"Fruit is delicious\",\n",
        "]\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING EMBEDDINGS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for text in texts:\n",
        "    vector = model.encode(text)\n",
        "    embeddings.append(vector)\n",
        "    print(f\"‚úì '{text}' ‚Üí Vector length: {len(vector)}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüìä Embedding Dimension: {len(embeddings[0])}\")\n",
        "print(f\"Number of texts embedded: {len(embeddings)}\")\n",
        "\n",
        "# Show a sample embedding (first 20 values)\n",
        "print(f\"\\nSample embedding (first 20 values) for '{texts[0]}':\")\n",
        "print(embeddings[0][:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBK90zy7zNOg"
      },
      "source": [
        "## Step 5: R√©duction de dimensionnalit√© et visualisation (25 min)\n",
        "\n",
        "R√©duire les Embeddings √† 2D pour permettre la visualisation :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6F6KyBEzNOg"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert to numpy array\n",
        "embeddings_array = np.array(embeddings)\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(embeddings_array)\n",
        "\n",
        "# Get explained variance\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "\n",
        "print(\"\\nüìà PCA Explained Variance:\")\n",
        "print(f\" PC1: {explained_var[0]:.4f} ({explained_var[0]*100:.2f}%)\")\n",
        "print(f\" PC2: {explained_var[1]:.4f} ({explained_var[1]*100:.2f}%)\")\n",
        "print(f\" Total: {sum(explained_var):.4f} ({sum(explained_var)*100:.2f}%)\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "colors = ['#FF6B6B', '#FF6B6B', '#4ECDC4', '#4ECDC4', '#4ECDC4']\n",
        "\n",
        "for i, (txt, color) in enumerate(zip(texts, colors)):\n",
        "    x, y = embeddings_2d[i]\n",
        "    plt.scatter(x, y, s=300, alpha=0.7, color=color, edgecolors='black', linewidth=2)\n",
        "    plt.text(x + 0.05, y + 0.05, txt, fontsize=10, fontweight='bold',\n",
        "            bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.3))\n",
        "\n",
        "plt.xlabel(f'PC1 ({explained_var[0]*100:.1f}%)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel(f'PC2 ({explained_var[1]*100:.1f}%)', fontsize=12, fontweight='bold')\n",
        "plt.title('Embedding Visualization (PCA 2D)', fontsize=14, fontweight='bold')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.axhline(y=0, color='k', linewidth=0.5)\n",
        "plt.axvline(x=0, color='k', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Remarque : Les textes s√©mantiquement similaires se retrouvent proches les uns des autres !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSNtmX7YzNOg"
      },
      "source": [
        "## Step 6: Classe utilitaire pour analyse compl√®te\n",
        "\n",
        "Cr√©ons une classe r√©utilisable pour analyser n'importe quel texte :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjAbn3NhzNOg"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "class EmbeddingAnalyzer:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.reference_embedding = None\n",
        "\n",
        "    def set_reference(self, text):\n",
        "        \"\"\"Set reference text for similarity comparison\"\"\"\n",
        "        self.reference_embedding = self.model.encode(text)\n",
        "\n",
        "    def compute_similarity(self, embedding):\n",
        "        \"\"\"Compute cosine similarity with reference embedding\"\"\"\n",
        "        if self.reference_embedding is None:\n",
        "            return None\n",
        "        return 1 - cosine(self.reference_embedding, embedding)\n",
        "\n",
        "    def display_analysis(self, text):\n",
        "        \"\"\"Display complete analysis for a text\"\"\"\n",
        "        # Tokenization\n",
        "        tokens = self.tokenizer.encode(text)\n",
        "        tokens_decoded = self.tokenizer.tokenize(text)\n",
        "\n",
        "        # Embedding\n",
        "        embedding = self.model.encode(text)\n",
        "\n",
        "        # Similarity\n",
        "        similarity = self.compute_similarity(embedding)\n",
        "\n",
        "        # Display\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"TEXTE: '{text}'\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Nombre de caract√®res : {len(text)}\")\n",
        "        print(f\"Nombre de tokens : {len(tokens)}\")\n",
        "        print(f\"Dimension Embedding : {len(embedding)}\")\n",
        "\n",
        "        if similarity is not None:\n",
        "            print(f\"\\nSimilarit√© avec la Reference : {similarity:.4f}\")\n",
        "\n",
        "        print(f\"\\nTokens:\")\n",
        "        for i, token in enumerate(tokens_decoded, 1):\n",
        "            print(f\" {i}. {token}\")\n",
        "\n",
        "        print(f\"\\nPr√©visualisation des Embeddings (10 premi√®res valeurs): {embedding[:10].tolist()}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "# Initialize analyzer\n",
        "analyzer = EmbeddingAnalyzer(model, tokenizer)\n",
        "analyzer.set_reference(\"AI and agents\")\n",
        "\n",
        "# Test with custom texts\n",
        "custom_texts = [\n",
        "    \"Your sentence here\",\n",
        "    \"Another sentence\",\n",
        "    \"Add as many as you want\",\n",
        "]\n",
        "\n",
        "for text in custom_texts:\n",
        "    analyzer.display_analysis(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ruXVWsVzNOg"
      },
      "source": [
        "## Step 7: Visualisation avanc√©e - R√©seau de similarit√©\n",
        "\n",
        "Cr√©ons un graphique de r√©seau illustrant les relations de similarit√© :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AadcqSvvzNOg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.patches as mpatches\n",
        "import networkx as nx\n",
        "\n",
        "# Prepare all texts for analysis\n",
        "all_texts = texts + custom_texts[:2]  # Use first 2 custom texts\n",
        "\n",
        "# Generate embeddings for all texts\n",
        "all_embeddings = [model.encode(text) for text in all_texts]\n",
        "\n",
        "# Compute similarity matrix\n",
        "similarity_matrix = []\n",
        "for emb1 in all_embeddings:\n",
        "    row = []\n",
        "    for emb2 in all_embeddings:\n",
        "        sim = 1 - cosine(emb1, emb2)\n",
        "        row.append(sim)\n",
        "    similarity_matrix.append(row)\n",
        "\n",
        "similarity_matrix = np.array(similarity_matrix)\n",
        "\n",
        "# Create network graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes\n",
        "for i, text in enumerate(all_texts):\n",
        "    G.add_node(i, label=text)\n",
        "\n",
        "# Add edges for high similarity (>0.5)\n",
        "for i in range(len(all_texts)):\n",
        "    for j in range(i+1, len(all_texts)):\n",
        "        if similarity_matrix[i][j] > 0.5:\n",
        "            G.add_edge(i, j, weight=similarity_matrix[i][j])\n",
        "\n",
        "# Draw the network\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "# Layout\n",
        "pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
        "\n",
        "# Node colors (categories)\n",
        "node_colors = []\n",
        "for i, text in enumerate(all_texts):\n",
        "    if 'AI' in text or 'agent' in text.lower():\n",
        "        node_colors.append('#FF6B6B')  # Red for AI/Agents\n",
        "    else:\n",
        "        node_colors.append('#4ECDC4')  # Teal for other\n",
        "\n",
        "# Draw nodes\n",
        "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=3000, alpha=0.8, ax=ax)\n",
        "\n",
        "# Draw edges\n",
        "edges = G.edges()\n",
        "weights = [G[u][v]['weight'] for u, v in edges]\n",
        "nx.draw_networkx_edges(G, pos, width=[w*3 for w in weights], alpha=0.6, ax=ax)\n",
        "\n",
        "# Draw labels\n",
        "labels = {i: all_texts[i][:20] + '...' if len(all_texts[i]) > 20 else all_texts[i]\n",
        "          for i in range(len(all_texts))}\n",
        "nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold', ax=ax)\n",
        "\n",
        "# Legend\n",
        "ai_patch = mpatches.Patch(color='#FF6B6B', label='AI/Agents', edgecolor='black')\n",
        "food_patch = mpatches.Patch(color='#4ECDC4', label='Other', edgecolor='black')\n",
        "ax.legend(handles=[ai_patch, food_patch], loc='upper left', fontsize=12)\n",
        "\n",
        "plt.title('Embedding Similarity Network (Threshold > 0.5)', fontsize=14, fontweight='bold')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüîó Graphique de r√©seau : Les lignes connectent les textes similaires !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BJAvEfgzNOg"
      },
      "source": [
        "## Step 8: R√©sum√© et conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHqGxwIfzNOg"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"R√âSUM√â DE L'APPRENTISSAGE\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "‚úì Tokenisation : Comment les textes sont divis√©s en unit√©s plus petites\n",
        "‚úì Embeddings : Conversion de texte en vecteurs num√©riques haute dimension\n",
        "‚úì Similarit√© s√©mantique : Les textes similaires ont des embeddings proches\n",
        "‚úì Visualisation : R√©duction 2D pour explorer l'espace s√©mantique\n",
        "\n",
        "Mod√®les utilis√©s (100% open source et gratuit):\n",
        "- Tokenizer: DistilBERT-base-uncased\n",
        "- Embeddings: Sentence-Transformers (all-MiniLM-L6-v2)\n",
        "- R√©duction: PCA (scikit-learn)\n",
        "- Graphique: NetworkX + Matplotlib\n",
        "\"\"\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARFWxyvBzNOh"
      },
      "source": [
        "## Points cl√©s\n",
        "\n",
        "| Concept | Explication |\n",
        "|---------|-------------|\n",
        "| **Tokenization** | Processus de division du texte en unit√©s discr√®tes (tokens) |\n",
        "| **Token** | Unit√© individuelle (mot, sous-mot, caract√®re sp√©cial) |\n",
        "| **Embedding** | Vecteur num√©rique repr√©sentant le sens du texte |\n",
        "| **Dimension** | Nombre de valeurs dans le vecteur (ex: 384 pour all-MiniLM-L6-v2) |\n",
        "| **Similarit√©** | Distance entre deux embeddings (cosine similarity) |\n",
        "| **PCA** | Technique pour r√©duire les dimensions en pr√©servant la variance |\n",
        "\n",
        "---\n",
        "\n",
        "## Mod√®les open source recommand√©s\n",
        "\n",
        "**Tokenizers :**\n",
        "- `distilbert-base-uncased` (tr√®s l√©ger, 66M param√®tres)\n",
        "- `roberta-base` (plus puissant, 125M param√®tres)\n",
        "- `bert-base-multilingual-cased` (multilingual)\n",
        "\n",
        "**Embeddings (Sentence-Transformers) :**\n",
        "- `all-MiniLM-L6-v2` : **Recommand√©** (22M params, ultra-rapide)\n",
        "- `all-mpnet-base-v2` : Plus puissant (109M params)\n",
        "- `distiluse-base-multilingual-cased-v2` : Multilingual\n",
        "\n",
        "**Avantages :**\n",
        "- ‚úÖ Gratuit et sans limites\n",
        "- ‚úÖ Fonctionnent hors ligne\n",
        "- ‚úÖ Pas de cl√©s API requises\n",
        "- ‚úÖ Ex√©cution locale rapide sur GPU/CPU"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}