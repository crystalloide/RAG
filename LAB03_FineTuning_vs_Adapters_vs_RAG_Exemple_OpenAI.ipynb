{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/RAG/blob/main/LAB03_FineTuning_vs_Adapters_vs_RAG_Exemple_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWuAGMaEnlxO"
      },
      "source": [
        "# LAB03 : Fine-Tuning vs Adapters vs RAG â€“ Comparaison des 3 approches - OpenAI\n",
        "\n",
        "## Objectif\n",
        "- Comparer comment le fine-tuning, les adapters et RAG rÃ©solvent le mÃªme problÃ¨me : faire rÃ©pondre un LLM Ã  des questions spÃ©cifiques Ã  un domaine.\n",
        "\n",
        "## DurÃ©e estimÃ©e\n",
        "- ~15 minutes\n",
        "\n",
        "## Livrables\n",
        "- Notebook montrant les rÃ©sultats Q&A des trois stratÃ©gies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzknAvmgnlxP"
      },
      "source": [
        "## Ã‰tape 1 : Setup (5 min)\n",
        "\n",
        "Installation des bibliothÃ¨ques nÃ©cessaires :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZSnV-GlnlxQ"
      },
      "outputs": [],
      "source": [
        "# !pip install -q --upgrade pip\n",
        "\n",
        "# Installer les packages principaux\n",
        "!pip install -q openai\n",
        "!pip install -q datasets\n",
        "!pip install -q transformers\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain langchain-community langchain-openai\n",
        "!pip install -q langchain-huggingface\n",
        "\n",
        "# Ignorer les erreurs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-chroma\n",
        "\n",
        "# Ignorer les erreurs"
      ],
      "metadata": {
        "id": "JuJZ87Az18t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2-eLsHanlxQ"
      },
      "source": [
        "## Ã‰tape 2 : crÃ©ation d'une Base de connaissances basique (5 min)\n",
        "\n",
        "Nous crÃ©ons une base de connaissances fictive sur l'IA Agentique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llazsGWFnlxQ"
      },
      "outputs": [],
      "source": [
        "# Base de connaissances sur l'IA Agentique\n",
        "kb = [\n",
        "    \"Agentic AI agents use memory, tools, and goals to act.\",\n",
        "    \"LangChain and CrewAI are popular frameworks for building AI agents.\",\n",
        "    \"Retrieval-Augmented Generation (RAG) improves accuracy by fetching external knowledge.\",\n",
        "    \"Agents can iterate and refine their actions based on feedback.\",\n",
        "    \"Tool calling allows agents to interact with APIs and external systems.\",\n",
        "    \"Les agents d'IA agentiques utilisent la mÃ©moire, des outils et des objectifs pour agir.\",\n",
        "    \"LangChain et CrewAI sont des frameworks populaires pour la crÃ©ation d'agents d'IA.\",\n",
        "    \"La gÃ©nÃ©ration augmentÃ©e par rÃ©cupÃ©ration (RAG) amÃ©liore la prÃ©cision en rÃ©cupÃ©rant des connaissances externes.\",\n",
        "    \"Les agents peuvent itÃ©rer et affiner leurs actions en fonction des retours d'information.\",\n",
        "    \"L'appel d'outils permet aux agents d'interagir avec des API et des systÃ¨mes externes.\"\n",
        "]\n",
        "\n",
        "# Questions de test\n",
        "questions = [\n",
        "    \"What are the key components of Agentic AI?\",\n",
        "    \"Name one framework for AI agents.\",\n",
        "    \"How does RAG improve answers?\",\n",
        "    \"Quels sont les composants clÃ©s de l'IA agentique ?\",\n",
        "    \"Citez un framework pour agents d'IA\",\n",
        "    \"Comment la RAG amÃ©liore-t-elle les rÃ©ponses ?\",\n",
        "]\n",
        "\n",
        "print(\"Knowledge Base (KB):\")\n",
        "for i, doc in enumerate(kb, 1):\n",
        "    print(f\"  {i}. {doc}\")\n",
        "\n",
        "print(\"\\nTest Questions:\")\n",
        "for i, q in enumerate(questions, 1):\n",
        "    print(f\"  {i}. {q}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNR7cYlknlxQ"
      },
      "source": [
        "## Ã‰tape 3 : Fine-Tuning (dÃ©mo conceptuelle : 5 min)\n",
        "\n",
        "**Fine-tuning** = consiste Ã  mettre Ã  jour les poids du modÃ¨le avec de nouveaux exemples Ã©tiquetÃ©s.\n",
        "\n",
        "Voici Ã  quoi ressemblerait un dataset de fine-tuning :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH49Anb1nlxQ"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# DonnÃ©es d'entraÃ®nement pour fine-tuning\n",
        "train_data = Dataset.from_dict({\n",
        "    \"prompt\": [\n",
        "        \"Q: What are the key components of Agentic AI?\\nA:\",\n",
        "        \"Q: Name one framework for AI agents.\\nA:\",\n",
        "        \"Q: How does RAG improve answers?\\nA:\"\n",
        "    ],\n",
        "    \"completion\": [\n",
        "        \" Agentic AI agents use memory, tools, and goals to act.\",\n",
        "        \" LangChain is a framework for building AI agents.\",\n",
        "        \" RAG improves accuracy by fetching external knowledge before answering.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Fine-Tuning Training Dataset:\")\n",
        "print(train_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINE-TUNING OBSERVATIONS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "âœ“ Avantages:\n",
        "  - ModÃ¨le hautement adaptÃ© au domaine\n",
        "  - Connaissances \"baked-in\" et rapides\n",
        "\n",
        "âœ— InconvÃ©nients:\n",
        "  - CoÃ»teux (calcul + infrastructure)\n",
        "  - Rigide (difficile de mettre Ã  jour les connaissances)\n",
        "  - NÃ©cessite un rÃ©entraÃ®nement pour chaque nouveau domaine\n",
        "  - DÃ©pend de la qualitÃ© des donnÃ©es d'entraÃ®nement\n",
        "\n",
        "ğŸ“Œ Avec OpenAI ou Hugging Face, vous uploaderiez ce dataset pour fine-tuning.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeIXdYg2nlxR"
      },
      "source": [
        "## Ã‰tape 4 : Adapters / LoRA (dÃ©mo conceptuelle : 5 min)\n",
        "\n",
        "**Adapters** = petites couches parameter-efficient que l'on entraÃ®ne au lieu de rÃ©-entraÃ®ner tout le modÃ¨le entier.\n",
        "\n",
        "C'est une alternative au fine-tuning complet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdqvT1v6nlxR"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Charger un petit modÃ¨le pour la dÃ©mo\n",
        "model_name = \"distilgpt2\"\n",
        "try:\n",
        "    tok = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"âœ“ ModÃ¨le chargÃ©: {model_name}\")\n",
        "    print(f\"  Nombre total de paramÃ¨tres: {total_params:,}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Note: TÃ©lÃ©chargement du modÃ¨le Ã©chouÃ© (attendu en mode hors-ligne)\")\n",
        "    print(f\"Erreur: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ADAPTERS / LoRA OBSERVATIONS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "âœ“ Avantages:\n",
        "  - TRÃˆS peu de paramÃ¨tres Ã  entraÃ®ner (~0.1-1% du modÃ¨le original)\n",
        "  - Ã‰conomique (moins de calcul, moins de mÃ©moire)\n",
        "  - Modulaire (plusieurs adapters pour diffÃ©rents domaines)\n",
        "  - Rapide Ã  mettre Ã  jour\n",
        "\n",
        "âœ— InconvÃ©nients:\n",
        "  - NÃ©cessite toujours une infrastructure d'entraÃ®nement\n",
        "  - Performance lÃ©gÃ¨rement infÃ©rieure au fine-tuning complet\n",
        "  - NÃ©cessite un framework compatible (PEFT, AdapterHub)\n",
        "\n",
        "ğŸ“Œ Avec LoRA, vous ne mettriez Ã  jour que quelques millions de paramÃ¨tres\n",
        "    au lieu de milliards, rendant l'entraÃ®nement beaucoup plus efficace.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ByhjDujnlxR"
      },
      "source": [
        "## Ã‰tape 5 : RAG â€“ dÃ©mo intÃ©ractive (5 min)\n",
        "\n",
        "Contrairement Ã  l'entraÃ®nement, **RAG** rÃ©cupÃ¨re les connaissances externes au moment de l'exÃ©cution.\n",
        "\n",
        "Construisons un systÃ¨me RAG complet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV1rjy-UnlxR"
      },
      "source": [
        "### Configuration de RAG avec LangChain et ChromaDB\n",
        "\n",
        "Nous utilisons ChromaDB (une alternative lÃ©gÃ¨re Ã  FAISS) pour cette dÃ©mo :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "381WLHkTnlxR"
      },
      "outputs": [],
      "source": [
        "# from langchain_community.vectorstores import Chroma\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os\n",
        "\n",
        "print(\"LangChain et ChromaDB importÃ©s avec succÃ¨s.\")\n",
        "print(\"\\nğŸ“Œ Configuration du systÃ¨me RAG...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctpcbaodnlxR"
      },
      "source": [
        "### Construction d'un RAG avec OpenAI (nÃ©cessite une clÃ© API valide OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get API key from Colab Secrets (add it in the Secrets manager: ğŸ”‘ icon on left panel)\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Or if running locally with .env file:\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "\n",
        "print(\"âœ“ API Key configured successfully\")\n"
      ],
      "metadata": {
        "id": "HA1oQjDhxXw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DÃ©monstration du fonctionnement du RAG : Retrieval (RÃ©cupÃ©ration de documents)"
      ],
      "metadata": {
        "id": "H_KkEJX9JExS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_rag_with_openai():\n",
        "    \"\"\"\n",
        "    Configure RAG avec OpenAI embeddings et ChatOpenAI - VERSION MODERNE.\n",
        "    NÃ©cessite une clÃ© API OpenAI.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    from getpass import getpass\n",
        "\n",
        "    # VÃ©rifier ou demander la clÃ© API\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        print(\"\\nğŸ”‘ ClÃ© API OpenAI requise pour cette dÃ©mo.\")\n",
        "        print(\"   CrÃ©ez une clÃ© sur https://platform.openai.com/api-keys\")\n",
        "        api_key = getpass(\"Entrez votre OPENAI_API_KEY: \")\n",
        "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "    from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "    from langchain_core.documents import Document\n",
        "    from langchain_core.prompts import ChatPromptTemplate\n",
        "    from langchain_core.runnables import RunnablePassthrough\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "    try:\n",
        "    # ğŸ—‘ï¸ Supprimer la base Chroma existante\n",
        "    #    chroma_dir = \"/content/chroma_db\"\n",
        "    #    if os.path.exists(chroma_dir):\n",
        "    #        print(f\"ğŸ—‘ï¸  Suppression de la base existante: {chroma_dir}\")\n",
        "    #        shutil.rmtree(chroma_dir)\n",
        "    #        print(\"âœ“ Base supprimÃ©e\")\n",
        "        print(\"ğŸ“š CrÃ©ation des documents...\")\n",
        "        docs = [Document(page_content=x) for x in kb]\n",
        "\n",
        "        print(\"ğŸ”¨ Initialisation des embeddings OpenAI...\")\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "\n",
        "        print(\"ğŸ’¾ Construction de la base vectorielle...\")\n",
        "        db = Chroma.from_documents(docs, embeddings)\n",
        "        #db = Chroma.from_documents(docs, embeddings, persist_directory=chroma_dir)\n",
        "        retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "        print(\"ğŸ¤– Configuration de la chaÃ®ne RAG...\")\n",
        "\n",
        "        # Template de prompt\n",
        "        template = \"\"\"RÃ©ponds Ã  la question en te basant sur le contexte suivant :\n",
        "\n",
        "Contexte: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "RÃ©ponse:\"\"\"\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_template(template)\n",
        "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "        # Fonction pour formater les documents\n",
        "        def format_docs(docs):\n",
        "            return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # ChaÃ®ne RAG moderne avec LCEL\n",
        "        qa_chain = (\n",
        "            {\n",
        "                \"context\": retriever | format_docs,\n",
        "                \"question\": RunnablePassthrough()\n",
        "            }\n",
        "            | prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        print(\"âœ… SystÃ¨me RAG configurÃ© avec succÃ¨s!\\n\")\n",
        "        return qa_chain, db\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Erreur lors de la configuration RAG: {e}\")\n",
        "        return None, None\n",
        "\n",
        "print(\"âœ“ Fonction setup_rag_with_openai() dÃ©finie (version moderne LCEL).\")"
      ],
      "metadata": {
        "id": "D8TOTk0G_2FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QyHefdnlxS"
      },
      "source": [
        "### Full RAG Pipeline avec Q&A ComplÃ¨te\n",
        "\n",
        "Maintenant, montrons une pipeline Q&A RAG complÃ¨te :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M5INxxjnlxS"
      },
      "outputs": [],
      "source": [
        "# Utilisation d'OpenAI\n",
        "import os\n",
        "if os.getenv(\"OPENAI_API_KEY\"):\n",
        "    qa, db = setup_rag_with_openai()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FULL RAG Q&A avec ChatOpenAI (gpt-4o-mini)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for question in questions:\n",
        "        print(f\"\\nâ“ Q: {question}\")\n",
        "        answer = qa.invoke(question)\n",
        "        print(f\"âœ… A: {answer}\")\n",
        "\n",
        "    # Question bonus\n",
        "    bonus_q = \"Explain Agentic AI like I'm 10 years old.\"\n",
        "    print(f\"\\nâ“ BONUS Q: {bonus_q}\")\n",
        "    bonus_answer = qa.invoke(bonus_q)\n",
        "    print(f\"âœ… A: {bonus_answer}\")\n",
        "else:\n",
        "    print(\"ğŸ’¡ Pour la dÃ©mo :\")\n",
        "    print(\"   1. CrÃ©ez une clÃ© sur https://platform.openai.com/api-keys\")\n",
        "    print(\"   2. Entrez votre clÃ© valide API\")\n",
        "##\"\"\"\n",
        "\n",
        "print(\"\\nâœ“ Code RAG avec OpenAI .\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ6tuzS_nlxS"
      },
      "source": [
        "## Ã‰tape 6 : Comparaison des 3 mÃ©thodes (5 min)\n",
        "\n",
        "CrÃ©ons un tableau comparatif des trois approches :"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# CrÃ©er le tableau comparatif\n",
        "comparison_data = {\n",
        "    \"Aspect\": [\n",
        "        \"Accuracy\",\n",
        "        \"Cost\",\n",
        "        \"Training Time\",\n",
        "        \"Update Frequency\",\n",
        "        \"Knowledge Freshness\",\n",
        "        \"Scalability\",\n",
        "        \"Complexity\",\n",
        "        \"Infrastructure\"\n",
        "    ],\n",
        "    \"Fine-Tuning\": [\n",
        "        \"â­â­â­â­â­\",\n",
        "        \"ğŸ’°ğŸ’°ğŸ’°ğŸ’°\",\n",
        "        \"ğŸ• Hours/Days\",\n",
        "        \"Monthly/Quarterly\",\n",
        "        \"Stale (until retrained)\",\n",
        "        \"Fair (one model per domain)\",\n",
        "        \"ğŸ”´ High\",\n",
        "        \"GPUs required\"\n",
        "    ],\n",
        "    \"Adapters/LoRA\": [\n",
        "        \"â­â­â­â­\",\n",
        "        \"ğŸ’°ğŸ’°\",\n",
        "        \"ğŸ• Minutes/Hours\",\n",
        "        \"Weekly\",\n",
        "        \"Semi-fresh\",\n",
        "        \"Good (modular)\",\n",
        "        \"ğŸŸ¡ Medium\",\n",
        "        \"GPUs (lighter)\"\n",
        "    ],\n",
        "    \"RAG\": [\n",
        "        \"â­â­â­\",\n",
        "        \"ğŸ’°\",\n",
        "        \"âš¡ Real-time\",\n",
        "        \"Real-time\",\n",
        "        \"âœ¨ Always Fresh\",\n",
        "        \"Excellent\",\n",
        "        \"ğŸŸ¢ Low\",\n",
        "        \"Vector DB + API\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# CrÃ©ation du HTML stylisÃ©\n",
        "html = \"\"\"\n",
        "<style>\n",
        "    .comparison-table {\n",
        "        width: 100%;\n",
        "        max-width: 1200px;\n",
        "        margin: 20px auto;\n",
        "        background: linear-gradient(to bottom right, #f8fafc, #eff6ff);\n",
        "        border-radius: 8px;\n",
        "        overflow: hidden;\n",
        "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    .table-header {\n",
        "        background: linear-gradient(to right, #2563eb, #4f46e5);\n",
        "        padding: 20px;\n",
        "        text-align: center;\n",
        "    }\n",
        "    .table-header h1 {\n",
        "        color: white;\n",
        "        font-size: 24px;\n",
        "        font-weight: bold;\n",
        "        margin: 0;\n",
        "    }\n",
        "    .styled-table {\n",
        "        width: 100%;\n",
        "        border-collapse: collapse;\n",
        "        background: white;\n",
        "    }\n",
        "    .styled-table thead tr {\n",
        "        background-color: #f1f5f9;\n",
        "        border-bottom: 2px solid #cbd5e1;\n",
        "    }\n",
        "    .styled-table th {\n",
        "        padding: 16px;\n",
        "        text-align: left;\n",
        "        font-weight: 600;\n",
        "        color: #334155;\n",
        "        width: 25%;\n",
        "    }\n",
        "    .styled-table td {\n",
        "        padding: 16px;\n",
        "        color: #475569;\n",
        "        border-bottom: 1px solid #e2e8f0;\n",
        "    }\n",
        "    .styled-table tbody tr:nth-child(even) {\n",
        "        background-color: #f8fafc;\n",
        "    }\n",
        "    .styled-table tbody tr:hover {\n",
        "        background-color: #dbeafe;\n",
        "        transition: background-color 0.2s;\n",
        "    }\n",
        "    .styled-table tbody tr td:first-child {\n",
        "        font-weight: 500;\n",
        "        color: #1e293b;\n",
        "    }\n",
        "    .table-footer {\n",
        "        background-color: #f8fafc;\n",
        "        padding: 20px;\n",
        "        border-top: 1px solid #e2e8f0;\n",
        "    }\n",
        "    .footer-grid {\n",
        "        display: grid;\n",
        "        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n",
        "        gap: 16px;\n",
        "        font-size: 14px;\n",
        "    }\n",
        "    .footer-item {\n",
        "        display: flex;\n",
        "        gap: 8px;\n",
        "        color: #1e40af;  /* Bleu foncÃ© pour tout le texte */\n",
        "    }\n",
        "    .footer-item strong {\n",
        "        color: #1e3a8a;  /* Bleu foncÃ© */\n",
        "    }\n",
        "    .footer-item span:first-child {\n",
        "        font-size: 18px;\n",
        "    }\n",
        "</style>\n",
        "\n",
        "<div class=\"comparison-table\">\n",
        "    <div class=\"table-header\">\n",
        "        <h1>Tableau Comparatif: Fine-Tuning vs Adapters/LoRA vs RAG</h1>\n",
        "    </div>\n",
        "\n",
        "    <table class=\"styled-table\">\n",
        "        <thead>\n",
        "            <tr>\n",
        "                <th>Aspect</th>\n",
        "                <th>Fine-Tuning</th>\n",
        "                <th>Adapters/LoRA</th>\n",
        "                <th>RAG</th>\n",
        "            </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "\"\"\"\n",
        "\n",
        "# Ajouter les lignes du tableau\n",
        "for _, row in df.iterrows():\n",
        "    html += f\"\"\"\n",
        "            <tr>\n",
        "                <td>{row['Aspect']}</td>\n",
        "                <td>{row['Fine-Tuning']}</td>\n",
        "                <td>{row['Adapters/LoRA']}</td>\n",
        "                <td>{row['RAG']}</td>\n",
        "            </tr>\n",
        "\"\"\"\n",
        "\n",
        "html += \"\"\"\n",
        "        </tbody>\n",
        "    </table>\n",
        "\n",
        "    <div class=\"table-footer\">\n",
        "        <div class=\"footer-grid\">\n",
        "            <div class=\"footer-item\">\n",
        "                <span>ğŸ’¡</span>\n",
        "                <div><strong>Fine-Tuning:</strong> Meilleure prÃ©cision mais coÃ»teux</div>\n",
        "            </div>\n",
        "            <div class=\"footer-item\">\n",
        "                <span>ğŸ’¡</span>\n",
        "                <div><strong>Adapters/LoRA:</strong> Bon compromis coÃ»t/performance</div>\n",
        "            </div>\n",
        "            <div class=\"footer-item\">\n",
        "                <span>ğŸ’¡</span>\n",
        "                <div><strong>RAG:</strong> DonnÃ©es fraÃ®ches, faible complexitÃ©</div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "# Afficher le tableau stylisÃ©\n",
        "display(HTML(html))"
      ],
      "metadata": {
        "id": "PKmcHDrbA48S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diGmb8p2nlxS"
      },
      "source": [
        "### Tableau DÃ©taillÃ© des Cas d'Usage"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oscs-aztCvVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "# Create the DataFrame\n",
        "df_usecases = pd.DataFrame({\n",
        "    'Method': [\n",
        "        'Fine-Tuning',\n",
        "        'Adapters (LoRA)',\n",
        "        'RAG'\n",
        "    ],\n",
        "    'Pros': [\n",
        "        'âœ“ Highly accurate\\nâœ“ Baked-in knowledge\\nâœ“ Very fast inference',\n",
        "        'âœ“ Cheap fine-tuning\\nâœ“ Modular/multi-domain\\nâœ“ Fast training',\n",
        "        'âœ“ Flexible\\nâœ“ Real-time updates\\nâœ“ No retraining'\n",
        "    ],\n",
        "    'Cons': [\n",
        "        'âœ— Expensive\\nâœ— Rigid (hard to update)\\nâœ— Retrain per domain',\n",
        "        'âœ— Still needs training\\nâœ— Slightly lower accuracy\\nâœ— Framework dependent',\n",
        "        'âœ— Retriever quality matters\\nâœ— Latency (retrieval step)\\nâœ— Context window limits'\n",
        "    ],\n",
        "    'Best Use Case': [\n",
        "        'Applications Ã  domaine restreint (par exemple : chatbot mÃ©dical)',\n",
        "        'Adaptation au domaine (tÃ¢ches spÃ©cialisÃ©es multiples)',\n",
        "        'Connaissances dynamiques (actualitÃ©s, FAQ, donnÃ©es en temps rÃ©el)'\n",
        "    ]\n",
        "})\n",
        "\n",
        "\n",
        "html_table = \"\"\"\n",
        "<table style=\"width:100%; border-collapse: collapse; font-family: monospace;\">\n",
        "<tr style=\"background-color: #aaaaaa;\">\n",
        "    <th style=\"border: 1px solid #ddd; padding: 16px; color: Black; text-align: left;\">MÃ©thode</th>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 16px; color: black; text-align: left;\">Avantages</th>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 16px; color: black; text-align: left;\">InconvÃ©nients</th>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 16px; color: black; text-align: left;\">Cas d'usage</th>\n",
        "</tr>\n",
        "\"\"\"\n",
        "\n",
        "for idx, row in df_usecases.iterrows():\n",
        "    pros_html = \"<br>\".join(row['Pros'].split('\\n'))\n",
        "    cons_html = \"<br>\".join(row['Cons'].split('\\n'))\n",
        "    use_html = \"<br>\".join(row['Best Use Case'].split('\\n'))\n",
        "\n",
        "    html_table += f\"\"\"\n",
        "<tr>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; color: white; font-weight: bold;\">{row['Method']}</td>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; color: green;\">{pros_html}</td>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; color: red;\">{cons_html}</td>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px;\">{use_html}</td>\n",
        "</tr>\n",
        "\"\"\"\n",
        "\n",
        "html_table += \"</table>\"\n",
        "display(HTML(html_table))\n"
      ],
      "metadata": {
        "id": "fOeDLpPX46H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGiC4SfWnlxT"
      },
      "source": [
        "## RÃ©sumÃ© et Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWh7Oau8nlxT"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘              LAB03 - CONCLUSIONS & KEY TAKEAWAYS                              â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "ğŸ¯ TROIS STRATÃ‰GIES POUR ADAPTER LES LLMs Ã€ UN DOMAINE:\n",
        "\n",
        "1ï¸âƒ£  FINE-TUNING\n",
        "   â””â”€ EntraÃ®ner le modÃ¨le sur de nouvelles donnÃ©es\n",
        "   â””â”€ âœ… TrÃ¨s prÃ©cis | âŒ CoÃ»teux, rigide\n",
        "   â””â”€ ğŸ’¡ Meilleur pour: Applications de niche\n",
        "\n",
        "2ï¸âƒ£  ADAPTERS / LoRA\n",
        "   â””â”€ EntraÃ®ner seulement de petits modules paramÃ©triques\n",
        "   â””â”€ âœ… Ã‰conomique, modulaire | âŒ NÃ©cessite toujours du training\n",
        "   â””â”€ ğŸ’¡ Meilleur pour: Adaptation multi-domaine\n",
        "\n",
        "3ï¸âƒ£  RAG (Retrieval-Augmented Generation)\n",
        "   â””â”€ RÃ©cupÃ©rer des connaissances externes en temps rÃ©el\n",
        "   â””â”€ âœ… Flexible, mises Ã  jour faciles | âŒ DÃ©pend de la qualitÃ© du retriever\n",
        "   â””â”€ ğŸ’¡ Meilleur pour: Connaissances dynamiques, agents IA\n",
        "\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                    ğŸš€ RECOMMANDATION POUR LES AGENTS IA                       â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "Pour les Agentic AI systems (LangChain, CrewAI, etc.):\n",
        "\n",
        "   â¡ï¸  RAG est souvent le GO-TO car:\n",
        "      â€¢ Les agents ont besoin de connaissances Ã  jour\n",
        "      â€¢ Les tools et l'itÃ©ration demandent de la flexibilitÃ©\n",
        "      â€¢ Les coÃ»ts sont raisonnables vs fine-tuning\n",
        "\n",
        "   â¡ï¸  Combinaison optimale:\n",
        "      â€¢ RAG pour les connaissances mÃ©tier dynamiques\n",
        "      â€¢ Fine-tuning/Adapters pour le format/style du modÃ¨le\n",
        "      â€¢ Tool calling pour les actions externes\n",
        "\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                         âœ… NEXT STEPS                                         â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "1. Pratiquez RAG avec votre propre base de connaissances\n",
        "2. Explorez les frameworks (LangChain, Llama Index, etc.)\n",
        "3. Optimisez le retriever avec diffÃ©rentes embeddings\n",
        "4. Testez les agents IA intÃ©grant ces stratÃ©gies\n",
        "5. Mesurez la qualitÃ© des rÃ©ponses (eval)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ‰ LAB03 FinalisÃ© !\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogPnvje6nlxT"
      },
      "source": [
        "## Resources SupplÃ©mentaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thW41_X9nlxT"
      },
      "outputs": [],
      "source": [
        "resources = \"\"\"\n",
        "ğŸ“š RESSOURCES RECOMMANDÃ‰ES:\n",
        "\n",
        "RAG & LangChain:\n",
        "  â€¢ https://python.langchain.com/\n",
        "  â€¢ https://js.langchain.com/\n",
        "  â€¢ RAG Best Practices: https://docs.llamaindex.ai/\n",
        "\n",
        "Fine-Tuning & LoRA:\n",
        "  â€¢ HuggingFace Fine-Tuning: https://huggingface.co/docs/transformers/training\n",
        "  â€¢ PEFT (LoRA): https://github.com/huggingface/peft\n",
        "  â€¢ OpenAI Fine-Tuning: https://platform.openai.com/docs/guides/fine-tuning\n",
        "\n",
        "Vector Databases:\n",
        "  â€¢ Chroma: https://www.trychroma.com/\n",
        "  â€¢ Pinecone: https://www.pinecone.io/\n",
        "  â€¢ Weaviate: https://weaviate.io/\n",
        "\n",
        "Frameworks Agentic AI:\n",
        "  â€¢ LangChain: https://python.langchain.com/\n",
        "  â€¢ CrewAI: https://www.crewai.com/\n",
        "  â€¢ Llama Index: https://www.llamaindex.ai/\n",
        "\n",
        "Ã‰valuation des systÃ¨mes RAG:\n",
        "  â€¢ RAGAS: https://github.com/explodinggradients/ragas\n",
        "  â€¢ DeepEval: https://www.deepeval.com/\n",
        "\"\"\"\n",
        "print(resources)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}