{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjJSLFUCdlil"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/docling-project/docling/blob/main/docs/examples/rag_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkTMEJEYdlim"
      },
      "source": [
        "# RAG with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oJCbSlndlim"
      },
      "source": [
        "| Step | Tech | Execution |\n",
        "| --- | --- | --- |\n",
        "| Embedding | Hugging Face / Sentence Transformers | üíª Local |\n",
        "| Vector store | Milvus | üíª Local |\n",
        "| Gen AI | Hugging Face Inference API | üåê Remote |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17abfPJ9dlim"
      },
      "source": [
        "This example leverages the\n",
        "[LangChain Docling integration](../../integrations/langchain/), along with a Milvus\n",
        "vector store, as well as sentence-transformers embeddings.\n",
        "\n",
        "The presented `DoclingLoader` component enables you to:\n",
        "- use various document types in your LLM applications with ease and speed, and\n",
        "- leverage Docling's rich format for advanced, document-native grounding.\n",
        "\n",
        "`DoclingLoader` supports two different export modes:\n",
        "- `ExportType.MARKDOWN`: if you want to capture each input document as a separate\n",
        "  LangChain document, or\n",
        "- `ExportType.DOC_CHUNKS` (default): if you want to have each input document chunked and\n",
        "  to then capture each individual chunk as a separate LangChain document downstream.\n",
        "\n",
        "The example allows exploring both modes via parameter `EXPORT_TYPE`; depending on the\n",
        "value set, the example pipeline is then set up accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NG2icPjdlin"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgRI-7Eqdlin"
      },
      "source": [
        "- üëâ For best conversion speed, use GPU acceleration whenever available; e.g. if running on Colab, use GPU-enabled runtime.\n",
        "- Notebook uses HuggingFace's Inference API; for increased LLM quota, token can be provided via env var `HF_TOKEN`.\n",
        "- Requirements can be installed as shown below (`--no-warn-conflicts` meant for Colab's pre-populated Python env; feel free to remove for stricter usage):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les notebooks (Jupyter/Colab) tournent parfois sur un environnement virtuel cach√©.\n",
        "\n",
        "La commande shell **!pip** installe parfois les paquets dans le syst√®me global de la machine virtuelle, alors que le notebook utilise un environnement Python local.\n",
        "\n",
        "La commande **%pip** est un alias con√ßu sp√©cifiquement pour dire \"Installe l√† o√π ce notebook s'ex√©cute\"."
      ],
      "metadata": {
        "id": "h4Vu52vzrjyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y langchain langchain-classic\n",
        "# %pip uninstall -y langchain langchain-classic"
      ],
      "metadata": {
        "id": "3E14xN_pn6gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wNZeILHdlin"
      },
      "outputs": [],
      "source": [
        "#%pip install -q --progress-bar off --no-warn-conflicts langchain-docling langchain-core langchain-huggingface langchain_milvus langchain python-dotenv\n",
        "%pip install -q --progress-bar off langchain-classic langchain-docling langchain-core langchain-huggingface langchain_milvus langchain python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep langchain"
      ],
      "metadata": {
        "id": "MfAfCMphsvpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import langchain\n",
        "\n",
        "print(f\"Python executable: {sys.executable}\")\n",
        "print(f\"LangChain location: {langchain.__file__}\")\n",
        "# Cela devrait afficher quelque chose comme : .../site-packages/langchain/__init__.py\n"
      ],
      "metadata": {
        "id": "GVCiX4Z2s_0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important avec langchain 1.2.0** :\n",
        "=> Dans cette version de fin 2025), une rupture de compatibilit√© a √©t√© introduite : tout le module langchain.chains a √©t√© d√©plac√© dans le paquet langchain-classic pour all√©ger le c≈ìur du framework.\n",
        "\n",
        "Le paquet langchain-classic est d√©j√† install√© (version 1.0.1),\n",
        "Il suffit de mettre √† jour les imports."
      ],
      "metadata": {
        "id": "Q1ly0etRta9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Au lieu de : from langchain.chains import ...\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n"
      ],
      "metadata": {
        "id": "yM9QhOEFtHpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdf9gADtdlio"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_docling.loader import ExportType\n",
        "\n",
        "\n",
        "def _get_env_from_colab_or_os(key):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "\n",
        "        try:\n",
        "            return userdata.get(key)\n",
        "        except userdata.SecretNotFoundError:\n",
        "            pass\n",
        "    except ImportError:\n",
        "        pass\n",
        "    return os.getenv(key)\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# https://github.com/huggingface/transformers/issues/5486:\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "HF_TOKEN = _get_env_from_colab_or_os(\"HF_TOKEN\")\n",
        "FILE_PATH = [\"https://arxiv.org/pdf/2408.09869\"]  # Docling Technical Report\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "#GEN_MODEL_ID = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "GEN_MODEL_ID = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "EXPORT_TYPE = ExportType.DOC_CHUNKS\n",
        "QUESTION = \"Which are the main AI models in Docling?\"\n",
        "PROMPT = PromptTemplate.from_template(\n",
        "    \"Context information is below.\\n---------------------\\n{context}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {input}\\nAnswer:\\n\",\n",
        ")\n",
        "TOP_K = 3\n",
        "MILVUS_URI = str(Path(mkdtemp()) / \"docling.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INwTiHkrdlio"
      },
      "source": [
        "## Document loading\n",
        "\n",
        "Now we can instantiate our loader and load documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGXjO_Cedlio"
      },
      "outputs": [],
      "source": [
        "from langchain_docling import DoclingLoader\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "loader = DoclingLoader(\n",
        "    file_path=FILE_PATH,\n",
        "    export_type=EXPORT_TYPE,\n",
        "    chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        ")\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyTha7XRdlip"
      },
      "source": [
        "> Note: a message saying `\"Token indices sequence length is longer than the specified\n",
        "maximum sequence length...\"` can be ignored in this case ‚Äî details\n",
        "[here](https://github.com/docling-project/docling-core/issues/119#issuecomment-2577418826)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgU4-br9dlip"
      },
      "source": [
        "Determining the splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGRwZVPldlip"
      },
      "outputs": [],
      "source": [
        "if EXPORT_TYPE == ExportType.DOC_CHUNKS:\n",
        "    splits = docs\n",
        "elif EXPORT_TYPE == ExportType.MARKDOWN:\n",
        "    from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "\n",
        "    splitter = MarkdownHeaderTextSplitter(\n",
        "        headers_to_split_on=[\n",
        "            (\"#\", \"Header_1\"),\n",
        "            (\"##\", \"Header_2\"),\n",
        "            (\"###\", \"Header_3\"),\n",
        "        ],\n",
        "    )\n",
        "    splits = [split for doc in docs for split in splitter.split_text(doc.page_content)]\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected export type: {EXPORT_TYPE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqoJl0WCdlip"
      },
      "source": [
        "Inspecting some sample splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxIAB0Xedlip"
      },
      "outputs": [],
      "source": [
        "for d in splits[:3]:\n",
        "    print(f\"- {d.page_content=}\")\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPFhL44tdlip"
      },
      "source": [
        "## Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pymilvus[milvus_lite]"
      ],
      "metadata": {
        "id": "RtZ5VjBTek5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2BAG3Kcdlip"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_milvus import Milvus\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "milvus_uri = str(Path(mkdtemp()) / \"docling.db\")  # or set as needed\n",
        "vectorstore = Milvus.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    collection_name=\"docling_demo\",\n",
        "    connection_args={\"uri\": milvus_uri},\n",
        "    index_params={\"index_type\": \"FLAT\"},\n",
        "    drop_old=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPFhHOgxdlip"
      },
      "source": [
        "## RAG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip list | grep langchain"
      ],
      "metadata": {
        "id": "yRxhZMzfnhR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=GEN_MODEL_ID,\n",
        "    huggingfacehub_api_token=HF_TOKEN,\n",
        "    pipeline_kwargs={\n",
        "        \"max_new_tokens\": 500,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.95,\n",
        "        \"do_sample\": True,\n",
        "        \"return_full_text\": False  # ‚ö†Ô∏è CRITIQUE\n",
        "    }\n",
        ")\n",
        "\n",
        "def clip_text(text, threshold=100):\n",
        "    return f\"{text[:threshold]}...\" if len(text) > threshold else text"
      ],
      "metadata": {
        "id": "DQ45_SH7kOVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ7RON1Iu2Sd"
      },
      "outputs": [],
      "source": [
        "question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "resp_dict = rag_chain.invoke({\"input\": QUESTION})\n",
        "\n",
        "clipped_answer = clip_text(resp_dict[\"answer\"], threshold=200)\n",
        "print(f\"Question:\\n{resp_dict['input']}\\n\\nAnswer:\\n{clipped_answer}\")\n",
        "for i, doc in enumerate(resp_dict[\"context\"]):\n",
        "    print()\n",
        "    print(f\"Source {i + 1}:\")\n",
        "    print(f\"  text: {json.dumps(clip_text(doc.page_content, threshold=350))}\")\n",
        "    for key in doc.metadata:\n",
        "        if key != \"pk\":\n",
        "            val = doc.metadata.get(key)\n",
        "            clipped_val = clip_text(val) if isinstance(val, str) else val\n",
        "            print(f\"  {key}: {clipped_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Autre exemple** :"
      ],
      "metadata": {
        "id": "k7nOSY8P7qoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_classic.prompts import ChatPromptTemplate\n",
        "\n",
        "# Configuration du LLM local\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    task=\"text-generation\",  # ‚úÖ Correct pour mod√®le local\n",
        "    pipeline_kwargs={\n",
        "        \"max_new_tokens\": 500,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.95,\n",
        "        \"do_sample\": True,\n",
        "        \"return_full_text\": False  # ‚ö†Ô∏è CRITIQUE\n",
        "    }\n",
        ")\n",
        "\n",
        "# Saisie manuelle\n",
        "# context = input(\"Entrez le contexte: \")\n",
        "# question = input(\"Entrez la question: \")\n",
        "\n",
        "PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
        "Context: {context}\n",
        "Question: {input}\n",
        "Answer:\"\"\")\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "resp_dict = rag_chain.invoke({\"input\": QUESTION})\n",
        "\n",
        "# Affichage des r√©sultats\n",
        "clipped_answer = clip_text(resp_dict[\"answer\"], threshold=200)\n",
        "print(f\"Question:\\n{resp_dict['input']}\\n\\nAnswer:\\n{clipped_answer}\")\n",
        "\n",
        "for i, doc in enumerate(resp_dict[\"context\"]):\n",
        "    print(f\"\\nSource {i + 1}:\")\n",
        "    print(f\"  text: {json.dumps(clip_text(doc.page_content, threshold=350))}\")\n",
        "    for key in doc.metadata:\n",
        "        if key != \"pk\":\n",
        "            val = doc.metadata.get(key)\n",
        "            clipped_val = clip_text(val) if isinstance(val, str) else val\n",
        "            print(f\"  {key}: {clipped_val}\")\n"
      ],
      "metadata": {
        "id": "k-F7OIgEz4GH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}