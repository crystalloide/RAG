{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/RAG/blob/main/LAB03_FineTuning_Adapters_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWuAGMaEnlxO"
      },
      "source": [
        "# LAB03 : Fine-Tuning vs Adapters vs RAG ‚Äì Comparaison des 3 approches\n",
        "\n",
        "## Objectif\n",
        "- Comparer comment le fine-tuning, les adapters et RAG r√©solvent le m√™me probl√®me : faire r√©pondre un LLM √† des questions sp√©cifiques √† un domaine.\n",
        "\n",
        "## Dur√©e estim√©e\n",
        "- 90‚Äì120 minutes\n",
        "\n",
        "## Livrables\n",
        "- Notebook montrant les r√©sultats Q&A des trois strat√©gies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzknAvmgnlxP"
      },
      "source": [
        "## √âtape 1 : Setup (5 min)\n",
        "\n",
        "Installation des biblioth√®ques n√©cessaires :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "8ZSnV-GlnlxQ"
      },
      "outputs": [],
      "source": [
        "# !pip install -q openai datasets transformers faiss-cpu langchain chromadb\n",
        "!pip install -q --upgrade pip\n",
        "# Installer les packages principaux\n",
        "!pip install -q openai\n",
        "!pip install -q datasets\n",
        "!pip install -q transformers\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain langchain-community langchain-openai\n",
        "!pip install -q chromadb\n",
        "\n",
        "# Red√©marrer le runtime apr√®s installation\n",
        "# import os\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2-eLsHanlxQ"
      },
      "source": [
        "## √âtape 2 : D√©finir une Base de Connaissances Simple (10 min)\n",
        "\n",
        "Nous cr√©ons une base de connaissances fictive sur l'IA Agentique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llazsGWFnlxQ",
        "outputId": "dafe6762-5388-448f-f02c-d0e5876ea9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Base (KB):\n",
            "  1. Agentic AI agents use memory, tools, and goals to act.\n",
            "  2. LangChain and CrewAI are popular frameworks for building AI agents.\n",
            "  3. Retrieval-Augmented Generation (RAG) improves accuracy by fetching external knowledge.\n",
            "  4. Agents can iterate and refine their actions based on feedback.\n",
            "  5. Tool calling allows agents to interact with APIs and external systems.\n",
            "\n",
            "Test Questions:\n",
            "  1. What are the key components of Agentic AI?\n",
            "  2. Name one framework for AI agents.\n",
            "  3. How does RAG improve answers?\n"
          ]
        }
      ],
      "source": [
        "# Base de connaissances sur l'IA Agentique\n",
        "kb = [\n",
        "    \"Agentic AI agents use memory, tools, and goals to act.\",\n",
        "    \"LangChain and CrewAI are popular frameworks for building AI agents.\",\n",
        "    \"Retrieval-Augmented Generation (RAG) improves accuracy by fetching external knowledge.\",\n",
        "    \"Agents can iterate and refine their actions based on feedback.\",\n",
        "    \"Tool calling allows agents to interact with APIs and external systems.\"\n",
        "]\n",
        "\n",
        "# Questions de test\n",
        "questions = [\n",
        "    \"What are the key components of Agentic AI?\",\n",
        "    \"Name one framework for AI agents.\",\n",
        "    \"How does RAG improve answers?\"\n",
        "]\n",
        "\n",
        "print(\"Knowledge Base (KB):\")\n",
        "for i, doc in enumerate(kb, 1):\n",
        "    print(f\"  {i}. {doc}\")\n",
        "\n",
        "print(\"\\nTest Questions:\")\n",
        "for i, q in enumerate(questions, 1):\n",
        "    print(f\"  {i}. {q}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNR7cYlknlxQ"
      },
      "source": [
        "## √âtape 3 : Fine-Tuning (D√©mo Conceptuelle, 20 min)\n",
        "\n",
        "**Fine-tuning** = consiste √† mettre √† jour les poids du mod√®le avec de nouveaux exemples √©tiquet√©s.\n",
        "\n",
        "Voici √† quoi ressemblerait un dataset de fine-tuning :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH49Anb1nlxQ",
        "outputId": "6723b592-45b7-48aa-f855-301a334ff748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuning Training Dataset:\n",
            "Dataset({\n",
            "    features: ['prompt', 'completion'],\n",
            "    num_rows: 3\n",
            "})\n",
            "\n",
            "================================================================================\n",
            "FINE-TUNING OBSERVATIONS:\n",
            "================================================================================\n",
            "\n",
            "‚úì Avantages:\n",
            "  - Mod√®le hautement adapt√© au domaine\n",
            "  - Connaissances \"baked-in\" et rapides\n",
            "\n",
            "‚úó Inconv√©nients:\n",
            "  - Co√ªteux (calcul + infrastructure)\n",
            "  - Rigide (difficile de mettre √† jour les connaissances)\n",
            "  - N√©cessite un r√©entra√Ænement pour chaque nouveau domaine\n",
            "  - D√©pend de la qualit√© des donn√©es d'entra√Ænement\n",
            "\n",
            "üìå Avec OpenAI ou Hugging Face, vous uploaderiez ce dataset pour fine-tuning.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Donn√©es d'entra√Ænement pour fine-tuning\n",
        "train_data = Dataset.from_dict({\n",
        "    \"prompt\": [\n",
        "        \"Q: What are the key components of Agentic AI?\\nA:\",\n",
        "        \"Q: Name one framework for AI agents.\\nA:\",\n",
        "        \"Q: How does RAG improve answers?\\nA:\"\n",
        "    ],\n",
        "    \"completion\": [\n",
        "        \" Agentic AI agents use memory, tools, and goals to act.\",\n",
        "        \" LangChain is a framework for building AI agents.\",\n",
        "        \" RAG improves accuracy by fetching external knowledge before answering.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Fine-Tuning Training Dataset:\")\n",
        "print(train_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINE-TUNING OBSERVATIONS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "‚úì Avantages:\n",
        "  - Mod√®le hautement adapt√© au domaine\n",
        "  - Connaissances \"baked-in\" et rapides\n",
        "\n",
        "‚úó Inconv√©nients:\n",
        "  - Co√ªteux (calcul + infrastructure)\n",
        "  - Rigide (difficile de mettre √† jour les connaissances)\n",
        "  - N√©cessite un r√©entra√Ænement pour chaque nouveau domaine\n",
        "  - D√©pend de la qualit√© des donn√©es d'entra√Ænement\n",
        "\n",
        "üìå Avec OpenAI ou Hugging Face, vous uploaderiez ce dataset pour fine-tuning.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeIXdYg2nlxR"
      },
      "source": [
        "## √âtape 4 : Adapters / LoRA (D√©mo Conceptuelle, 20 min)\n",
        "\n",
        "**Adapters** = petites couches parameter-efficient que l'on entra√Æne au lieu de r√©-entra√Ænez le mod√®le entier.\n",
        "\n",
        "C'est une alternative au fine-tuning complet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdqvT1v6nlxR",
        "outputId": "bca796a1-cdf6-492d-8c3c-60a5ab1bd5b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Mod√®le charg√©: distilgpt2\n",
            "  Nombre total de param√®tres: 81,912,576\n",
            "\n",
            "================================================================================\n",
            "ADAPTERS / LoRA OBSERVATIONS:\n",
            "================================================================================\n",
            "\n",
            "‚úì Avantages:\n",
            "  - TR√àS peu de param√®tres √† entra√Æner (~0.1-1% du mod√®le original)\n",
            "  - √âconomique (moins de calcul, moins de m√©moire)\n",
            "  - Modulaire (plusieurs adapters pour diff√©rents domaines)\n",
            "  - Rapide √† mettre √† jour\n",
            "\n",
            "‚úó Inconv√©nients:\n",
            "  - N√©cessite toujours une infrastructure d'entra√Ænement\n",
            "  - Performance l√©g√®rement inf√©rieure au fine-tuning complet\n",
            "  - N√©cessite un framework compatible (PEFT, AdapterHub)\n",
            "\n",
            "üìå Avec LoRA, vous ne mettriez √† jour que quelques millions de param√®tres\n",
            "    au lieu de milliards, rendant l'entra√Ænement beaucoup plus efficace.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Charger un petit mod√®le pour la d√©mo\n",
        "model_name = \"distilgpt2\"\n",
        "try:\n",
        "    tok = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"‚úì Mod√®le charg√©: {model_name}\")\n",
        "    print(f\"  Nombre total de param√®tres: {total_params:,}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Note: T√©l√©chargement du mod√®le √©chou√© (attendu en mode hors-ligne)\")\n",
        "    print(f\"Erreur: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ADAPTERS / LoRA OBSERVATIONS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "‚úì Avantages:\n",
        "  - TR√àS peu de param√®tres √† entra√Æner (~0.1-1% du mod√®le original)\n",
        "  - √âconomique (moins de calcul, moins de m√©moire)\n",
        "  - Modulaire (plusieurs adapters pour diff√©rents domaines)\n",
        "  - Rapide √† mettre √† jour\n",
        "\n",
        "‚úó Inconv√©nients:\n",
        "  - N√©cessite toujours une infrastructure d'entra√Ænement\n",
        "  - Performance l√©g√®rement inf√©rieure au fine-tuning complet\n",
        "  - N√©cessite un framework compatible (PEFT, AdapterHub)\n",
        "\n",
        "üìå Avec LoRA, vous ne mettriez √† jour que quelques millions de param√®tres\n",
        "    au lieu de milliards, rendant l'entra√Ænement beaucoup plus efficace.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ByhjDujnlxR"
      },
      "source": [
        "## √âtape 5 : RAG ‚Äì D√©mo Interactive (30 min)\n",
        "\n",
        "Contrairement √† l'entra√Ænement, **RAG** r√©cup√®re les connaissances externes au moment de l'ex√©cution.\n",
        "\n",
        "Construisons un syst√®me RAG complet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV1rjy-UnlxR"
      },
      "source": [
        "### Configuration de RAG avec LangChain et ChromaDB\n",
        "\n",
        "Nous utilisons ChromaDB (une alternative l√©g√®re √† FAISS) pour cette d√©mo :"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community langchain-openai langchain-huggingface chromadb"
      ],
      "metadata": {
        "id": "s2-h0M0CtIs4"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "381WLHkTnlxR",
        "outputId": "e6d5faa0-f256-41b3-fbaa-e23c9b0044d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain et ChromaDB import√©s avec succ√®s.\n",
            "\n",
            "üìå Configuration du syst√®me RAG...\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os\n",
        "\n",
        "print(\"LangChain et ChromaDB import√©s avec succ√®s.\")\n",
        "print(\"\\nüìå Configuration du syst√®me RAG...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctpcbaodnlxR"
      },
      "source": [
        "### Option payante : RAG avec OpenAI (n√©cessite OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get API key from Colab Secrets (add it in the Secrets manager: üîë icon on left panel)\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Or if running locally with .env file:\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "\n",
        "print(\"‚úì API Key configured successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA1oQjDhxXw8",
        "outputId": "37419e78-e723-4fe7-eded-82a3ac688c8d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì API Key configured successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D√©mo RAG : Retrieval (R√©cup√©ration de Documents)"
      ],
      "metadata": {
        "id": "H_KkEJX9JExS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_rag_with_openai():\n",
        "    \"\"\"\n",
        "    Configure RAG avec OpenAI embeddings et ChatOpenAI - VERSION MODERNE.\n",
        "    N√©cessite une cl√© API OpenAI.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    from getpass import getpass\n",
        "\n",
        "    # V√©rifier ou demander la cl√© API\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        print(\"\\nüîë Cl√© API OpenAI requise pour cette d√©mo.\")\n",
        "        print(\"   Cr√©ez une cl√© sur https://platform.openai.com/api-keys\")\n",
        "        api_key = getpass(\"Entrez votre OPENAI_API_KEY: \")\n",
        "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "    from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "    from langchain_core.documents import Document\n",
        "    from langchain_core.prompts import ChatPromptTemplate\n",
        "    from langchain_core.runnables import RunnablePassthrough\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "    try:\n",
        "    # üóëÔ∏è Supprimer la base Chroma existante\n",
        "    #    chroma_dir = \"/content/chroma_db\"\n",
        "    #    if os.path.exists(chroma_dir):\n",
        "    #        print(f\"üóëÔ∏è  Suppression de la base existante: {chroma_dir}\")\n",
        "    #        shutil.rmtree(chroma_dir)\n",
        "    #        print(\"‚úì Base supprim√©e\")\n",
        "        print(\"üìö Cr√©ation des documents...\")\n",
        "        docs = [Document(page_content=x) for x in kb]\n",
        "\n",
        "        print(\"üî® Initialisation des embeddings OpenAI...\")\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "\n",
        "        print(\"üíæ Construction de la base vectorielle...\")\n",
        "        db = Chroma.from_documents(docs, embeddings)\n",
        "        #db = Chroma.from_documents(docs, embeddings, persist_directory=chroma_dir)\n",
        "        retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "        print(\"ü§ñ Configuration de la cha√Æne RAG...\")\n",
        "\n",
        "        # Template de prompt\n",
        "        template = \"\"\"R√©ponds √† la question en te basant sur le contexte suivant :\n",
        "\n",
        "Contexte: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "R√©ponse:\"\"\"\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_template(template)\n",
        "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "        # Fonction pour formater les documents\n",
        "        def format_docs(docs):\n",
        "            return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # Cha√Æne RAG moderne avec LCEL\n",
        "        qa_chain = (\n",
        "            {\n",
        "                \"context\": retriever | format_docs,\n",
        "                \"question\": RunnablePassthrough()\n",
        "            }\n",
        "            | prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Syst√®me RAG configur√© avec succ√®s!\\n\")\n",
        "        return qa_chain, db\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur lors de la configuration RAG: {e}\")\n",
        "        return None, None\n",
        "\n",
        "print(\"‚úì Fonction setup_rag_with_openai() d√©finie (version moderne LCEL).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8TOTk0G_2FT",
        "outputId": "407d5dd6-893a-4464-e01c-1bbcba0e3337"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Fonction setup_rag_with_openai() d√©finie (version moderne LCEL).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QyHefdnlxS"
      },
      "source": [
        "### Full RAG Pipeline avec Q&A Compl√®te\n",
        "\n",
        "Maintenant, montrons une pipeline Q&A RAG compl√®te :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M5INxxjnlxS",
        "outputId": "7588963b-0e79-44b9-fb7d-cead285cafb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Cr√©ation des documents...\n",
            "üî® Initialisation des embeddings OpenAI...\n",
            "üíæ Construction de la base vectorielle...\n",
            "ü§ñ Configuration de la cha√Æne RAG...\n",
            "‚úÖ Syst√®me RAG configur√© avec succ√®s!\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FULL RAG Q&A avec ChatOpenAI (gpt-4o-mini)\n",
            "================================================================================\n",
            "\n",
            "‚ùì Q: What are the components of Agentic AI?\n",
            "‚úÖ A: Les composants de l'Agentic AI sont la planification, l'action et l'apprentissage.\n",
            "\n",
            "‚ùì BONUS Q: Explain Agentic AI like I'm 10 years old.\n",
            "‚úÖ A: D'accord ! Imagine que tu as un robot qui veut devenir un super cuisinier. Pour y arriver, ce robot doit faire trois choses :\n",
            "\n",
            "1. **Planifier** : D'abord, il doit d√©cider quel plat il veut cuisiner. Par exemple, il peut choisir de faire des cr√™pes. Il doit penser √† tous les ingr√©dients dont il a besoin et √† la recette.\n",
            "\n",
            "2. **Agir** : Ensuite, le robot doit se mettre au travail. Il va chercher les ingr√©dients, les m√©langer et cuire les cr√™pes. C'est comme quand tu suis une recette pour faire un g√¢teau.\n",
            "\n",
            "3. **Apprendre** : Apr√®s avoir cuisin√©, le robot go√ªte ses cr√™pes. S'il trouve qu'elles sont un peu trop sal√©es, il se souviendra de ne pas mettre autant de sel la prochaine fois. Il apprend de ses erreurs pour s'am√©liorer.\n",
            "\n",
            "Donc, l'Agentic AI, c'est comme un robot qui planifie ce qu'il veut faire, agit pour le faire, et apprend de ses exp√©riences pour devenir meilleur. C'est un peu comme toi quand tu apprends √† faire quelque chose de nouveau !\n",
            "\n",
            "‚úì Code RAG avec OpenAI .\n"
          ]
        }
      ],
      "source": [
        "# OPTION Payante : utilisation d'OpenAI\n",
        "# \"\"\"\n",
        "import os\n",
        "if os.getenv(\"OPENAI_API_KEY\"):\n",
        "    qa, db = setup_rag_with_openai()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FULL RAG Q&A avec ChatOpenAI (gpt-4o-mini)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for question in questions:\n",
        "        print(f\"\\n‚ùì Q: {question}\")\n",
        "        answer = qa.invoke(question)\n",
        "        print(f\"‚úÖ A: {answer}\")\n",
        "\n",
        "    # Question bonus\n",
        "    bonus_q = \"Explain Agentic AI like I'm 10 years old.\"\n",
        "    print(f\"\\n‚ùì BONUS Q: {bonus_q}\")\n",
        "    bonus_answer = qa.invoke(bonus_q)\n",
        "    print(f\"‚úÖ A: {bonus_answer}\")\n",
        "else:\n",
        "    print(\"üí° Pour la d√©mo :\")\n",
        "    print(\"   1. Cr√©ez une cl√© sur https://platform.openai.com/api-keys\")\n",
        "    print(\"   2. Entrez votre cl√© valide API\")\n",
        "##\"\"\"\n",
        "\n",
        "print(\"\\n‚úì Code RAG avec OpenAI .\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG3JMYuInlxR"
      },
      "source": [
        "__________________\n",
        "## Option gratuite : RAG avec HuggingFace Embeddings (sans API externe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLhEAgepnlxS"
      },
      "source": [
        "### D√©mo RAG : Retrieval (R√©cup√©ration de Documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "vec = model.encode(\"test\", convert_to_numpy=True)\n",
        "print(\"sentence-transformers/all-mpnet-base-v2\",\"embedding shape:\", vec.shape)  # ex: (384,) ou (768,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hLxcy_cMaZ5",
        "outputId": "dd1d29bb-df81-4c85-c54c-f6603d01933e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence-transformers/all-mpnet-base-v2 embedding shape: (768,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vec = model.encode(\"test\", convert_to_numpy=True)\n",
        "print(\"sentence-transformers/all-MiniLM-L6-v2\",\"embedding shape:\", vec.shape)  # ex: (384,) ou (768,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRtI_D-tRXVk",
        "outputId": "f46d7a27-4535-4dd7-c6a5-df7e92e9dea7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence-transformers/all-MiniLM-L6-v2 embedding shape: (384,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_rag_with_huggingface(kb):\n",
        "    \"\"\"\n",
        "    Configure RAG avec HuggingFace embeddings.\n",
        "    kb : liste de textes (knowledge base)\n",
        "    \"\"\"\n",
        "    import os, shutil\n",
        "    from langchain_community.vectorstores import chroma\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    from langchain_core.documents import Document\n",
        "\n",
        "    #persist_dir=\"/content/chroma_db\"\n",
        "\n",
        "    try:\n",
        "    # üóëÔ∏è Supprimer la base Chroma si existante\n",
        "    #    if os.path.exists(persist_dir):\n",
        "    #        print(f\"üóëÔ∏è  Suppression de la base existante: {persist_dir}\")\n",
        "    #        shutil.rmtree(persist_dir)\n",
        "    #        print(\"‚úì Base supprim√©e\")\n",
        "\n",
        "        # Choisir un mod√®le et l'utiliser partout\n",
        "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        print(f\"üì¶ Mod√®le s√©lectionn√©: {model_name}\")\n",
        "\n",
        "        # Cr√©er les documents\n",
        "        docs = [Document(page_content=x) for x in kb]\n",
        "\n",
        "        # Charger les embeddings HF\n",
        "        print(\"üìö Chargement des embeddings HuggingFace...\")\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "        # (Debug) v√©rifier la dimension d'embedding si l'API le permet\n",
        "        try:\n",
        "            # Certains wrappers exposent .embed_query ou .embed_documents ; on teste avec une phrase\n",
        "            sample = embeddings.embed_query(\"test\")\n",
        "            print(\"üîé Embedding sample length:\", len(sample))\n",
        "        except Exception:\n",
        "            # Si la m√©thode diff√®re, on ignore le test\n",
        "            pass\n",
        "\n",
        "        # Construire la base vectorielle (Chroma)\n",
        "        print(\"üî® Construction de la base vectorielle...\")\n",
        "        #db = Chroma.from_documents(docs, embeddings, persist_directory=persist_dir)\n",
        "        db = Chroma.from_documents(docs, embeddings)\n",
        "\n",
        "        # Cr√©er un retriever\n",
        "        retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "        return retriever, db\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Erreur lors de la configuration RAG: {e}\")\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "byq0K-_HNAfH"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_rag_qa(kb, questions):\n",
        "    \"\"\"\n",
        "    D√©montre un Q&A RAG simple sans LLM externe.\n",
        "    kb : liste de textes (knowledge base)\n",
        "    questions : liste de cha√Ænes (questions)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SIMPLE RAG Q&A (Retrieval-based)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    retriever, _ = setup_rag_with_huggingface(kb)\n",
        "\n",
        "    if not retriever:\n",
        "        print(\"‚ö†Ô∏è  Impossible de configurer le retriever.\")\n",
        "        return\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for question in questions:\n",
        "        # Essayer la m√©thode la plus courante, fallback si n√©cessaire\n",
        "        try:\n",
        "            docs = retriever.get_relevant_documents(question)\n",
        "        except AttributeError:\n",
        "            try:\n",
        "                docs = retriever.retrieve(question)\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    docs = retriever.invoke(question)\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Impossible de r√©cup√©rer des documents pour la question: {e}\")\n",
        "                    docs = []\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Erreur lors de la r√©cup√©ration: {e}\")\n",
        "            docs = []\n",
        "\n",
        "        # Si docs est None, le convertir en liste vide\n",
        "        if docs is None:\n",
        "            docs = []\n",
        "\n",
        "        # Combiner les documents r√©cup√©r√©s comme contexte\n",
        "        context = \"\\n\".join([f\"- {getattr(doc, 'page_content', str(doc))}\" for doc in docs])\n",
        "\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"context\": context,\n",
        "            \"source_docs\": len(docs)\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"\\n‚ùì Q: {question}\")\n",
        "        print(f\"üìñ Context Retrieved ({result['source_docs']} doc):\")\n",
        "        print(f\"   {context}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Exemple d'appel (d√©finir kb et questions avant)\n",
        "kb = [\n",
        "    \"Agentic AI combines planning, acting, and learning to achieve goals.\",\n",
        "    \"Components include a planner, executor, memory, and reward model.\"\n",
        "]\n",
        "questions = [\"What are the components of Agentic AI?\"]\n",
        "\n",
        "rag_results = simple_rag_qa(kb, questions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B62dNMONzEE",
        "outputId": "42a3fb7d-97ae-4df7-d03b-e2d201473f80"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SIMPLE RAG Q&A (Retrieval-based)\n",
            "================================================================================\n",
            "üì¶ Mod√®le s√©lectionn√©: sentence-transformers/all-MiniLM-L6-v2\n",
            "üìö Chargement des embeddings HuggingFace...\n",
            "üîé Embedding sample length: 384\n",
            "üî® Construction de la base vectorielle...\n",
            "‚ö†Ô∏è  Erreur lors de la configuration RAG: Collection expecting embedding with dimension of 1536, got 384\n",
            "‚ö†Ô∏è  Impossible de configurer le retriever.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5JjwYx0nlxS",
        "outputId": "b5981270-8814-45f6-9497-ddc05135b8db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Mod√®le s√©lectionn√©: sentence-transformers/all-MiniLM-L6-v2\n",
            "üìö Chargement des embeddings HuggingFace...\n",
            "üîé Embedding sample length: 384\n",
            "üî® Construction de la base vectorielle...\n",
            "‚ö†Ô∏è  Erreur lors de la configuration RAG: Collection expecting embedding with dimension of 1536, got 384\n",
            "‚ö†Ô∏è  RAG n'a pas pu √™tre configur√©. V√©rifiez votre connexion.\n"
          ]
        }
      ],
      "source": [
        "# Configurer RAG avec HuggingFace (pas de cl√© API requise)\n",
        "retriever, db = setup_rag_with_huggingface(kb)\n",
        "\n",
        "if retriever:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RAG RETRIEVAL DEMO\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Tester la r√©cup√©ration\n",
        "    test_query = \"What are the components of Agentic AI?\"\n",
        "    print(f\"\\nüìù Question: {test_query}\")\n",
        "\n",
        "    retrieved_docs = retriever.invoke(test_query)\n",
        "\n",
        "    print(f\"\\nüìñ Documents r√©cup√©r√©s (top {len(retrieved_docs)}):\")\n",
        "    for i, doc in enumerate(retrieved_docs, 1):\n",
        "        print(f\"  [{i}] {doc.page_content}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  RAG n'a pas pu √™tre configur√©. V√©rifiez votre connexion.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYfaP_pWnlxS",
        "outputId": "8e6fa4f5-cdcc-4eb0-a6da-9b6764cd3ed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SIMPLE RAG Q&A (Retrieval-based)\n",
            "================================================================================\n",
            "üì¶ Mod√®le s√©lectionn√©: sentence-transformers/all-MiniLM-L6-v2\n",
            "üìö Chargement des embeddings HuggingFace...\n",
            "üîé Embedding sample length: 384\n",
            "üî® Construction de la base vectorielle...\n",
            "‚ö†Ô∏è  Erreur lors de la configuration RAG: Collection expecting embedding with dimension of 1536, got 384\n",
            "‚ö†Ô∏è  Impossible de configurer le retriever.\n"
          ]
        }
      ],
      "source": [
        "# Fonction pour effectuer Q&A RAG simple\n",
        "def simple_rag_qa():\n",
        "    \"\"\"\n",
        "    D√©montre un Q&A RAG simple sans LLM externe.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SIMPLE RAG Q&A (Retrieval-based)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    retriever, _ = setup_rag_with_huggingface(kb)\n",
        "\n",
        "    if not retriever:\n",
        "        print(\"‚ö†Ô∏è  Impossible de configurer le retriever.\")\n",
        "        return\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for question in questions:\n",
        "        docs = retriever.invoke(question)\n",
        "\n",
        "        # Combiner les documents r√©cup√©r√©s comme contexte\n",
        "        context = \"\\n\".join([f\"- {doc.page_content}\" for doc in docs])\n",
        "\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"context\": context,\n",
        "            \"source_docs\": len(docs)\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"\\n‚ùì Q: {question}\")\n",
        "        print(f\"üìñ Context Retrieved ({result['source_docs']} doc):\")\n",
        "        print(f\"   {context}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Ex√©cuter la d√©mo\n",
        "rag_results = simple_rag_qa()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ6tuzS_nlxS"
      },
      "source": [
        "## √âtape 6 : Comparaison des R√©sultats (15‚Äì20 min)\n",
        "\n",
        "Cr√©ons un tableau comparatif des trois approches :"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Cr√©er le tableau comparatif\n",
        "comparison_data = {\n",
        "    \"Aspect\": [\n",
        "        \"Accuracy\",\n",
        "        \"Cost\",\n",
        "        \"Training Time\",\n",
        "        \"Update Frequency\",\n",
        "        \"Knowledge Freshness\",\n",
        "        \"Scalability\",\n",
        "        \"Complexity\",\n",
        "        \"Infrastructure\"\n",
        "    ],\n",
        "    \"Fine-Tuning\": [\n",
        "        \"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\",\n",
        "        \"üí∞üí∞üí∞üí∞\",\n",
        "        \"üïê Hours/Days\",\n",
        "        \"Monthly/Quarterly\",\n",
        "        \"Stale (until retrained)\",\n",
        "        \"Fair (one model per domain)\",\n",
        "        \"üî¥ High\",\n",
        "        \"GPUs required\"\n",
        "    ],\n",
        "    \"Adapters/LoRA\": [\n",
        "        \"‚≠ê‚≠ê‚≠ê‚≠ê\",\n",
        "        \"üí∞üí∞\",\n",
        "        \"üïê Minutes/Hours\",\n",
        "        \"Weekly\",\n",
        "        \"Semi-fresh\",\n",
        "        \"Good (modular)\",\n",
        "        \"üü° Medium\",\n",
        "        \"GPUs (lighter)\"\n",
        "    ],\n",
        "    \"RAG\": [\n",
        "        \"‚≠ê‚≠ê‚≠ê\",\n",
        "        \"üí∞\",\n",
        "        \"‚ö° Real-time\",\n",
        "        \"Real-time\",\n",
        "        \"‚ú® Always Fresh\",\n",
        "        \"Excellent\",\n",
        "        \"üü¢ Low\",\n",
        "        \"Vector DB + API\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Cr√©ation du HTML stylis√©\n",
        "html = \"\"\"\n",
        "<style>\n",
        "    .comparison-table {\n",
        "        width: 100%;\n",
        "        max-width: 1200px;\n",
        "        margin: 20px auto;\n",
        "        background: linear-gradient(to bottom right, #f8fafc, #eff6ff);\n",
        "        border-radius: 8px;\n",
        "        overflow: hidden;\n",
        "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    .table-header {\n",
        "        background: linear-gradient(to right, #2563eb, #4f46e5);\n",
        "        padding: 20px;\n",
        "        text-align: center;\n",
        "    }\n",
        "    .table-header h1 {\n",
        "        color: white;\n",
        "        font-size: 24px;\n",
        "        font-weight: bold;\n",
        "        margin: 0;\n",
        "    }\n",
        "    .styled-table {\n",
        "        width: 100%;\n",
        "        border-collapse: collapse;\n",
        "        background: white;\n",
        "    }\n",
        "    .styled-table thead tr {\n",
        "        background-color: #f1f5f9;\n",
        "        border-bottom: 2px solid #cbd5e1;\n",
        "    }\n",
        "    .styled-table th {\n",
        "        padding: 16px;\n",
        "        text-align: left;\n",
        "        font-weight: 600;\n",
        "        color: #334155;\n",
        "        width: 25%;\n",
        "    }\n",
        "    .styled-table td {\n",
        "        padding: 16px;\n",
        "        color: #475569;\n",
        "        border-bottom: 1px solid #e2e8f0;\n",
        "    }\n",
        "    .styled-table tbody tr:nth-child(even) {\n",
        "        background-color: #f8fafc;\n",
        "    }\n",
        "    .styled-table tbody tr:hover {\n",
        "        background-color: #dbeafe;\n",
        "        transition: background-color 0.2s;\n",
        "    }\n",
        "    .styled-table tbody tr td:first-child {\n",
        "        font-weight: 500;\n",
        "        color: #1e293b;\n",
        "    }\n",
        "    .table-footer {\n",
        "        background-color: #f8fafc;\n",
        "        padding: 20px;\n",
        "        border-top: 1px solid #e2e8f0;\n",
        "    }\n",
        "    .footer-grid {\n",
        "        display: grid;\n",
        "        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n",
        "        gap: 16px;\n",
        "        font-size: 14px;\n",
        "    }\n",
        "    .footer-item {\n",
        "        display: flex;\n",
        "        gap: 8px;\n",
        "        color: #1e40af;  /* Bleu fonc√© pour tout le texte */\n",
        "    }\n",
        "    .footer-item strong {\n",
        "        color: #1e3a8a;  /* Bleu fonc√© */\n",
        "    }\n",
        "    .footer-item span:first-child {\n",
        "        font-size: 18px;\n",
        "    }\n",
        "</style>\n",
        "\n",
        "<div class=\"comparison-table\">\n",
        "    <div class=\"table-header\">\n",
        "        <h1>Tableau Comparatif: Fine-Tuning vs Adapters/LoRA vs RAG</h1>\n",
        "    </div>\n",
        "\n",
        "    <table class=\"styled-table\">\n",
        "        <thead>\n",
        "            <tr>\n",
        "                <th>Aspect</th>\n",
        "                <th>Fine-Tuning</th>\n",
        "                <th>Adapters/LoRA</th>\n",
        "                <th>RAG</th>\n",
        "            </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "\"\"\"\n",
        "\n",
        "# Ajouter les lignes du tableau\n",
        "for _, row in df.iterrows():\n",
        "    html += f\"\"\"\n",
        "            <tr>\n",
        "                <td>{row['Aspect']}</td>\n",
        "                <td>{row['Fine-Tuning']}</td>\n",
        "                <td>{row['Adapters/LoRA']}</td>\n",
        "                <td>{row['RAG']}</td>\n",
        "            </tr>\n",
        "\"\"\"\n",
        "\n",
        "html += \"\"\"\n",
        "        </tbody>\n",
        "    </table>\n",
        "\n",
        "    <div class=\"table-footer\">\n",
        "        <div class=\"footer-grid\">\n",
        "            <div class=\"footer-item\">\n",
        "                <span>üí°</span>\n",
        "                <div><strong>Fine-Tuning:</strong> Meilleure pr√©cision mais co√ªteux</div>\n",
        "            </div>\n",
        "            <div class=\"footer-item\">\n",
        "                <span>üí°</span>\n",
        "                <div><strong>Adapters/LoRA:</strong> Bon compromis co√ªt/performance</div>\n",
        "            </div>\n",
        "            <div class=\"footer-item\">\n",
        "                <span>üí°</span>\n",
        "                <div><strong>RAG:</strong> Donn√©es fra√Æches, faible complexit√©</div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "# Afficher le tableau stylis√©\n",
        "display(HTML(html))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "PKmcHDrbA48S",
        "outputId": "d298eab5-46d2-4e6b-e136-944615e275b6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    .comparison-table {\n",
              "        width: 100%;\n",
              "        max-width: 1200px;\n",
              "        margin: 20px auto;\n",
              "        background: linear-gradient(to bottom right, #f8fafc, #eff6ff);\n",
              "        border-radius: 8px;\n",
              "        overflow: hidden;\n",
              "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
              "    }\n",
              "    .table-header {\n",
              "        background: linear-gradient(to right, #2563eb, #4f46e5);\n",
              "        padding: 20px;\n",
              "        text-align: center;\n",
              "    }\n",
              "    .table-header h1 {\n",
              "        color: white;\n",
              "        font-size: 24px;\n",
              "        font-weight: bold;\n",
              "        margin: 0;\n",
              "    }\n",
              "    .styled-table {\n",
              "        width: 100%;\n",
              "        border-collapse: collapse;\n",
              "        background: white;\n",
              "    }\n",
              "    .styled-table thead tr {\n",
              "        background-color: #f1f5f9;\n",
              "        border-bottom: 2px solid #cbd5e1;\n",
              "    }\n",
              "    .styled-table th {\n",
              "        padding: 16px;\n",
              "        text-align: left;\n",
              "        font-weight: 600;\n",
              "        color: #334155;\n",
              "        width: 25%;\n",
              "    }\n",
              "    .styled-table td {\n",
              "        padding: 16px;\n",
              "        color: #475569;\n",
              "        border-bottom: 1px solid #e2e8f0;\n",
              "    }\n",
              "    .styled-table tbody tr:nth-child(even) {\n",
              "        background-color: #f8fafc;\n",
              "    }\n",
              "    .styled-table tbody tr:hover {\n",
              "        background-color: #dbeafe;\n",
              "        transition: background-color 0.2s;\n",
              "    }\n",
              "    .styled-table tbody tr td:first-child {\n",
              "        font-weight: 500;\n",
              "        color: #1e293b;\n",
              "    }\n",
              "    .table-footer {\n",
              "        background-color: #f8fafc;\n",
              "        padding: 20px;\n",
              "        border-top: 1px solid #e2e8f0;\n",
              "    }\n",
              "    .footer-grid {\n",
              "        display: grid;\n",
              "        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n",
              "        gap: 16px;\n",
              "        font-size: 14px;\n",
              "    }\n",
              "    .footer-item {\n",
              "        display: flex;\n",
              "        gap: 8px;\n",
              "        color: #1e40af;  /* Bleu fonc√© pour tout le texte */\n",
              "    }\n",
              "    .footer-item strong {\n",
              "        color: #1e3a8a;  /* Bleu fonc√© */\n",
              "    }\n",
              "    .footer-item span:first-child {\n",
              "        font-size: 18px;\n",
              "    }\n",
              "</style>\n",
              "\n",
              "<div class=\"comparison-table\">\n",
              "    <div class=\"table-header\">\n",
              "        <h1>Tableau Comparatif: Fine-Tuning vs Adapters/LoRA vs RAG</h1>\n",
              "    </div>\n",
              "    \n",
              "    <table class=\"styled-table\">\n",
              "        <thead>\n",
              "            <tr>\n",
              "                <th>Aspect</th>\n",
              "                <th>Fine-Tuning</th>\n",
              "                <th>Adapters/LoRA</th>\n",
              "                <th>RAG</th>\n",
              "            </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "\n",
              "            <tr>\n",
              "                <td>Accuracy</td>\n",
              "                <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>\n",
              "                <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>\n",
              "                <td>‚≠ê‚≠ê‚≠ê</td>\n",
              "            </tr>\n",
              "\n",
              "            <tr>\n",
              "                <td>Cost</td>\n",
              "                <td>üí∞üí∞üí∞üí∞</td>\n",
              "                <td>üí∞üí∞</td>\n",
              "                <td>üí∞</td>\n",
              "            </tr>\n",
              "\n",
              "            <tr>\n",
              "                <td>Training Time</td>\n",
              "                <td>üïê Hours/Days</td>\n",
              "                <td>üïê Minutes/Hours</td>\n",
              "                <td>‚ö° Real-time</td>\n",
              "            </tr>\n",
              "\n",
              "            <tr>\n",
              "                <td>Update Frequency</td>\n",
              "                <td>Monthly/Quarterly</td>\n",
              "                <td>Weekly</td>\n",
              "                <td>Real-time</td>\n",
              "            </tr>\n",
              "\n",
              "            <tr>\n",
              "                <td>Knowledge Freshness</td>\n",
              "                <td>Stale (until retrained)</td>\n",
              "                <td>Semi-fresh</td>\n",
              "                <td>‚ú® Always Fresh</td>\n",
              "            </tr>\n",
              "\n",
              "            <tr>\n",
              "                <td>Scalability</td>\n",
              "                <td>Fair (one model per domain)</td>\n",
              "                <td>Good (modular)</td>\n",
              "                <td>Excellent</td>\n",
              "            </tr>\n",
              "\n",
              "            <tr>\n",
              "                <td>Complexity</td>\n",
              "                <td>üî¥ High</td>\n",
              "                <td>üü° Medium</td>\n",
              "                <td>üü¢ Low</td>\n",
              "            </tr>\n",
              "\n",
              "            <tr>\n",
              "                <td>Infrastructure</td>\n",
              "                <td>GPUs required</td>\n",
              "                <td>GPUs (lighter)</td>\n",
              "                <td>Vector DB + API</td>\n",
              "            </tr>\n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "    \n",
              "    <div class=\"table-footer\">\n",
              "        <div class=\"footer-grid\">\n",
              "            <div class=\"footer-item\">\n",
              "                <span>üí°</span>\n",
              "                <div><strong>Fine-Tuning:</strong> Meilleure pr√©cision mais co√ªteux</div>\n",
              "            </div>\n",
              "            <div class=\"footer-item\">\n",
              "                <span>üí°</span>\n",
              "                <div><strong>Adapters/LoRA:</strong> Bon compromis co√ªt/performance</div>\n",
              "            </div>\n",
              "            <div class=\"footer-item\">\n",
              "                <span>üí°</span>\n",
              "                <div><strong>RAG:</strong> Donn√©es fra√Æches, faible complexit√©</div>\n",
              "            </div>\n",
              "        </div>\n",
              "    </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diGmb8p2nlxS"
      },
      "source": [
        "### Tableau D√©taill√© des Cas d'Usage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "html_table = \"\"\"\n",
        "<table style=\"width:100%; border-collapse: collapse; font-family: monospace;\">\n",
        "<tr style=\"background-color: #aaaaaa;\">\n",
        "    <th style=\"border: 1px solid #ddd; padding: 16px; color: Black; text-align: left;\">M√©thode</th>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 16px; color: black; text-align: left;\">Avantages</th>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 16px; color: black; text-align: left;\">Inconv√©nients</th>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 16px; color: black; text-align: left;\">Cas d'usage</th>\n",
        "</tr>\n",
        "\"\"\"\n",
        "\n",
        "for idx, row in df_usecases.iterrows():\n",
        "    pros_html = \"<br>\".join(row['Pros'].split('\\n'))\n",
        "    cons_html = \"<br>\".join(row['Cons'].split('\\n'))\n",
        "    use_html = \"<br>\".join(row['Best Use Case'].split('\\n'))\n",
        "\n",
        "    html_table += f\"\"\"\n",
        "<tr>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; color: white; font-weight: bold;\">{row['Method']}</td>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; color: green;\">{pros_html}</td>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; color: red;\">{cons_html}</td>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px;\">{use_html}</td>\n",
        "</tr>\n",
        "\"\"\"\n",
        "\n",
        "html_table += \"</table>\"\n",
        "display(HTML(html_table))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "Oscs-aztCvVh",
        "outputId": "b89a0730-03e4-4a15-c413-eefad9ff96f8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<table style=\"width:100%; border-collapse: collapse; font-family: monospace;\">\n",
              "<tr style=\"background-color: #aaaaaa;\">\n",
              "    <th style=\"border: 1px solid #ddd; padding: 16px; color: Black; text-align: left;\">M√©thode</th>\n",
              "    <th style=\"border: 1px solid #ddd; padding: 16px; color: black; text-align: left;\">Avantages</th>\n",
              "    <th style=\"border: 1px solid #ddd; padding: 16px; color: black; text-align: left;\">Inconv√©nients</th>\n",
              "    <th style=\"border: 1px solid #ddd; padding: 16px; color: black; text-align: left;\">Cas d'usage</th>\n",
              "</tr>\n",
              "\n",
              "<tr>\n",
              "    <td style=\"border: 1px solid #ddd; padding: 10px; color: white; font-weight: bold;\">Fine-Tuning</td>\n",
              "    <td style=\"border: 1px solid #ddd; padding: 10px; color: green;\">‚úì Highly accurate<br>‚úì Baked-in knowledge<br>‚úì Very fast inference</td>\n",
              "    <td style=\"border: 1px solid #ddd; padding: 10px; color: red;\">‚úó Expensive<br>‚úó Rigid (hard to update)<br>‚úó Retrain per domain</td>\n",
              "    <td style=\"border: 1px solid #ddd; padding: 10px;\">Narrow domain apps<br>(e.g., medical chatbot)</td>\n",
              "</tr>\n",
              "\n",
              "<tr>\n",
              "    <td style=\"border: 1px solid #ddd; padding: 10px; color: white; font-weight: bold;\">Adapters (LoRA)</td>\n",
              "    <td style=\"border: 1px solid #ddd; padding: 10px; color: green;\">‚úì Cheap fine-tuning<br>‚úì Modular/multi-domain<br>‚úì Fast training</td>\n",
              "    <td style=\"border: 1px solid #ddd; padding: 10px; color: red;\">‚úó Still needs training<br>‚úó Slightly lower accuracy<br>‚úó Framework dependent</td>\n",
              "    <td style=\"border: 1px solid #ddd; padding: 10px;\">Domain adaptation<br>(multiple specialized tasks)</td>\n",
              "</tr>\n",
              "\n",
              "<tr>\n",
              "    <td style=\"border: 1px solid #ddd; padding: 10px; color: white; font-weight: bold;\">RAG</td>\n",
              "    <td style=\"border: 1px solid #ddd; padding: 10px; color: green;\">‚úì Flexible<br>‚úì Real-time updates<br>‚úì No retraining</td>\n",
              "    <td style=\"border: 1px solid #ddd; padding: 10px; color: red;\">‚úó Retriever quality matters<br>‚úó Latency (retrieval step)<br>‚úó Context window limits</td>\n",
              "    <td style=\"border: 1px solid #ddd; padding: 10px;\">Dynamic knowledge<br>(News, FAQ, real-time data)</td>\n",
              "</tr>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGiC4SfWnlxT"
      },
      "source": [
        "## R√©sum√© et Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWh7Oau8nlxT"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë              LAB03 - CONCLUSIONS & KEY TAKEAWAYS                              ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "üéØ TROIS STRAT√âGIES POUR ADAPTER LES LLMs √Ä UN DOMAINE:\n",
        "\n",
        "1Ô∏è‚É£  FINE-TUNING\n",
        "   ‚îî‚îÄ Entra√Æner le mod√®le sur de nouvelles donn√©es\n",
        "   ‚îî‚îÄ ‚úÖ Tr√®s pr√©cis | ‚ùå Co√ªteux, rigide\n",
        "   ‚îî‚îÄ üí° Meilleur pour: Applications de niche\n",
        "\n",
        "2Ô∏è‚É£  ADAPTERS / LoRA\n",
        "   ‚îî‚îÄ Entra√Æner seulement de petits modules param√©triques\n",
        "   ‚îî‚îÄ ‚úÖ √âconomique, modulaire | ‚ùå N√©cessite toujours du training\n",
        "   ‚îî‚îÄ üí° Meilleur pour: Adaptation multi-domaine\n",
        "\n",
        "3Ô∏è‚É£  RAG (Retrieval-Augmented Generation)\n",
        "   ‚îî‚îÄ R√©cup√©rer des connaissances externes en temps r√©el\n",
        "   ‚îî‚îÄ ‚úÖ Flexible, mises √† jour faciles | ‚ùå D√©pend de la qualit√© du retriever\n",
        "   ‚îî‚îÄ üí° Meilleur pour: Connaissances dynamiques, agents IA\n",
        "\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                    üöÄ RECOMMANDATION POUR LES AGENTS IA                       ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "Pour les Agentic AI systems (LangChain, CrewAI, etc.):\n",
        "\n",
        "   ‚û°Ô∏è  RAG est souvent le GO-TO car:\n",
        "      ‚Ä¢ Les agents ont besoin de connaissances √† jour\n",
        "      ‚Ä¢ Les tools et l'it√©ration demandent de la flexibilit√©\n",
        "      ‚Ä¢ Les co√ªts sont raisonnables vs fine-tuning\n",
        "\n",
        "   ‚û°Ô∏è  Combinaison optimale:\n",
        "      ‚Ä¢ RAG pour les connaissances m√©tier dynamiques\n",
        "      ‚Ä¢ Fine-tuning/Adapters pour le format/style du mod√®le\n",
        "      ‚Ä¢ Tool calling pour les actions externes\n",
        "\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                         ‚úÖ NEXT STEPS                                         ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "1. Pratiquez RAG avec votre propre base de connaissances\n",
        "2. Explorez les frameworks (LangChain, Llama Index, etc.)\n",
        "3. Optimisez le retriever avec diff√©rentes embeddings\n",
        "4. Testez les agents IA int√©grant ces strat√©gies\n",
        "5. Mesurez la qualit√© des r√©ponses (eval)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ LAB03 COMPLETE!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogPnvje6nlxT"
      },
      "source": [
        "## Resources Suppl√©mentaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thW41_X9nlxT"
      },
      "outputs": [],
      "source": [
        "resources = \"\"\"\n",
        "üìö RESSOURCES RECOMMAND√âES:\n",
        "\n",
        "RAG & LangChain:\n",
        "  ‚Ä¢ https://python.langchain.com/\n",
        "  ‚Ä¢ https://js.langchain.com/\n",
        "  ‚Ä¢ RAG Best Practices: https://docs.llamaindex.ai/\n",
        "\n",
        "Fine-Tuning & LoRA:\n",
        "  ‚Ä¢ HuggingFace Fine-Tuning: https://huggingface.co/docs/transformers/training\n",
        "  ‚Ä¢ PEFT (LoRA): https://github.com/huggingface/peft\n",
        "  ‚Ä¢ OpenAI Fine-Tuning: https://platform.openai.com/docs/guides/fine-tuning\n",
        "\n",
        "Vector Databases:\n",
        "  ‚Ä¢ Chroma: https://www.trychroma.com/\n",
        "  ‚Ä¢ Pinecone: https://www.pinecone.io/\n",
        "  ‚Ä¢ Weaviate: https://weaviate.io/\n",
        "\n",
        "Frameworks Agentic AI:\n",
        "  ‚Ä¢ LangChain: https://python.langchain.com/\n",
        "  ‚Ä¢ CrewAI: https://www.crewai.com/\n",
        "  ‚Ä¢ Llama Index: https://www.llamaindex.ai/\n",
        "\n",
        "√âvaluation des syst√®mes RAG:\n",
        "  ‚Ä¢ RAGAS: https://github.com/explodinggradients/ragas\n",
        "  ‚Ä¢ DeepEval: https://www.deepeval.com/\n",
        "\"\"\"\n",
        "print(resources)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}