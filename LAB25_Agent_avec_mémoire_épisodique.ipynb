{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/RAG/blob/main/LAB25_Agent_avec_m%C3%A9moire_%C3%A9pisodique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAB25 : ImplÃ©mentation d'un agent avec mÃ©moire Ã©pisodique :\n",
        "\n",
        "**Objectif:** Capturer des \"Ã©pisodes\" d'interaction (qui/quoi/quand), les stocker avec des embeddings, et rÃ©cupÃ©rer les plus pertinents pour enrichir le contexte de l'agent.\n",
        "\n",
        "**DurÃ©e estimÃ©e:** 25â€“30 minutes\n",
        "\n",
        "**Livrable:** Notebook qui met en oeuvre un agent enregistre des Ã©pisodes, les rÃ©cupÃ¨re, et les utilise pour rÃ©pondre aux questions.\n",
        "\n",
        "---\n",
        "\n",
        "## Table des matiÃ¨res\n",
        "1. Installation & Configuration de l'environnement (5 min)\n",
        "2. DÃ©finition du store de mÃ©moire Ã©pisodique avec Chroma (2 min)\n",
        "3. Scoring recenceâ€“similaritÃ© (5 min)\n",
        "4. Agent simple qui enregistre les Ã©pisodes (5 min)\n",
        "5. Seed et tests (5 min)\n",
        "6. Enrichissement avec rÃ´les & tags (optionnel, 5 min)\n",
        "7. IntÃ©gration future dans des agents avancÃ©s"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1. Installation & Configuration de l'environnement\n",
        "**DurÃ©e:** 5 minutes\n",
        "\n",
        "Installation des dÃ©pendances nÃ©cessaires pour OpenAI, ChromaDB et gestion des variables d'environnement."
      ],
      "metadata": {
        "id": "section1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation des packages\n",
        "!pip install -q openai chromadb python-dotenv\n",
        "\n",
        "print(\"âœ… Packages installÃ©s avec succÃ¨s\")"
      ],
      "metadata": {
        "id": "install",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f098ae-e01c-4179-a01c-330056d428cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… Packages installÃ©s avec succÃ¨s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration de la clÃ© API OpenAI\n",
        "\n",
        "**Deux options:**\n",
        "- **Option A (recommandÃ©e pour Colab):** Utiliser les secrets de Colab\n",
        "- **Option B:** Entrer directement la clÃ© (âš ï¸ attention Ã  ne pas partager le notebook)"
      ],
      "metadata": {
        "id": "api_config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Option A: Utiliser les secrets de Google Colab (recommandÃ©)\n",
        "# Dans le menu de gauche: ğŸ”‘ Secrets â†’ Ajouter OPENAI_API_KEY\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"âœ… ClÃ© API chargÃ©e depuis les secrets Colab\")\n",
        "except:\n",
        "    # Option B: Saisie manuelle (la clÃ© ne sera pas affichÃ©e)\n",
        "    os.environ['OPENAI_API_KEY'] = getpass(\"Entrez votre clÃ© API OpenAI: \")\n",
        "    print(\"âœ… ClÃ© API configurÃ©e manuellement\")\n",
        "\n",
        "# VÃ©rification\n",
        "if not os.environ.get('OPENAI_API_KEY'):\n",
        "    raise ValueError(\"âŒ OPENAI_API_KEY n'est pas dÃ©finie!\")\n",
        "else:\n",
        "    print(f\"âœ… ClÃ© API prÃ©sente (longueur: {len(os.environ['OPENAI_API_KEY'])} caractÃ¨res)\")"
      ],
      "metadata": {
        "id": "api_key",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbda50dd-dd25-4291-e797-783b5b68d05b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ClÃ© API chargÃ©e depuis les secrets Colab\n",
            "âœ… ClÃ© API prÃ©sente (longueur: 164 caractÃ¨res)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. DÃ©finition du stockage de la mÃ©moire Ã©pisodique (Chroma)\n",
        "**DurÃ©e:** 2 minutes\n",
        "\n",
        "Mise en place de ChromaDB pour stocker les Ã©pisodes avec leurs embeddings et mÃ©tadonnÃ©es."
      ],
      "metadata": {
        "id": "section2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import uuid\n",
        "import math\n",
        "from datetime import datetime, timezone\n",
        "from openai import OpenAI\n",
        "import chromadb\n",
        "\n",
        "# Initialisation du client OpenAI\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "# ModÃ¨le d'embedding Ã  utiliser\n",
        "EMB_MODEL = \"text-embedding-3-small\"\n",
        "\n",
        "print(f\"âœ… Client OpenAI initialisÃ©\")\n",
        "print(f\"ğŸ“Š ModÃ¨le d'embedding: {EMB_MODEL}\")"
      ],
      "metadata": {
        "id": "init_clients",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6477f5-793d-4efc-8d5b-bc18a057be8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Client OpenAI initialisÃ©\n",
            "ğŸ“Š ModÃ¨le d'embedding: text-embedding-3-small\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def embed(text: str):\n",
        "    \"\"\"GÃ©nÃ¨re un embedding pour le texte donnÃ©.\"\"\"\n",
        "    response = client.embeddings.create(model=EMB_MODEL, input=text)\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# Test de la fonction d'embedding\n",
        "test_embedding = embed(\"Test de gÃ©nÃ©ration d'embedding\")\n",
        "print(f\"âœ… Fonction embed() opÃ©rationnelle\")\n",
        "print(f\"ğŸ“ Dimension de l'embedding: {len(test_embedding)}\")"
      ],
      "metadata": {
        "id": "embed_function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b9452b-6a95-4de0-ebc4-da917c21756a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fonction embed() opÃ©rationnelle\n",
            "ğŸ“ Dimension de l'embedding: 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CrÃ©ation d'une collection Chroma locale pour les Ã©pisodes\n",
        "chroma = chromadb.Client()\n",
        "episodes = chroma.get_or_create_collection(name=\"episodic_memory\")\n",
        "\n",
        "print(f\"âœ… Collection ChromaDB crÃ©Ã©e/rÃ©cupÃ©rÃ©e\")\n",
        "print(f\"ğŸ“¦ Nom de la collection: episodic_memory\")\n",
        "print(f\"ğŸ”¢ Nombre d'Ã©pisodes actuels: {episodes.count()}\")"
      ],
      "metadata": {
        "id": "chroma_init",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c806ac-60ba-4ccd-d81d-d20601d8e517"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collection ChromaDB crÃ©Ã©e/rÃ©cupÃ©rÃ©e\n",
            "ğŸ“¦ Nom de la collection: episodic_memory\n",
            "ğŸ”¢ Nombre d'Ã©pisodes actuels: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def now_ts():\n",
        "    \"\"\"Retourne le timestamp actuel en secondes.\"\"\"\n",
        "    return int(time.time())\n",
        "\n",
        "def ts_to_str(ts: int):\n",
        "    \"\"\"Convertit un timestamp en chaÃ®ne de caractÃ¨res lisible.\"\"\"\n",
        "    return datetime.fromtimestamp(ts, tz=timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "def add_episode(summary: str, who: str = \"user\", tags=None):\n",
        "    \"\"\"Stocke un Ã©pisode avec un rÃ©sumÃ© textuel + mÃ©tadonnÃ©es.\"\"\"\n",
        "    eid = str(uuid.uuid4())\n",
        "    meta = {\n",
        "        \"who\": who,\n",
        "        \"summary\": summary,\n",
        "        \"ts\": now_ts(),\n",
        "        \"tags\": \",\".join(tags) if tags else \"\"\n",
        "    }\n",
        "    episodes.add(\n",
        "        ids=[eid],\n",
        "        embeddings=[embed(summary)],\n",
        "        metadatas=[meta],\n",
        "        documents=[summary]\n",
        "    )\n",
        "    return eid\n",
        "\n",
        "# Test de la fonction d'ajout\n",
        "test_id = add_episode(\n",
        "    summary=\"Ã‰pisode de test pour vÃ©rifier le fonctionnement du systÃ¨me\",\n",
        "    who=\"system\",\n",
        "    tags=[\"test\", \"init\"]\n",
        ")\n",
        "print(f\"âœ… Fonction add_episode() opÃ©rationnelle\")\n",
        "print(f\"ğŸ†” ID de l'Ã©pisode test: {test_id}\")\n",
        "print(f\"ğŸ”¢ Nombre total d'Ã©pisodes: {episodes.count()}\")"
      ],
      "metadata": {
        "id": "episode_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b16cb082-54e0-4fcd-bcb2-c57da878c602"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fonction add_episode() opÃ©rationnelle\n",
            "ğŸ†” ID de l'Ã©pisode test: d4018dcd-f9d3-46ad-a68d-f2e59a7297e1\n",
            "ğŸ”¢ Nombre total d'Ã©pisodes: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Scoring FraÃ®cheurâ€“SimilaritÃ©\n",
        "**DurÃ©e:** 5 minutes\n",
        "\n",
        "ImplÃ©mentation d'un score hybride combinant la similaritÃ© sÃ©mantique et la fraÃ®cheur temporelle (ou \"rÃ©cence\") :\n",
        "\n",
        "**Formule:** `score = Î±Â·similarity + (1â€“Î±)Â·recency_weight`\n",
        "\n",
        "oÃ¹ `recency_weight = exp(-Î”t/Ï„)`\n",
        "\n",
        "- **Î±** (alpha): pondÃ©ration entre similaritÃ© et rÃ©cence (0.7 par dÃ©faut = 70% similaritÃ©, 30% rÃ©cence)\n",
        "- **Ï„** (tau): temps de demi-vie en heures (72h par dÃ©faut = 3 jours)"
      ],
      "metadata": {
        "id": "section3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_episodes(query: str, k: int = 5, alpha: float = 0.7, tau_hours: float = 72):\n",
        "    \"\"\"\n",
        "    Retourne les top-k Ã©pisodes par score hybride similaritÃ© + rÃ©cence.\n",
        "\n",
        "    Args:\n",
        "        query: Texte de la requÃªte\n",
        "        k: Nombre d'Ã©pisodes Ã  retourner\n",
        "        alpha: PondÃ©ration similaritÃ© (0-1, dÃ©faut 0.7)\n",
        "        tau_hours: Temps de demi-vie pour la rÃ©cence en heures (dÃ©faut 72h)\n",
        "\n",
        "    Returns:\n",
        "        Liste de dictionnaires contenant les Ã©pisodes et leurs scores\n",
        "    \"\"\"\n",
        "    qemb = embed(query)\n",
        "\n",
        "    # Recherche par similaritÃ© brute (on rÃ©cupÃ¨re plus de rÃ©sultats pour le re-ranking)\n",
        "    res = episodes.query(\n",
        "        query_embeddings=[qemb],\n",
        "        n_results=min(20, episodes.count()),\n",
        "        include=[\"metadatas\", \"distances\", \"documents\", \"embeddings\"]\n",
        "    )\n",
        "\n",
        "    if not res[\"ids\"] or len(res[\"ids\"][0]) == 0:\n",
        "        return []\n",
        "\n",
        "    now = now_ts()\n",
        "    tau = tau_hours * 3600.0  # conversion en secondes\n",
        "    out = []\n",
        "\n",
        "    for i in range(len(res[\"ids\"][0])):\n",
        "        meta = res[\"metadatas\"][0][i]\n",
        "        doc = res[\"documents\"][0][i]\n",
        "\n",
        "        # Chroma retourne une distance; conversion en similaritÃ©\n",
        "        distance = res[\"distances\"][0][i] if res[\"distances\"] else 0.0\n",
        "        similarity = 1 - distance  # inversion simple pour la dÃ©mo\n",
        "\n",
        "        # Calcul de la rÃ©cence\n",
        "        dt = max(0, now - int(meta[\"ts\"]))\n",
        "        rec = math.exp(-dt / tau)\n",
        "\n",
        "        # Score hybride\n",
        "        blended = alpha * similarity + (1 - alpha) * rec\n",
        "\n",
        "        out.append({\n",
        "            \"id\": res[\"ids\"][0][i],\n",
        "            \"summary\": doc,\n",
        "            \"who\": meta[\"who\"],\n",
        "            \"ts\": meta[\"ts\"],\n",
        "            \"when\": ts_to_str(int(meta[\"ts\"])),\n",
        "            \"similarity\": round(similarity, 4),\n",
        "            \"recency\": round(rec, 4),\n",
        "            \"score\": round(blended, 4),\n",
        "            \"tags\": meta.get(\"tags\", \"\")\n",
        "        })\n",
        "\n",
        "    # Tri par score dÃ©croissant\n",
        "    out.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    return out[:k]\n",
        "\n",
        "print(\"âœ… Fonction search_episodes() dÃ©finie\")\n",
        "print(\"ğŸ“Š ParamÃ¨tres par dÃ©faut:\")\n",
        "print(\"   - k = 5 (nombre de rÃ©sultats)\")\n",
        "print(\"   - Î± = 0.7 (70% similaritÃ©, 30% rÃ©cence)\")\n",
        "print(\"   - Ï„ = 72h (temps de demi-vie)\")"
      ],
      "metadata": {
        "id": "search_function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad9b415-674e-4bed-a519-f0956d8410a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fonction search_episodes() dÃ©finie\n",
            "ğŸ“Š ParamÃ¨tres par dÃ©faut:\n",
            "   - k = 5 (nombre de rÃ©sultats)\n",
            "   - Î± = 0.7 (70% similaritÃ©, 30% rÃ©cence)\n",
            "   - Ï„ = 72h (temps de demi-vie)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test de la fonction de recherche"
      ],
      "metadata": {
        "id": "test_search"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test de recherche sur l'Ã©pisode de test crÃ©Ã© prÃ©cÃ©demment\n",
        "test_results = search_episodes(\"systÃ¨me de test\", k=3)\n",
        "\n",
        "print(\"ğŸ” RÃ©sultats de recherche pour 'systÃ¨me de test':\")\n",
        "print(f\"ğŸ“Š Nombre de rÃ©sultats: {len(test_results)}\\n\")\n",
        "\n",
        "for i, result in enumerate(test_results, 1):\n",
        "    print(f\"\\n--- RÃ©sultat {i} ---\")\n",
        "    print(f\"ğŸ“ RÃ©sumÃ©: {result['summary'][:80]}...\")\n",
        "    print(f\"ğŸ‘¤ Qui: {result['who']}\")\n",
        "    print(f\"ğŸ“… Quand: {result['when']}\")\n",
        "    print(f\"ğŸ¯ Score total: {result['score']}\")\n",
        "    print(f\"   - SimilaritÃ©: {result['similarity']}\")\n",
        "    print(f\"   - RÃ©cence: {result['recency']}\")\n",
        "    print(f\"ğŸ·ï¸  Tags: {result['tags'] if result['tags'] else 'Aucun'}\")"
      ],
      "metadata": {
        "id": "test_search_exec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290e7b94-ce0d-469b-e8d9-9dcf40f5f85a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” RÃ©sultats de recherche pour 'systÃ¨me de test':\n",
            "ğŸ“Š Nombre de rÃ©sultats: 1\n",
            "\n",
            "\n",
            "--- RÃ©sultat 1 ---\n",
            "ğŸ“ RÃ©sumÃ©: Ã‰pisode de test pour vÃ©rifier le fonctionnement du systÃ¨me...\n",
            "ğŸ‘¤ Qui: system\n",
            "ğŸ“… Quand: 2026-01-30 11:09:57 UTC\n",
            "ğŸ¯ Score total: 0.5889\n",
            "   - SimilaritÃ©: 0.4129\n",
            "   - RÃ©cence: 0.9994\n",
            "ğŸ·ï¸  Tags: test,init\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Agent simple qui enregistre les Ã©pisodes\n",
        "**DurÃ©e:** 5 minutes\n",
        "\n",
        "L'agent:\n",
        "1. Prend une question de l'utilisateur\n",
        "2. RÃ©cupÃ¨re les mÃ©moires Ã©pisodiques les plus pertinentes\n",
        "3. Incorpore ces mÃ©moires dans le contexte systÃ¨me\n",
        "4. GÃ©nÃ¨re une rÃ©ponse\n",
        "5. Enregistre l'interaction comme un nouvel Ã©pisode"
      ],
      "metadata": {
        "id": "section4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_chat(system: str, user: str, model: str = \"gpt-4o-mini\"):\n",
        "    \"\"\"Appel simple au LLM avec contexte systÃ¨me et message utilisateur.\"\"\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user}\n",
        "        ],\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "def format_memories(mem_list):\n",
        "    \"\"\"Formate une liste d'informations mÃ©morisÃ©es pour inclusion dans le prompt.\"\"\"\n",
        "    if not mem_list:\n",
        "        return \"Aucune\"\n",
        "\n",
        "    lines = []\n",
        "    for m in mem_list:\n",
        "        lines.append(f\"- ({m['when']}) [{m['who']}] {m['summary']}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def agent_respond(user_text: str, k: int = 3, alpha: float = 0.7, tau_hours: float = 72, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    RÃ©ponse de l'agent avec mÃ©moire Ã©pisodique.\n",
        "\n",
        "    Args:\n",
        "        user_text: Question/message de l'utilisateur\n",
        "        k: Nombre de mÃ©moires Ã  rÃ©cupÃ©rer\n",
        "        alpha: PondÃ©ration similaritÃ©/rÃ©cence\n",
        "        tau_hours: Temps de demi-vie pour la rÃ©cence\n",
        "        verbose: Afficher les dÃ©tails de traitement\n",
        "\n",
        "    Returns:\n",
        "        Tuple (rÃ©ponse, mÃ©moires_utilisÃ©es)\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ğŸ¤– TRAITEMENT DE LA REQUÃŠTE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # 1) RÃ©cupÃ©rer les Ã©pisodes pertinents\n",
        "    if verbose:\n",
        "        print(f\"\\nğŸ” Recherche de {k} Ã©pisodes pertinents...\")\n",
        "\n",
        "    mems = search_episodes(user_text, k=k, alpha=alpha, tau_hours=tau_hours)\n",
        "    mem_context = format_memories(mems)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"âœ… {len(mems)} Ã©pisode(s) rÃ©cupÃ©rÃ©(s)\\n\")\n",
        "        if mems:\n",
        "            print(\"ğŸ“š MÃ©moires utilisÃ©es:\")\n",
        "            for i, m in enumerate(mems, 1):\n",
        "                print(f\"  {i}. [{m['who']}] {m['summary'][:60]}... (score: {m['score']})\")\n",
        "\n",
        "    # 2) Construire le prompt systÃ¨me avec contexte mÃ©moire\n",
        "    system = (\n",
        "        \"Tu es un assistant avec une mÃ©moire Ã©pisodique. \"\n",
        "        \"Si les 'Ã‰pisodes pertinents' contiennent du contexte utile, utilise-le pour informer ta rÃ©ponse. \"\n",
        "        \"Sois concis et cite les faits dans la mesure oÃ¹ ils apparaissent dans les Ã©pisodes. \"\n",
        "        \"RÃ©ponds en franÃ§ais de maniÃ¨re naturelle et professionnelle.\"\n",
        "        f\"\\n\\nÃ‰pisodes pertinents:\\n{mem_context}\"\n",
        "    )\n",
        "\n",
        "    # 3) GÃ©nÃ©rer la rÃ©ponse\n",
        "    if verbose:\n",
        "        print(\"\\nğŸ’­ GÃ©nÃ©ration de la rÃ©ponse...\")\n",
        "\n",
        "    answer = llm_chat(system, user_text)\n",
        "\n",
        "    # 4) Enregistrer cette interaction comme un nouvel Ã©pisode\n",
        "    episode_summary = f\"Q: {user_text} | A: {answer[:250]}\"\n",
        "    if len(answer) > 250:\n",
        "        episode_summary += \"...\"\n",
        "\n",
        "    add_episode(summary=episode_summary, who=\"agent\", tags=[\"dialog\", \"qa\"])\n",
        "\n",
        "    if verbose:\n",
        "        print(\"âœ… RÃ©ponse gÃ©nÃ©rÃ©e et enregistrÃ©e comme Ã©pisode\\n\")\n",
        "\n",
        "    return answer, mems\n",
        "\n",
        "print(\"âœ… Agent avec mÃ©moire Ã©pisodique configurÃ©\")"
      ],
      "metadata": {
        "id": "agent_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcfff122-8e01-4b2b-a9a5-094d9a2cac78"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Agent avec mÃ©moire Ã©pisodique configurÃ©\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 5. Alimentation et Tests\n",
        "**DurÃ©e:** 5 minutes\n",
        "\n",
        "Ajout de quelques Ã©pisodes de \"vie passÃ©e\" ou prÃ©fÃ©rences (le genre de choses dont un assistant devrait se souvenir plus tard)."
      ],
      "metadata": {
        "id": "section5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Initialisation de la mÃ©moire avec des Ã©pisodes"
      ],
      "metadata": {
        "id": "seed_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸŒ± Initialisation de la mÃ©moire avec des Ã©pisodes de dÃ©mo...\\n\")\n",
        "\n",
        "# Ã‰pisode 1: PrÃ©fÃ©rence framework\n",
        "eid1 = add_episode(\n",
        "    \"L'utilisateur a dit que son framework prÃ©fÃ©rÃ© est LangChain et qu'il construit des agents pour l'Ã©ducation.\",\n",
        "    who=\"user\",\n",
        "    tags=[\"pref\", \"framework\", \"langchain\", \"education\"]\n",
        ")\n",
        "print(f\"âœ… Ã‰pisode 1 ajoutÃ©: PrÃ©fÃ©rence LangChain (ID: {eid1[:8]}...)\")\n",
        "\n",
        "# Ã‰pisode 2: IntÃ©gration infrastructure\n",
        "eid2 = add_episode(\n",
        "    \"Nous avons intÃ©grÃ© Pinecone hier pour accÃ©lÃ©rer la recherche sÃ©mantique.\",\n",
        "    who=\"agent\",\n",
        "    tags=[\"project\", \"infra\", \"pinecone\", \"vector-db\"]\n",
        ")\n",
        "print(f\"âœ… Ã‰pisode 2 ajoutÃ©: IntÃ©gration Pinecone (ID: {eid2[:8]}...)\")\n",
        "\n",
        "# Ã‰pisode 3: Discussion technique\n",
        "eid3 = add_episode(\n",
        "    \"L'utilisateur a posÃ© des questions sur les boucles de planification (planning loops) vs les boucles rÃ©actives (reactive loops) dans les agents.\",\n",
        "    who=\"user\",\n",
        "    tags=[\"topic\", \"planning\", \"architecture\", \"agents\"]\n",
        ")\n",
        "print(f\"âœ… Ã‰pisode 3 ajoutÃ©: Discussion planning loops (ID: {eid3[:8]}...)\")\n",
        "\n",
        "# Ã‰pisode 4: Contexte projet\n",
        "eid4 = add_episode(\n",
        "    \"L'utilisateur travaille sur un projet de RAG pour l'Ã©ducation avec ChromaDB et OpenAI.\",\n",
        "    who=\"user\",\n",
        "    tags=[\"project\", \"rag\", \"education\", \"chromadb\"]\n",
        ")\n",
        "print(f\"âœ… Ã‰pisode 4 ajoutÃ©: Projet RAG Ã©ducation (ID: {eid4[:8]}...)\")\n",
        "\n",
        "# Ã‰pisode 5: PrÃ©fÃ©rence modÃ¨le\n",
        "eid5 = add_episode(\n",
        "    \"L'utilisateur prÃ©fÃ¨re utiliser GPT-4o-mini pour un bon Ã©quilibre coÃ»t/performance dans ses expÃ©rimentations.\",\n",
        "    who=\"user\",\n",
        "    tags=[\"pref\", \"model\", \"openai\", \"cost-optimization\"]\n",
        ")\n",
        "print(f\"âœ… Ã‰pisode 5 ajoutÃ©: PrÃ©fÃ©rence GPT-4o-mini (ID: {eid5[:8]}...)\")\n",
        "\n",
        "print(f\"\\nğŸ‰ {episodes.count()} Ã©pisodes au total dans la mÃ©moire\")"
      ],
      "metadata": {
        "id": "seed_memory",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acf6e75d-e329-4232-d59a-aaba6d6929a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒ± Initialisation de la mÃ©moire avec des Ã©pisodes de dÃ©mo...\n",
            "\n",
            "âœ… Ã‰pisode 1 ajoutÃ©: PrÃ©fÃ©rence LangChain (ID: 2b2bed46...)\n",
            "âœ… Ã‰pisode 2 ajoutÃ©: IntÃ©gration Pinecone (ID: 4297d173...)\n",
            "âœ… Ã‰pisode 3 ajoutÃ©: Discussion planning loops (ID: 77daafa8...)\n",
            "âœ… Ã‰pisode 4 ajoutÃ©: Projet RAG Ã©ducation (ID: 50b17ec5...)\n",
            "âœ… Ã‰pisode 5 ajoutÃ©: PrÃ©fÃ©rence GPT-4o-mini (ID: e5bb1fd0...)\n",
            "\n",
            "ğŸ‰ 6 Ã©pisodes au total dans la mÃ©moire\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Test 1: Question composite faisant appel plusieurs Ã©pisodes mÃ©morisÃ©s"
      ],
      "metadata": {
        "id": "test1_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"Quel framework ai-je dit que j'aimais, et qu'avons-nous intÃ©grÃ© hier ?\"\n",
        "\n",
        "print(\"\\n\" + \"#\"*80)\n",
        "print(\"TEST 1: Question composite\")\n",
        "print(\"#\"*80)\n",
        "print(f\"\\nâ“ Question: {question1}\\n\")\n",
        "\n",
        "answer1, used_mems1 = agent_respond(question1)\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ğŸ’¬ RÃ‰PONSE DE L'AGENT\")\n",
        "print(\"-\"*80)\n",
        "print(answer1)\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ğŸ“š MÃ‰MOIRES UTILISÃ‰ES\")\n",
        "print(\"-\"*80)\n",
        "for i, m in enumerate(used_mems1, 1):\n",
        "    print(f\"\\n{i}. {m['when']} :: {m['summary']}\")\n",
        "    print(f\"   Score: {m['score']} (similaritÃ©: {m['similarity']}, rÃ©cence: {m['recency']})\")\n",
        "    print(f\"   Tags: {m['tags']}\")"
      ],
      "metadata": {
        "id": "test1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e07f2634-55bf-4013-c7d6-b7a7aacccdbe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################################################################################\n",
            "TEST 1: Question composite\n",
            "################################################################################\n",
            "\n",
            "â“ Question: Quel framework ai-je dit que j'aimais, et qu'avons-nous intÃ©grÃ© hier ?\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ¤– TRAITEMENT DE LA REQUÃŠTE\n",
            "================================================================================\n",
            "\n",
            "ğŸ” Recherche de 3 Ã©pisodes pertinents...\n",
            "âœ… 3 Ã©pisode(s) rÃ©cupÃ©rÃ©(s)\n",
            "\n",
            "ğŸ“š MÃ©moires utilisÃ©es:\n",
            "  1. [user] L'utilisateur a dit que son framework prÃ©fÃ©rÃ© est LangChain ... (score: 0.3015)\n",
            "  2. [agent] Nous avons intÃ©grÃ© Pinecone hier pour accÃ©lÃ©rer la recherche... (score: 0.0892)\n",
            "  3. [user] L'utilisateur a posÃ© des questions sur les boucles de planif... (score: 0.0735)\n",
            "\n",
            "ğŸ’­ GÃ©nÃ©ration de la rÃ©ponse...\n",
            "âœ… RÃ©ponse gÃ©nÃ©rÃ©e et enregistrÃ©e comme Ã©pisode\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ’¬ RÃ‰PONSE DE L'AGENT\n",
            "--------------------------------------------------------------------------------\n",
            "Vous avez dit que votre framework prÃ©fÃ©rÃ© est LangChain, et nous avons intÃ©grÃ© Pinecone hier pour accÃ©lÃ©rer la recherche sÃ©mantique.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ“š MÃ‰MOIRES UTILISÃ‰ES\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. 2026-01-30 11:14:30 UTC :: L'utilisateur a dit que son framework prÃ©fÃ©rÃ© est LangChain et qu'il construit des agents pour l'Ã©ducation.\n",
            "   Score: 0.3015 (similaritÃ©: 0.0023, rÃ©cence: 0.9996)\n",
            "   Tags: pref,framework,langchain,education\n",
            "\n",
            "2. 2026-01-30 11:14:31 UTC :: Nous avons intÃ©grÃ© Pinecone hier pour accÃ©lÃ©rer la recherche sÃ©mantique.\n",
            "   Score: 0.0892 (similaritÃ©: -0.3009, rÃ©cence: 0.9996)\n",
            "   Tags: project,infra,pinecone,vector-db\n",
            "\n",
            "3. 2026-01-30 11:14:32 UTC :: L'utilisateur a posÃ© des questions sur les boucles de planification (planning loops) vs les boucles rÃ©actives (reactive loops) dans les agents.\n",
            "   Score: 0.0735 (similaritÃ©: -0.3234, rÃ©cence: 0.9996)\n",
            "   Tags: topic,planning,architecture,agents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Test 2: Rappel d'une discussion technique"
      ],
      "metadata": {
        "id": "test2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question2 = \"Rappelle-moi ce que nous avons discutÃ© Ã  propos des boucles de planification vs rÃ©actives.\"\n",
        "\n",
        "print(\"\\n\" + \"#\"*80)\n",
        "print(\"TEST 2: Rappel discussion technique\")\n",
        "print(\"#\"*80)\n",
        "print(f\"\\nâ“ Question: {question2}\\n\")\n",
        "\n",
        "answer2, used_mems2 = agent_respond(question2)\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ğŸ’¬ RÃ‰PONSE DE L'AGENT\")\n",
        "print(\"-\"*80)\n",
        "print(answer2)\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ğŸ“š MÃ‰MOIRES UTILISÃ‰ES\")\n",
        "print(\"-\"*80)\n",
        "for i, m in enumerate(used_mems2, 1):\n",
        "    print(f\"\\n{i}. {m['when']} :: {m['summary']}\")\n",
        "    print(f\"   Score: {m['score']} (similaritÃ©: {m['similarity']}, rÃ©cence: {m['recency']})\")"
      ],
      "metadata": {
        "id": "test2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860d1139-6233-4edc-c7a4-06df62db3f8b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################################################################################\n",
            "TEST 2: Rappel discussion technique\n",
            "################################################################################\n",
            "\n",
            "â“ Question: Rappelle-moi ce que nous avons discutÃ© Ã  propos des boucles de planification vs rÃ©actives.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ¤– TRAITEMENT DE LA REQUÃŠTE\n",
            "================================================================================\n",
            "\n",
            "ğŸ” Recherche de 3 Ã©pisodes pertinents...\n",
            "âœ… 3 Ã©pisode(s) rÃ©cupÃ©rÃ©(s)\n",
            "\n",
            "ğŸ“š MÃ©moires utilisÃ©es:\n",
            "  1. [user] L'utilisateur a posÃ© des questions sur les boucles de planif... (score: 0.5866)\n",
            "  2. [agent] Q: Quel framework ai-je dit que j'aimais, et qu'avons-nous i... (score: 0.1933)\n",
            "  3. [user] L'utilisateur a dit que son framework prÃ©fÃ©rÃ© est LangChain ... (score: 0.0343)\n",
            "\n",
            "ğŸ’­ GÃ©nÃ©ration de la rÃ©ponse...\n",
            "âœ… RÃ©ponse gÃ©nÃ©rÃ©e et enregistrÃ©e comme Ã©pisode\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ’¬ RÃ‰PONSE DE L'AGENT\n",
            "--------------------------------------------------------------------------------\n",
            "Vous avez posÃ© des questions sur les boucles de planification (planning loops) par rapport aux boucles rÃ©actives (reactive loops) dans les agents. Si vous souhaitez approfondir ce sujet ou avez des questions spÃ©cifiques, n'hÃ©sitez pas Ã  me le faire savoir !\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ“š MÃ‰MOIRES UTILISÃ‰ES\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. 2026-01-30 11:14:32 UTC :: L'utilisateur a posÃ© des questions sur les boucles de planification (planning loops) vs les boucles rÃ©actives (reactive loops) dans les agents.\n",
            "   Score: 0.5866 (similaritÃ©: 0.4096, rÃ©cence: 0.9996)\n",
            "\n",
            "2. 2026-01-30 11:16:12 UTC :: Q: Quel framework ai-je dit que j'aimais, et qu'avons-nous intÃ©grÃ© hier ? | A: Vous avez dit que votre framework prÃ©fÃ©rÃ© est LangChain, et nous avons intÃ©grÃ© Pinecone hier pour accÃ©lÃ©rer la recherche sÃ©mantique.\n",
            "   Score: 0.1933 (similaritÃ©: -0.1524, rÃ©cence: 1.0)\n",
            "\n",
            "3. 2026-01-30 11:14:30 UTC :: L'utilisateur a dit que son framework prÃ©fÃ©rÃ© est LangChain et qu'il construit des agents pour l'Ã©ducation.\n",
            "   Score: 0.0343 (similaritÃ©: -0.3794, rÃ©cence: 0.9996)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Test 3: VÃ©rification infrastructure"
      ],
      "metadata": {
        "id": "test3_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question3 = \"Avons-nous dÃ©jÃ  une base de donnÃ©es vectorielle configurÃ©e ?\"\n",
        "\n",
        "print(\"\\n\" + \"#\"*80)\n",
        "print(\"TEST 3: VÃ©rification infrastructure\")\n",
        "print(\"#\"*80)\n",
        "print(f\"\\nâ“ Question: {question3}\\n\")\n",
        "\n",
        "answer3, used_mems3 = agent_respond(question3)\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ğŸ’¬ RÃ‰PONSE DE L'AGENT\")\n",
        "print(\"-\"*80)\n",
        "print(answer3)\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ğŸ“š MÃ‰MOIRES UTILISÃ‰ES\")\n",
        "print(\"-\"*80)\n",
        "for i, m in enumerate(used_mems3, 1):\n",
        "    print(f\"\\n{i}. {m['when']} :: {m['summary']}\")\n",
        "    print(f\"   Score: {m['score']}\")"
      ],
      "metadata": {
        "id": "test3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffc4b35-03e1-4ebf-f912-4e526d740f03"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################################################################################\n",
            "TEST 3: VÃ©rification infrastructure\n",
            "################################################################################\n",
            "\n",
            "â“ Question: Avons-nous dÃ©jÃ  une base de donnÃ©es vectorielle configurÃ©e ?\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ¤– TRAITEMENT DE LA REQUÃŠTE\n",
            "================================================================================\n",
            "\n",
            "ğŸ” Recherche de 3 Ã©pisodes pertinents...\n",
            "âœ… 3 Ã©pisode(s) rÃ©cupÃ©rÃ©(s)\n",
            "\n",
            "ğŸ“š MÃ©moires utilisÃ©es:\n",
            "  1. [agent] Nous avons intÃ©grÃ© Pinecone hier pour accÃ©lÃ©rer la recherche... (score: 0.0969)\n",
            "  2. [user] L'utilisateur travaille sur un projet de RAG pour l'Ã©ducatio... (score: 0.0831)\n",
            "  3. [agent] Q: Quel framework ai-je dit que j'aimais, et qu'avons-nous i... (score: 0.061)\n",
            "\n",
            "ğŸ’­ GÃ©nÃ©ration de la rÃ©ponse...\n",
            "âœ… RÃ©ponse gÃ©nÃ©rÃ©e et enregistrÃ©e comme Ã©pisode\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ’¬ RÃ‰PONSE DE L'AGENT\n",
            "--------------------------------------------------------------------------------\n",
            "Oui, nous avons intÃ©grÃ© Pinecone hier pour accÃ©lÃ©rer la recherche sÃ©mantique, ce qui signifie que nous avons une base de donnÃ©es vectorielle configurÃ©e.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ“š MÃ‰MOIRES UTILISÃ‰ES\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. 2026-01-30 11:14:31 UTC :: Nous avons intÃ©grÃ© Pinecone hier pour accÃ©lÃ©rer la recherche sÃ©mantique.\n",
            "   Score: 0.0969\n",
            "\n",
            "2. 2026-01-30 11:14:32 UTC :: L'utilisateur travaille sur un projet de RAG pour l'Ã©ducation avec ChromaDB et OpenAI.\n",
            "   Score: 0.0831\n",
            "\n",
            "3. 2026-01-30 11:16:12 UTC :: Q: Quel framework ai-je dit que j'aimais, et qu'avons-nous intÃ©grÃ© hier ? | A: Vous avez dit que votre framework prÃ©fÃ©rÃ© est LangChain, et nous avons intÃ©grÃ© Pinecone hier pour accÃ©lÃ©rer la recherche sÃ©mantique.\n",
            "   Score: 0.061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5 Test 4: Question sans mÃ©moire pertinente"
      ],
      "metadata": {
        "id": "test4_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question4 = \"Quelle est la capitale de la France ?\"\n",
        "\n",
        "print(\"\\n\" + \"#\"*80)\n",
        "print(\"TEST 4: Question sans mÃ©moire pertinente\")\n",
        "print(\"#\"*80)\n",
        "print(f\"\\nâ“ Question: {question4}\\n\")\n",
        "\n",
        "answer4, used_mems4 = agent_respond(question4)\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ğŸ’¬ RÃ‰PONSE DE L'AGENT\")\n",
        "print(\"-\"*80)\n",
        "print(answer4)\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ğŸ“š MÃ‰MOIRES UTILISÃ‰ES\")\n",
        "print(\"-\"*80)\n",
        "if used_mems4:\n",
        "    for i, m in enumerate(used_mems4, 1):\n",
        "        print(f\"\\n{i}. {m['when']} :: {m['summary'][:80]}...\")\n",
        "        print(f\"   Score: {m['score']}\")\n",
        "else:\n",
        "    print(\"Aucune mÃ©moire pertinente trouvÃ©e.\")"
      ],
      "metadata": {
        "id": "test4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655de149-fb73-4250-b1df-8bd8f1c6031f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################################################################################\n",
            "TEST 4: Question sans mÃ©moire pertinente\n",
            "################################################################################\n",
            "\n",
            "â“ Question: Quelle est la capitale de la France ?\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ¤– TRAITEMENT DE LA REQUÃŠTE\n",
            "================================================================================\n",
            "\n",
            "ğŸ” Recherche de 3 Ã©pisodes pertinents...\n",
            "âœ… 3 Ã©pisode(s) rÃ©cupÃ©rÃ©(s)\n",
            "\n",
            "ğŸ“š MÃ©moires utilisÃ©es:\n",
            "  1. [agent] Q: Quel framework ai-je dit que j'aimais, et qu'avons-nous i... (score: -0.2192)\n",
            "  2. [system] Ã‰pisode de test pour vÃ©rifier le fonctionnement du systÃ¨me... (score: -0.2225)\n",
            "  3. [agent] Q: Avons-nous dÃ©jÃ  une base de donnÃ©es vectorielle configurÃ©... (score: -0.2536)\n",
            "\n",
            "ğŸ’­ GÃ©nÃ©ration de la rÃ©ponse...\n",
            "âœ… RÃ©ponse gÃ©nÃ©rÃ©e et enregistrÃ©e comme Ã©pisode\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ’¬ RÃ‰PONSE DE L'AGENT\n",
            "--------------------------------------------------------------------------------\n",
            "La capitale de la France est Paris.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ“š MÃ‰MOIRES UTILISÃ‰ES\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. 2026-01-30 11:16:12 UTC :: Q: Quel framework ai-je dit que j'aimais, et qu'avons-nous intÃ©grÃ© hier ? | A: V...\n",
            "   Score: -0.2192\n",
            "\n",
            "2. 2026-01-30 11:09:57 UTC :: Ã‰pisode de test pour vÃ©rifier le fonctionnement du systÃ¨me...\n",
            "   Score: -0.2225\n",
            "\n",
            "3. 2026-01-30 11:16:27 UTC :: Q: Avons-nous dÃ©jÃ  une base de donnÃ©es vectorielle configurÃ©e ? | A: Oui, nous a...\n",
            "   Score: -0.2536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6 Visualisation de toute la mÃ©moire"
      ],
      "metadata": {
        "id": "view_memory_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“š CONTENU COMPLET DE LA MÃ‰MOIRE Ã‰PISODIQUE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nğŸ”¢ Total: {episodes.count()} Ã©pisodes\\n\")\n",
        "\n",
        "# RÃ©cupÃ©rer tous les Ã©pisodes\n",
        "all_episodes = episodes.get(include=[\"metadatas\", \"documents\"])\n",
        "\n",
        "if all_episodes[\"ids\"]:\n",
        "    for i, eid in enumerate(all_episodes[\"ids\"], 1):\n",
        "        meta = all_episodes[\"metadatas\"][i-1]\n",
        "        doc = all_episodes[\"documents\"][i-1]\n",
        "\n",
        "        print(f\"\\n{'-'*80}\")\n",
        "        print(f\"Ã‰pisode {i}\")\n",
        "        print(f\"{'-'*80}\")\n",
        "        print(f\"ğŸ†” ID: {eid}\")\n",
        "        print(f\"ğŸ‘¤ Qui: {meta['who']}\")\n",
        "        print(f\"ğŸ“… Quand: {ts_to_str(int(meta['ts']))}\")\n",
        "        print(f\"ğŸ·ï¸  Tags: {meta.get('tags', 'Aucun')}\")\n",
        "        print(f\"ğŸ“ Contenu: {doc}\")\n",
        "else:\n",
        "    print(\"Aucun Ã©pisode dans la mÃ©moire.\")"
      ],
      "metadata": {
        "id": "view_all_memory",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88bd9c2-29a1-40d5-c80a-4d740c552243"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ“š CONTENU COMPLET DE LA MÃ‰MOIRE Ã‰PISODIQUE\n",
            "================================================================================\n",
            "\n",
            "ğŸ”¢ Total: 10 Ã©pisodes\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Ã‰pisode 1\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ†” ID: d4018dcd-f9d3-46ad-a68d-f2e59a7297e1\n",
            "ğŸ‘¤ Qui: system\n",
            "ğŸ“… Quand: 2026-01-30 11:09:57 UTC\n",
            "ğŸ·ï¸  Tags: test,init\n",
            "ğŸ“ Contenu: Ã‰pisode de test pour vÃ©rifier le fonctionnement du systÃ¨me\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Ã‰pisode 2\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ†” ID: 2b2bed46-3f4e-469f-9559-3b76b32ff633\n",
            "ğŸ‘¤ Qui: user\n",
            "ğŸ“… Quand: 2026-01-30 11:14:30 UTC\n",
            "ğŸ·ï¸  Tags: pref,framework,langchain,education\n",
            "ğŸ“ Contenu: L'utilisateur a dit que son framework prÃ©fÃ©rÃ© est LangChain et qu'il construit des agents pour l'Ã©ducation.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Ã‰pisode 3\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ†” ID: 4297d173-761f-4305-91fd-53d8e37c2059\n",
            "ğŸ‘¤ Qui: agent\n",
            "ğŸ“… Quand: 2026-01-30 11:14:31 UTC\n",
            "ğŸ·ï¸  Tags: project,infra,pinecone,vector-db\n",
            "ğŸ“ Contenu: Nous avons intÃ©grÃ© Pinecone hier pour accÃ©lÃ©rer la recherche sÃ©mantique.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Ã‰pisode 4\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ†” ID: 77daafa8-2bf6-43d4-b8a1-4ebb884f7ff6\n",
            "ğŸ‘¤ Qui: user\n",
            "ğŸ“… Quand: 2026-01-30 11:14:32 UTC\n",
            "ğŸ·ï¸  Tags: topic,planning,architecture,agents\n",
            "ğŸ“ Contenu: L'utilisateur a posÃ© des questions sur les boucles de planification (planning loops) vs les boucles rÃ©actives (reactive loops) dans les agents.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Ã‰pisode 5\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ†” ID: 50b17ec5-ad42-415f-9363-cdb6cb04230a\n",
            "ğŸ‘¤ Qui: user\n",
            "ğŸ“… Quand: 2026-01-30 11:14:32 UTC\n",
            "ğŸ·ï¸  Tags: project,rag,education,chromadb\n",
            "ğŸ“ Contenu: L'utilisateur travaille sur un projet de RAG pour l'Ã©ducation avec ChromaDB et OpenAI.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Ã‰pisode 6\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ†” ID: e5bb1fd0-8657-45ae-9b16-38a27d5b12ee\n",
            "ğŸ‘¤ Qui: user\n",
            "ğŸ“… Quand: 2026-01-30 11:14:32 UTC\n",
            "ğŸ·ï¸  Tags: pref,model,openai,cost-optimization\n",
            "ğŸ“ Contenu: L'utilisateur prÃ©fÃ¨re utiliser GPT-4o-mini pour un bon Ã©quilibre coÃ»t/performance dans ses expÃ©rimentations.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Ã‰pisode 7\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ†” ID: d5b822a5-e1cc-4404-9757-15824e0476bf\n",
            "ğŸ‘¤ Qui: agent\n",
            "ğŸ“… Quand: 2026-01-30 11:16:12 UTC\n",
            "ğŸ·ï¸  Tags: dialog,qa\n",
            "ğŸ“ Contenu: Q: Quel framework ai-je dit que j'aimais, et qu'avons-nous intÃ©grÃ© hier ? | A: Vous avez dit que votre framework prÃ©fÃ©rÃ© est LangChain, et nous avons intÃ©grÃ© Pinecone hier pour accÃ©lÃ©rer la recherche sÃ©mantique.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Ã‰pisode 8\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ†” ID: 3166584a-ef80-4d7a-bf7b-ffe4a69c41d2\n",
            "ğŸ‘¤ Qui: agent\n",
            "ğŸ“… Quand: 2026-01-30 11:16:20 UTC\n",
            "ğŸ·ï¸  Tags: dialog,qa\n",
            "ğŸ“ Contenu: Q: Rappelle-moi ce que nous avons discutÃ© Ã  propos des boucles de planification vs rÃ©actives. | A: Vous avez posÃ© des questions sur les boucles de planification (planning loops) par rapport aux boucles rÃ©actives (reactive loops) dans les agents. Si vous souhaitez approfondir ce sujet ou avez des questions spÃ©cifiques, n'hÃ©sitez pas Ã  me le faire s...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Ã‰pisode 9\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ†” ID: 3e1c1543-8db2-4cc4-b1f4-63c94d2ba5cd\n",
            "ğŸ‘¤ Qui: agent\n",
            "ğŸ“… Quand: 2026-01-30 11:16:27 UTC\n",
            "ğŸ·ï¸  Tags: dialog,qa\n",
            "ğŸ“ Contenu: Q: Avons-nous dÃ©jÃ  une base de donnÃ©es vectorielle configurÃ©e ? | A: Oui, nous avons intÃ©grÃ© Pinecone hier pour accÃ©lÃ©rer la recherche sÃ©mantique, ce qui signifie que nous avons une base de donnÃ©es vectorielle configurÃ©e.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Ã‰pisode 10\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ†” ID: 63614573-00c7-4141-a758-ee07e20a196d\n",
            "ğŸ‘¤ Qui: agent\n",
            "ğŸ“… Quand: 2026-01-30 11:16:34 UTC\n",
            "ğŸ·ï¸  Tags: dialog,qa\n",
            "ğŸ“ Contenu: Q: Quelle est la capitale de la France ? | A: La capitale de la France est Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 6. Enrichissement avec RÃ´les & Tags (Optionnel)\n",
        "**DurÃ©e:** 10â€“15 minutes\n",
        "\n",
        "AmÃ©lioration du systÃ¨me avec:\n",
        "- **Importance**: Score 1-5 pour booster les mÃ©moires importantes\n",
        "- **Filtrage par tags**: RÃ©cupÃ©rer uniquement certains types d'Ã©pisodes"
      ],
      "metadata": {
        "id": "section6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Ajout d'un champ d'importance"
      ],
      "metadata": {
        "id": "importance_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_episode_enhanced(summary: str, who: str = \"user\", tags=None, importance: int = 3):\n",
        "    \"\"\"\n",
        "    Version enrichie: ajoute un champ d'importance (1-5).\n",
        "\n",
        "    Args:\n",
        "        summary: RÃ©sumÃ© de l'Ã©pisode\n",
        "        who: Qui a gÃ©nÃ©rÃ© l'Ã©pisode (user/agent/system)\n",
        "        tags: Liste de tags pour catÃ©gorisation\n",
        "        importance: Score d'importance 1-5 (1=trivial, 5=critique)\n",
        "    \"\"\"\n",
        "    importance = max(1, min(5, importance))  # Clamp entre 1 et 5\n",
        "\n",
        "    eid = str(uuid.uuid4())\n",
        "    meta = {\n",
        "        \"who\": who,\n",
        "        \"summary\": summary,\n",
        "        \"ts\": now_ts(),\n",
        "        \"tags\": \",\".join(tags) if tags else \"\",\n",
        "        \"importance\": importance\n",
        "    }\n",
        "    episodes.add(\n",
        "        ids=[eid],\n",
        "        embeddings=[embed(summary)],\n",
        "        metadatas=[meta],\n",
        "        documents=[summary]\n",
        "    )\n",
        "    return eid\n",
        "\n",
        "print(\"âœ… Fonction add_episode_enhanced() dÃ©finie\")\n",
        "print(\"ğŸ“Š Niveaux d'importance:\")\n",
        "print(\"   1 = Trivial (conversation informelle)\")\n",
        "print(\"   2 = Faible (information secondaire)\")\n",
        "print(\"   3 = Moyen (information utile) [dÃ©faut]\")\n",
        "print(\"   4 = Ã‰levÃ© (dÃ©cision, prÃ©fÃ©rence importante)\")\n",
        "print(\"   5 = Critique (deadline, milestone, dÃ©cision majeure)\")"
      ],
      "metadata": {
        "id": "enhanced_add",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22df3971-7ae1-4259-8a92-34eef9d6a146"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fonction add_episode_enhanced() dÃ©finie\n",
            "ğŸ“Š Niveaux d'importance:\n",
            "   1 = Trivial (conversation informelle)\n",
            "   2 = Faible (information secondaire)\n",
            "   3 = Moyen (information utile) [dÃ©faut]\n",
            "   4 = Ã‰levÃ© (dÃ©cision, prÃ©fÃ©rence importante)\n",
            "   5 = Critique (deadline, milestone, dÃ©cision majeure)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_episodes_enhanced(query: str, k: int = 5, alpha: float = 0.7, tau_hours: float = 72,\n",
        "                            importance_boost: float = 0.1):\n",
        "    \"\"\"\n",
        "    Recherche avec boost d'importance.\n",
        "\n",
        "    Score final = score_hybride Ã— (1 + importance_boost Ã— (importance - 3))\n",
        "\n",
        "    Args:\n",
        "        importance_boost: Facteur de boost par niveau d'importance (dÃ©faut 0.1 = 10%)\n",
        "    \"\"\"\n",
        "    qemb = embed(query)\n",
        "\n",
        "    res = episodes.query(\n",
        "        query_embeddings=[qemb],\n",
        "        n_results=min(20, episodes.count()),\n",
        "        include=[\"metadatas\", \"distances\", \"documents\"]\n",
        "    )\n",
        "\n",
        "    if not res[\"ids\"] or len(res[\"ids\"][0]) == 0:\n",
        "        return []\n",
        "\n",
        "    now = now_ts()\n",
        "    tau = tau_hours * 3600.0\n",
        "    out = []\n",
        "\n",
        "    for i in range(len(res[\"ids\"][0])):\n",
        "        meta = res[\"metadatas\"][0][i]\n",
        "        doc = res[\"documents\"][0][i]\n",
        "\n",
        "        distance = res[\"distances\"][0][i] if res[\"distances\"] else 0.0\n",
        "        similarity = 1 - distance\n",
        "\n",
        "        dt = max(0, now - int(meta[\"ts\"]))\n",
        "        rec = math.exp(-dt / tau)\n",
        "\n",
        "        blended = alpha * similarity + (1 - alpha) * rec\n",
        "\n",
        "        # Boost basÃ© sur l'importance\n",
        "        importance = int(meta.get(\"importance\", 3))\n",
        "        importance_multiplier = 1 + importance_boost * (importance - 3)\n",
        "        final_score = blended * importance_multiplier\n",
        "\n",
        "        out.append({\n",
        "            \"id\": res[\"ids\"][0][i],\n",
        "            \"summary\": doc,\n",
        "            \"who\": meta[\"who\"],\n",
        "            \"ts\": meta[\"ts\"],\n",
        "            \"when\": ts_to_str(int(meta[\"ts\"])),\n",
        "            \"similarity\": round(similarity, 4),\n",
        "            \"recency\": round(rec, 4),\n",
        "            \"importance\": importance,\n",
        "            \"base_score\": round(blended, 4),\n",
        "            \"score\": round(final_score, 4),\n",
        "            \"tags\": meta.get(\"tags\", \"\")\n",
        "        })\n",
        "\n",
        "    out.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    return out[:k]\n",
        "\n",
        "print(\"âœ… Fonction search_episodes_enhanced() dÃ©finie\")\n",
        "print(\"ğŸ“ˆ Le boost d'importance modifie le score final:\")\n",
        "print(\"   - Importance 5: +20% au score\")\n",
        "print(\"   - Importance 4: +10% au score\")\n",
        "print(\"   - Importance 3: Aucun changement (baseline)\")\n",
        "print(\"   - Importance 2: -10% au score\")\n",
        "print(\"   - Importance 1: -20% au score\")"
      ],
      "metadata": {
        "id": "enhanced_search",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2eb8dc3-3650-4471-f48e-3e1388b6c482"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fonction search_episodes_enhanced() dÃ©finie\n",
            "ğŸ“ˆ Le boost d'importance modifie le score final:\n",
            "   - Importance 5: +20% au score\n",
            "   - Importance 4: +10% au score\n",
            "   - Importance 3: Aucun changement (baseline)\n",
            "   - Importance 2: -10% au score\n",
            "   - Importance 1: -20% au score\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Recherche avec filtrage par tags"
      ],
      "metadata": {
        "id": "filter_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_episodes_with_filter(query: str, include_tags=None, exclude_tags=None,\n",
        "                                k: int = 5, **kwargs):\n",
        "    \"\"\"\n",
        "    Recherche avec filtrage cÃ´tÃ© client par tags.\n",
        "\n",
        "    Args:\n",
        "        query: Texte de recherche\n",
        "        include_tags: Liste de tags - l'Ã©pisode doit avoir AU MOINS UN de ces tags\n",
        "        exclude_tags: Liste de tags - l'Ã©pisode NE DOIT PAS avoir ces tags\n",
        "        k: Nombre de rÃ©sultats Ã  retourner\n",
        "        **kwargs: Autres paramÃ¨tres passÃ©s Ã  search_episodes_enhanced\n",
        "    \"\"\"\n",
        "    # RÃ©cupÃ©rer plus de rÃ©sultats pour compenser le filtrage\n",
        "    all_results = search_episodes_enhanced(query, k=k*3, **kwargs)\n",
        "\n",
        "    filtered = []\n",
        "    for m in all_results:\n",
        "        episode_tags = set(m[\"tags\"].split(\",\")) if m[\"tags\"] else set()\n",
        "\n",
        "        # VÃ©rifier les exclusions en premier\n",
        "        if exclude_tags:\n",
        "            if any(tag in episode_tags for tag in exclude_tags):\n",
        "                continue\n",
        "\n",
        "        # VÃ©rifier les inclusions\n",
        "        if include_tags:\n",
        "            if not any(tag in episode_tags for tag in include_tags):\n",
        "                continue\n",
        "\n",
        "        filtered.append(m)\n",
        "\n",
        "        if len(filtered) >= k:\n",
        "            break\n",
        "\n",
        "    return filtered\n",
        "\n",
        "print(\"âœ… Fonction search_episodes_with_filter() dÃ©finie\")\n",
        "print(\"ğŸ·ï¸  Permet de filtrer par tags:\")\n",
        "print(\"   - include_tags: Inclure uniquement les Ã©pisodes avec CES tags\")\n",
        "print(\"   - exclude_tags: Exclure les Ã©pisodes avec CES tags\")"
      ],
      "metadata": {
        "id": "filter_function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b6eced-1494-4101-fc39-5feaeb7600e6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fonction search_episodes_with_filter() dÃ©finie\n",
            "ğŸ·ï¸  Permet de filtrer par tags:\n",
            "   - include_tags: Inclure uniquement les Ã©pisodes avec CES tags\n",
            "   - exclude_tags: Exclure les Ã©pisodes avec CES tags\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Tests des fonctionnalitÃ©s enrichies"
      ],
      "metadata": {
        "id": "enhanced_tests_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ§ª TESTS DES FONCTIONNALITÃ‰S ENRICHIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Ajouter des Ã©pisodes avec diffÃ©rents niveaux d'importance\n",
        "print(\"\\nğŸ“ Ajout d'Ã©pisodes test avec importance variable...\\n\")\n",
        "\n",
        "eid_low = add_episode_enhanced(\n",
        "    \"Conversation informelle sur la mÃ©tÃ©o Ã  Paris.\",\n",
        "    who=\"user\",\n",
        "    tags=[\"small-talk\", \"weather\"],\n",
        "    importance=1\n",
        ")\n",
        "print(f\"âœ… Ã‰pisode trivial ajoutÃ© (importance=1): {eid_low[:8]}...\")\n",
        "\n",
        "eid_high = add_episode_enhanced(\n",
        "    \"DEADLINE CRITIQUE: Le projet doit Ãªtre livrÃ© avant le 15 fÃ©vrier 2026 pour la dÃ©mo client.\",\n",
        "    who=\"user\",\n",
        "    tags=[\"deadline\", \"milestone\", \"project\"],\n",
        "    importance=5\n",
        ")\n",
        "print(f\"âœ… Ã‰pisode critique ajoutÃ© (importance=5): {eid_high[:8]}...\")\n",
        "\n",
        "eid_decision = add_episode_enhanced(\n",
        "    \"DÃ©cision prise: nous utiliserons React pour le frontend au lieu de Vue.\",\n",
        "    who=\"agent\",\n",
        "    tags=[\"decision\", \"frontend\", \"tech-stack\"],\n",
        "    importance=4\n",
        ")\n",
        "print(f\"âœ… Ã‰pisode dÃ©cision ajoutÃ© (importance=4): {eid_decision[:8]}...\")"
      ],
      "metadata": {
        "id": "enhanced_test_add",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0fac54-a30b-41c9-9d83-4479d21ba5a3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ§ª TESTS DES FONCTIONNALITÃ‰S ENRICHIES\n",
            "================================================================================\n",
            "\n",
            "ğŸ“ Ajout d'Ã©pisodes test avec importance variable...\n",
            "\n",
            "âœ… Ã‰pisode trivial ajoutÃ© (importance=1): 9b00b4d4...\n",
            "âœ… Ã‰pisode critique ajoutÃ© (importance=5): 6a742db3...\n",
            "âœ… Ã‰pisode dÃ©cision ajoutÃ© (importance=4): 1b8adff6...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1: Recherche avec boost d'importance\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ğŸ” TEST: Recherche 'projet' avec boost d'importance\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "results_boosted = search_episodes_enhanced(\"projet\", k=5, importance_boost=0.1)\n",
        "\n",
        "print(f\"\\nğŸ“Š {len(results_boosted)} rÃ©sultats trouvÃ©s:\\n\")\n",
        "for i, r in enumerate(results_boosted, 1):\n",
        "    print(f\"{i}. [{r['who']}] {r['summary'][:60]}...\")\n",
        "    print(f\"   Importance: {r['importance']}/5 | Score base: {r['base_score']} â†’ Score final: {r['score']}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "enhanced_test_boost",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8eb8980-71af-48ed-ce14-9b10c365261a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ” TEST: Recherche 'projet' avec boost d'importance\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ğŸ“Š 5 rÃ©sultats trouvÃ©s:\n",
            "\n",
            "1. [user] DEADLINE CRITIQUE: Le projet doit Ãªtre livrÃ© avant le 15 fÃ©v...\n",
            "   Importance: 5/5 | Score base: 0.1782 â†’ Score final: 0.2138\n",
            "\n",
            "2. [user] L'utilisateur travaille sur un projet de RAG pour l'Ã©ducatio...\n",
            "   Importance: 3/5 | Score base: 0.1271 â†’ Score final: 0.1271\n",
            "\n",
            "3. [agent] Q: Rappelle-moi ce que nous avons discutÃ© Ã  propos des boucl...\n",
            "   Importance: 3/5 | Score base: 0.0418 â†’ Score final: 0.0418\n",
            "\n",
            "4. [agent] Q: Quel framework ai-je dit que j'aimais, et qu'avons-nous i...\n",
            "   Importance: 3/5 | Score base: 0.0128 â†’ Score final: 0.0128\n",
            "\n",
            "5. [agent] DÃ©cision prise: nous utiliserons React pour le frontend au l...\n",
            "   Importance: 4/5 | Score base: 0.0068 â†’ Score final: 0.0075\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2: Filtrage par tags (inclure uniquement les dÃ©cisions)\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ğŸ” TEST: Recherche avec filtre include_tags=['decision', 'milestone']\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "results_filtered = search_episodes_with_filter(\n",
        "    \"projet\",\n",
        "    include_tags=[\"decision\", \"milestone\"],\n",
        "    k=5\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ“Š {len(results_filtered)} rÃ©sultats filtrÃ©s:\\n\")\n",
        "for i, r in enumerate(results_filtered, 1):\n",
        "    print(f\"{i}. [{r['who']}] {r['summary'][:60]}...\")\n",
        "    print(f\"   Tags: {r['tags']} | Score: {r['score']}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "enhanced_test_filter",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9624fe-b369-45e9-df75-0c163709b5fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ” TEST: Recherche avec filtre include_tags=['decision', 'milestone']\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ğŸ“Š 2 rÃ©sultats filtrÃ©s:\n",
            "\n",
            "1. [user] DEADLINE CRITIQUE: Le projet doit Ãªtre livrÃ© avant le 15 fÃ©v...\n",
            "   Tags: deadline,milestone,project | Score: 0.2137\n",
            "\n",
            "2. [agent] DÃ©cision prise: nous utiliserons React pour le frontend au l...\n",
            "   Tags: decision,frontend,tech-stack | Score: 0.0076\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 3: Filtrage par exclusion (exclure small-talk)\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ğŸ” TEST: Recherche avec filtre exclude_tags=['small-talk', 'test']\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "results_excluded = search_episodes_with_filter(\n",
        "    \"projet framework\",\n",
        "    exclude_tags=[\"small-talk\", \"test\"],\n",
        "    k=5\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ“Š {len(results_excluded)} rÃ©sultats (sans small-talk ni test):\\n\")\n",
        "for i, r in enumerate(results_excluded, 1):\n",
        "    print(f\"{i}. [{r['who']}] {r['summary'][:70]}...\")\n",
        "    print(f\"   Tags: {r['tags']}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "enhanced_test_exclude",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af022b4c-77fa-4faf-977d-578ed120b550"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ” TEST: Recherche avec filtre exclude_tags=['small-talk', 'test']\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ğŸ“Š 5 rÃ©sultats (sans small-talk ni test):\n",
            "\n",
            "1. [agent] Q: Quel framework ai-je dit que j'aimais, et qu'avons-nous intÃ©grÃ© hie...\n",
            "   Tags: dialog,qa\n",
            "\n",
            "2. [user] L'utilisateur a dit que son framework prÃ©fÃ©rÃ© est LangChain et qu'il c...\n",
            "   Tags: pref,framework,langchain,education\n",
            "\n",
            "3. [user] DEADLINE CRITIQUE: Le projet doit Ãªtre livrÃ© avant le 15 fÃ©vrier 2026 ...\n",
            "   Tags: deadline,milestone,project\n",
            "\n",
            "4. [agent] Q: Rappelle-moi ce que nous avons discutÃ© Ã  propos des boucles de plan...\n",
            "   Tags: dialog,qa\n",
            "\n",
            "5. [user] L'utilisateur travaille sur un projet de RAG pour l'Ã©ducation avec Chr...\n",
            "   Tags: project,rag,education,chromadb\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 7. IntÃ©gration dans des Agents AvancÃ©s\n",
        "**DurÃ©e:** Lecture et application future\n",
        "\n",
        "Cette section explique comment intÃ©grer la mÃ©moire Ã©pisodique dans des architectures d'agents plus complexes."
      ],
      "metadata": {
        "id": "section7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 StratÃ©gies d'intÃ©gration\n",
        "\n",
        "#### **A. PrÃ©fixe de contexte automatique**\n",
        "```python\n",
        "def build_system_prompt_with_memory(base_prompt: str, user_query: str, k: int = 3):\n",
        "    \"\"\"Ajoute automatiquement les Ã©pisodes pertinents au prompt systÃ¨me.\"\"\"\n",
        "    memories = search_episodes(user_query, k=k)\n",
        "    memory_context = format_memories(memories)\n",
        "    \n",
        "    return f\"\"\"{base_prompt}\n",
        "\n",
        "MÃ‰MOIRE Ã‰PISODIQUE:\n",
        "{memory_context}\n",
        "\n",
        "Utilise ces informations pour contextualiser ta rÃ©ponse.\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "#### **B. Logging des rÃ©sultats d'outils**\n",
        "```python\n",
        "def execute_tool_with_memory(tool_name: str, params: dict):\n",
        "    \"\"\"ExÃ©cute un outil et enregistre le rÃ©sultat comme Ã©pisode.\"\"\"\n",
        "    result = execute_tool(tool_name, params)\n",
        "    \n",
        "    # Enregistrer le succÃ¨s/Ã©chec\n",
        "    if result.success:\n",
        "        add_episode_enhanced(\n",
        "            f\"Outil {tool_name} exÃ©cutÃ© avec succÃ¨s: {result.summary}\",\n",
        "            who=\"agent\",\n",
        "            tags=[\"tool\", tool_name, \"success\"],\n",
        "            importance=3\n",
        "        )\n",
        "    else:\n",
        "        add_episode_enhanced(\n",
        "            f\"Ã‰chec de {tool_name}: {result.error}\",\n",
        "            who=\"agent\",\n",
        "            tags=[\"tool\", tool_name, \"failure\"],\n",
        "            importance=4  # Les Ã©checs sont importants pour l'apprentissage\n",
        "        )\n",
        "    \n",
        "    return result\n",
        "```\n",
        "\n",
        "#### **C. IntÃ©gration avec RAG**\n",
        "```python\n",
        "def hybrid_retrieval(query: str, k_docs: int = 5, k_episodes: int = 3):\n",
        "    \"\"\"Combine rÃ©cupÃ©ration documentaire (RAG) et mÃ©moire Ã©pisodique.\"\"\"\n",
        "    # RÃ©cupÃ©ration documentaire classique\n",
        "    docs = retrieve_documents(query, k=k_docs)\n",
        "    \n",
        "    # RÃ©cupÃ©ration d'Ã©pisodes pertinents\n",
        "    episodes = search_episodes(query, k=k_episodes)\n",
        "    \n",
        "    context = f\"\"\"\n",
        "DOCUMENTS DE RÃ‰FÃ‰RENCE:\n",
        "{format_documents(docs)}\n",
        "\n",
        "EXPÃ‰RIENCE PASSÃ‰E (MÃ©moire Ã©pisodique):\n",
        "{format_memories(episodes)}\n",
        "\"\"\"\n",
        "    return context\n",
        "```\n",
        "\n",
        "#### **D. Apprentissage des prÃ©fÃ©rences utilisateur**\n",
        "```python\n",
        "def detect_and_store_preferences(user_message: str):\n",
        "    \"\"\"DÃ©tecte et enregistre les prÃ©fÃ©rences utilisateur.\"\"\"\n",
        "    # Patterns de prÃ©fÃ©rences\n",
        "    preference_patterns = [\n",
        "        r\"j'aime|je prÃ©fÃ¨re|je veux\",\n",
        "        r\"mon.*prÃ©fÃ©rÃ©\",\n",
        "        r\"je dÃ©teste|je n'aime pas\"\n",
        "    ]\n",
        "    \n",
        "    for pattern in preference_patterns:\n",
        "        if re.search(pattern, user_message, re.IGNORECASE):\n",
        "            add_episode_enhanced(\n",
        "                user_message,\n",
        "                who=\"user\",\n",
        "                tags=[\"preference\", \"user-profile\"],\n",
        "                importance=4\n",
        "            )\n",
        "            break\n",
        "```"
      ],
      "metadata": {
        "id": "integration_strategies"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Cas d'usage avancÃ©s\n",
        "\n",
        "#### **1. Agent avec planification**\n",
        "- Enregistrer chaque Ã©tape du plan comme Ã©pisode\n",
        "- RÃ©cupÃ©rer les plans similaires passÃ©s pour amÃ©liorer la planification actuelle\n",
        "- Tags: `[\"planning\", \"step-1\", \"success/failure\"]`\n",
        "\n",
        "#### **2. Agent multi-sessions**\n",
        "- Persistance de la mÃ©moire entre sessions\n",
        "- RÃ©sumÃ© automatique des sessions passÃ©es\n",
        "- Tags: `[\"session-{id}\", \"summary\"]`\n",
        "\n",
        "#### **3. Agent collaboratif**\n",
        "- MÃ©moire partagÃ©e entre plusieurs agents\n",
        "- Tracking de qui a fait quoi\n",
        "- Tags: `[\"agent-{name}\", \"collaboration\"]`\n",
        "\n",
        "#### **4. Agent apprenant**\n",
        "- Enregistrer les feedbacks utilisateurs\n",
        "- RÃ©cupÃ©rer les erreurs passÃ©es pour Ã©viter rÃ©pÃ©tition\n",
        "- Tags: `[\"feedback\", \"error\", \"correction\"]`"
      ],
      "metadata": {
        "id": "advanced_usecases"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Exemple complet: Agent RAG avec mÃ©moire"
      ],
      "metadata": {
        "id": "full_example_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_agent_with_memory(user_query: str, document_collection, k_docs: int = 3, k_episodes: int = 3):\n",
        "    \"\"\"\n",
        "    Agent RAG enrichi avec mÃ©moire Ã©pisodique.\n",
        "\n",
        "    Workflow:\n",
        "    1. RÃ©cupÃ¨re documents pertinents (RAG)\n",
        "    2. RÃ©cupÃ¨re Ã©pisodes pertinents (mÃ©moire)\n",
        "    3. Fusionne les deux sources de contexte\n",
        "    4. GÃ©nÃ¨re rÃ©ponse\n",
        "    5. Enregistre l'interaction\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ” Recherche RAG + MÃ©moire pour: '{user_query}'\\n\")\n",
        "\n",
        "    # 1. RÃ©cupÃ©ration documentaire (simulÃ©e ici)\n",
        "    print(f\"ğŸ“š RÃ©cupÃ©ration de {k_docs} documents...\")\n",
        "    # docs = document_collection.query(user_query, k=k_docs)\n",
        "    docs = [\"[Document simulÃ© 1]\", \"[Document simulÃ© 2]\"]  # Simulation\n",
        "    print(f\"âœ… {len(docs)} documents rÃ©cupÃ©rÃ©s\")\n",
        "\n",
        "    # 2. RÃ©cupÃ©ration Ã©pisodes\n",
        "    print(f\"ğŸ§  RÃ©cupÃ©ration de {k_episodes} Ã©pisodes...\")\n",
        "    episodes_mem = search_episodes_enhanced(user_query, k=k_episodes)\n",
        "    print(f\"âœ… {len(episodes_mem)} Ã©pisodes rÃ©cupÃ©rÃ©s\")\n",
        "\n",
        "    # 3. Construction du contexte hybride\n",
        "    doc_context = \"\\n\".join(f\"- {doc}\" for doc in docs)\n",
        "    memory_context = format_memories(episodes_mem)\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "Tu es un assistant RAG avec mÃ©moire Ã©pisodique.\n",
        "\n",
        "DOCUMENTS DE RÃ‰FÃ‰RENCE:\n",
        "{doc_context}\n",
        "\n",
        "EXPÃ‰RIENCE PASSÃ‰E (MÃ©moire Ã©pisodique):\n",
        "{memory_context}\n",
        "\n",
        "Instructions:\n",
        "- Priorise les documents de rÃ©fÃ©rence pour les faits objectifs\n",
        "- Utilise la mÃ©moire Ã©pisodique pour le contexte personnel et les prÃ©fÃ©rences\n",
        "- Cite tes sources quand possible\n",
        "- RÃ©ponds en franÃ§ais de maniÃ¨re concise et prÃ©cise\n",
        "\"\"\"\n",
        "\n",
        "    # 4. GÃ©nÃ©ration de rÃ©ponse\n",
        "    print(\"\\nğŸ’­ GÃ©nÃ©ration de la rÃ©ponse...\")\n",
        "    answer = llm_chat(system_prompt, user_query)\n",
        "\n",
        "    # 5. Enregistrement\n",
        "    add_episode_enhanced(\n",
        "        f\"RAG Query: {user_query} | Answer: {answer[:200]}\",\n",
        "        who=\"agent\",\n",
        "        tags=[\"rag\", \"query\", \"answered\"],\n",
        "        importance=3\n",
        "    )\n",
        "    print(\"âœ… Interaction enregistrÃ©e\\n\")\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"documents_used\": len(docs),\n",
        "        \"episodes_used\": episodes_mem\n",
        "    }\n",
        "\n",
        "print(\"âœ… Fonction rag_agent_with_memory() dÃ©finie\")"
      ],
      "metadata": {
        "id": "rag_agent_example",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd281c6b-c655-452d-c4c5-96d32727ffe0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fonction rag_agent_with_memory() dÃ©finie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test de l'agent RAG avec mÃ©moire\n",
        "result = rag_agent_with_memory(\n",
        "    \"Quel framework devrais-je utiliser pour mon projet Ã©ducatif ?\",\n",
        "    document_collection=None,  # Collection simulÃ©e\n",
        "    k_docs=3,\n",
        "    k_episodes=3\n",
        ")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ’¬ RÃ‰PONSE DE L'AGENT RAG\")\n",
        "print(\"=\"*80)\n",
        "print(result[\"answer\"])\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ğŸ“Š MÃ‰TADONNÃ‰ES\")\n",
        "print(\"-\"*80)\n",
        "print(f\"Documents utilisÃ©s: {result['documents_used']}\")\n",
        "print(f\"Ã‰pisodes utilisÃ©s: {len(result['episodes_used'])}\")\n",
        "\n",
        "if result['episodes_used']:\n",
        "    print(\"\\nğŸ§  DÃ©tail des Ã©pisodes:\")\n",
        "    for i, ep in enumerate(result['episodes_used'], 1):\n",
        "        print(f\"  {i}. {ep['summary'][:60]}... (score: {ep['score']})\")"
      ],
      "metadata": {
        "id": "rag_agent_test",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae24338-aca9-4fdb-ef8b-f350f268eaf4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Recherche RAG + MÃ©moire pour: 'Quel framework devrais-je utiliser pour mon projet Ã©ducatif ?'\n",
            "\n",
            "ğŸ“š RÃ©cupÃ©ration de 3 documents...\n",
            "âœ… 2 documents rÃ©cupÃ©rÃ©s\n",
            "ğŸ§  RÃ©cupÃ©ration de 3 Ã©pisodes...\n",
            "âœ… 3 Ã©pisodes rÃ©cupÃ©rÃ©s\n",
            "\n",
            "ğŸ’­ GÃ©nÃ©ration de la rÃ©ponse...\n",
            "âœ… Interaction enregistrÃ©e\n",
            "\n",
            "================================================================================\n",
            "ğŸ’¬ RÃ‰PONSE DE L'AGENT RAG\n",
            "================================================================================\n",
            "Vous avez mentionnÃ© que votre framework prÃ©fÃ©rÃ© est LangChain, ce qui pourrait Ãªtre un excellent choix pour votre projet Ã©ducatif. Ã‰tant donnÃ© que vous travaillez Ã©galement sur un projet de RAG (RÃ©cupÃ©ration-Augmentation de GÃ©nÃ©ration) avec ChromaDB et OpenAI, LangChain pourrait s'intÃ©grer efficacement dans votre flux de travail.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ“Š MÃ‰TADONNÃ‰ES\n",
            "--------------------------------------------------------------------------------\n",
            "Documents utilisÃ©s: 2\n",
            "Ã‰pisodes utilisÃ©s: 3\n",
            "\n",
            "ğŸ§  DÃ©tail des Ã©pisodes:\n",
            "  1. L'utilisateur a dit que son framework prÃ©fÃ©rÃ© est LangChain ... (score: 0.3306)\n",
            "  2. L'utilisateur travaille sur un projet de RAG pour l'Ã©ducatio... (score: 0.2871)\n",
            "  3. Q: Quel framework ai-je dit que j'aimais, et qu'avons-nous i... (score: 0.157)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ğŸ¯ RÃ©sumÃ© et Points ClÃ©s\n",
        "\n",
        "### Ce que vous avez appris:\n",
        "\n",
        "1. **MÃ©moire Ã©pisodique**: Stockage d'interactions avec mÃ©tadonnÃ©es (qui/quoi/quand)\n",
        "2. **Embeddings**: ReprÃ©sentation vectorielle pour recherche sÃ©mantique\n",
        "3. **Score hybride**: Combinaison similaritÃ© + rÃ©cence avec dÃ©croissance exponentielle\n",
        "4. **ChromaDB**: Base de donnÃ©es vectorielle simple et efficace\n",
        "5. **Enrichissement**: Importance, tags, filtrage pour amÃ©liorer la pertinence\n",
        "6. **IntÃ©gration**: Comment incorporer dans des agents RAG et planificateurs\n",
        "\n",
        "### Formules importantes:\n",
        "\n",
        "**Score hybride de base:**\n",
        "```\n",
        "score = Î±Â·similarity + (1-Î±)Â·recency\n",
        "recency = exp(-Î”t/Ï„)\n",
        "```\n",
        "\n",
        "**Score avec importance:**\n",
        "```\n",
        "final_score = score Ã— [1 + Î²Â·(importance - 3)]\n",
        "```\n",
        "\n",
        "### ParamÃ¨tres typiques:\n",
        "- **Î± (alpha)**: 0.7 (70% similaritÃ©, 30% rÃ©cence)\n",
        "- **Ï„ (tau)**: 72h (temps de demi-vie)\n",
        "- **Î² (beta)**: 0.1 (boost 10% par niveau d'importance)\n",
        "- **k**: 3-5 (nombre d'Ã©pisodes Ã  rÃ©cupÃ©rer)\n",
        "\n",
        "### Prochaines Ã©tapes:\n",
        "1. ExpÃ©rimenter avec diffÃ©rentes valeurs de Î± et Ï„\n",
        "2. Ajouter un systÃ¨me de rÃ©sumÃ© automatique des Ã©pisodes\n",
        "3. ImplÃ©menter un mÃ©canisme d'oubli (purge des vieux Ã©pisodes)\n",
        "4. IntÃ©grer avec vos propres agents et workflows\n",
        "5. Tester avec des donnÃ©es rÃ©elles de vos projets"
      ],
      "metadata": {
        "id": "summary"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ğŸ“ Exercices Pratiques (Optionnel)\n",
        "\n",
        "### Exercice 1: Ajustement des paramÃ¨tres\n",
        "Modifiez Î± et Ï„ pour diffÃ©rents cas d'usage:\n",
        "- Support client: Î±=0.5, Ï„=24h (Ã©quilibre, rÃ©cence importante)\n",
        "- Assistant personnel: Î±=0.8, Ï„=168h (similaritÃ© prioritaire, mÃ©moire longue)\n",
        "- Chatbot conversationnel: Î±=0.3, Ï„=2h (rÃ©cence forte, court terme)\n",
        "\n",
        "### Exercice 2: SystÃ¨me de notation\n",
        "ImplÃ©menter un systÃ¨me oÃ¹ l'utilisateur peut noter les rÃ©ponses (1-5 Ã©toiles) et ajuster l'importance des Ã©pisodes en fonction.\n",
        "\n",
        "### Exercice 3: RÃ©sumÃ© automatique\n",
        "CrÃ©er une fonction qui rÃ©sume automatiquement les N derniers Ã©pisodes en un seul Ã©pisode \"rÃ©sumÃ© de session\".\n",
        "\n",
        "### Exercice 4: Dashboard de mÃ©moire\n",
        "CrÃ©er une fonction de visualisation montrant:\n",
        "- Distribution des tags\n",
        "- Timeline des Ã©pisodes\n",
        "- Statistiques d'importance\n",
        "- Qui (user/agent) a gÃ©nÃ©rÃ© le plus d'Ã©pisodes"
      ],
      "metadata": {
        "id": "exercises"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ESPACE POUR VOS EXPÃ‰RIMENTATIONS\n",
        "# Utilisez cette cellule pour tester vos propres idÃ©es\n",
        "\n",
        "# Exemple: Tester diffÃ©rentes valeurs de Î±\n",
        "# for alpha_val in [0.3, 0.5, 0.7, 0.9]:\n",
        "#     print(f\"\\nAlpha = {alpha_val}\")\n",
        "#     results = search_episodes(\"votre query\", k=3, alpha=alpha_val)\n",
        "#     # Analyser les rÃ©sultats...\n",
        "\n",
        "pass"
      ],
      "metadata": {
        "id": "experimentation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ğŸ”§ Utilitaires & Debug"
      ],
      "metadata": {
        "id": "utilities_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_memory():\n",
        "    \"\"\"âš ï¸ ATTENTION: Efface TOUTE la mÃ©moire Ã©pisodique!\"\"\"\n",
        "    global episodes, chroma\n",
        "    try:\n",
        "        chroma.delete_collection(name=\"episodic_memory\")\n",
        "    except:\n",
        "        pass\n",
        "    episodes = chroma.get_or_create_collection(name=\"episodic_memory\")\n",
        "    print(\"ğŸ—‘ï¸ MÃ©moire Ã©pisodique rÃ©initialisÃ©e\")\n",
        "    print(f\"ğŸ”¢ Nombre d'Ã©pisodes: {episodes.count()}\")\n",
        "\n",
        "def memory_stats():\n",
        "    \"\"\"Affiche des statistiques sur la mÃ©moire.\"\"\"\n",
        "    all_eps = episodes.get(include=[\"metadatas\"])\n",
        "\n",
        "    if not all_eps[\"ids\"]:\n",
        "        print(\"âŒ MÃ©moire vide\")\n",
        "        return\n",
        "\n",
        "    total = len(all_eps[\"ids\"])\n",
        "\n",
        "    # Comptage par qui\n",
        "    who_counts = {}\n",
        "    tag_counts = {}\n",
        "    importance_counts = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
        "\n",
        "    for meta in all_eps[\"metadatas\"]:\n",
        "        # Who\n",
        "        who = meta[\"who\"]\n",
        "        who_counts[who] = who_counts.get(who, 0) + 1\n",
        "\n",
        "        # Tags\n",
        "        tags = meta.get(\"tags\", \"\").split(\",\")\n",
        "        for tag in tags:\n",
        "            if tag:\n",
        "                tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
        "\n",
        "        # Importance\n",
        "        imp = int(meta.get(\"importance\", 3))\n",
        "        importance_counts[imp] = importance_counts.get(imp, 0) + 1\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ“Š STATISTIQUES DE LA MÃ‰MOIRE Ã‰PISODIQUE\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nğŸ”¢ Total d'Ã©pisodes: {total}\")\n",
        "\n",
        "    print(\"\\nğŸ‘¥ Distribution par acteur:\")\n",
        "    for who, count in sorted(who_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        pct = (count/total)*100\n",
        "        print(f\"  {who}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "    print(\"\\nğŸ·ï¸  Top 10 tags:\")\n",
        "    for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "        print(f\"  {tag}: {count}\")\n",
        "\n",
        "    print(\"\\nâ­ Distribution importance:\")\n",
        "    for imp in range(1, 6):\n",
        "        count = importance_counts[imp]\n",
        "        pct = (count/total)*100 if total > 0 else 0\n",
        "        bar = \"â–ˆ\" * int(pct/2)\n",
        "        print(f\"  {imp}: {bar} {count} ({pct:.1f}%)\")\n",
        "\n",
        "def search_by_tags_only(tags: list, limit: int = 10):\n",
        "    \"\"\"Recherche des Ã©pisodes uniquement par tags (sans query sÃ©mantique).\"\"\"\n",
        "    all_eps = episodes.get(include=[\"metadatas\", \"documents\"])\n",
        "\n",
        "    if not all_eps[\"ids\"]:\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    for i, eid in enumerate(all_eps[\"ids\"]):\n",
        "        meta = all_eps[\"metadatas\"][i]\n",
        "        episode_tags = set(meta.get(\"tags\", \"\").split(\",\"))\n",
        "\n",
        "        if any(tag in episode_tags for tag in tags):\n",
        "            results.append({\n",
        "                \"id\": eid,\n",
        "                \"summary\": all_eps[\"documents\"][i],\n",
        "                \"who\": meta[\"who\"],\n",
        "                \"when\": ts_to_str(int(meta[\"ts\"])),\n",
        "                \"tags\": meta.get(\"tags\", \"\"),\n",
        "                \"importance\": int(meta.get(\"importance\", 3))\n",
        "            })\n",
        "\n",
        "            if len(results) >= limit:\n",
        "                break\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"âœ… Fonctions utilitaires chargÃ©es:\")\n",
        "print(\"   - reset_memory(): Efface toute la mÃ©moire\")\n",
        "print(\"   - memory_stats(): Affiche les statistiques\")\n",
        "print(\"   - search_by_tags_only(tags): Recherche par tags uniquement\")"
      ],
      "metadata": {
        "id": "utilities"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les statistiques de la mÃ©moire\n",
        "memory_stats()"
      ],
      "metadata": {
        "id": "show_stats"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ğŸ“ Conclusion\n",
        "\n",
        "FÃ©licitations! Vous avez implÃ©mentÃ© un systÃ¨me complet de mÃ©moire Ã©pisodique pour agents.\n",
        "\n",
        "**Ce lab vous a permis de:**\n",
        "- âœ… Comprendre les embeddings et la recherche vectorielle\n",
        "- âœ… ImplÃ©menter un scoring hybride similaritÃ©/rÃ©cence\n",
        "- âœ… Construire un agent avec mÃ©moire contextuelle\n",
        "- âœ… Enrichir avec importance et tags\n",
        "- âœ… IntÃ©grer dans des architectures avancÃ©es (RAG, planning)\n",
        "\n",
        "**Pour aller plus loin:**\n",
        "- Explorez d'autres bases vectorielles (Pinecone, Weaviate, Qdrant)\n",
        "- ImplÃ©mentez un systÃ¨me de mÃ©moire sÃ©mantique (faits gÃ©nÃ©raux) vs Ã©pisodique (expÃ©riences)\n",
        "- Ajoutez une mÃ©moire procÃ©durale (comment faire les choses)\n",
        "- CrÃ©ez des agents multi-mÃ©moires avec diffÃ©rentes stratÃ©gies de rÃ©cupÃ©ration\n",
        "\n",
        "**Ressources:**\n",
        "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
        "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
        "- [LangChain Memory](https://python.langchain.com/docs/modules/memory/)\n",
        "\n",
        "---\n",
        "\n",
        "**Questions? Suggestions? Feedback?**  \n",
        "N'hÃ©sitez pas Ã  expÃ©rimenter avec le code et adapter Ã  vos cas d'usage!\n",
        "\n",
        "ğŸš€ **Happy coding!**"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}